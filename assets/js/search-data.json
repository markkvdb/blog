{
  
    
        "post0": {
            "title": "Excess Mortality In 2020 (covid 19)",
            "content": "Official reported deaths due to COVID-19 are often under-reported for a multitude of reasons. The most important reason is that not all suspected cases get tested. Another way to measure COVID related deaths is to look at official reported total deaths. Administrative delays, however, make it that reported numbers are only reliable from a few weeks in the past. More recent numbers are often missing many deaths. The Human Mortality Database (HMD) provided detailed (yearly) mortality and population data for 41 selected countries. Recently, HMD published a dataset containing weekly mortality numbers in light of the COVID-19 pandemic. In this article we will explore this dataset to obtain insights in excess deaths for different countries. . Note: this article has been updated at the 26th of June. More recent data is used which includes more countries. . Load and process data . First we load some packages that we will use throughout the analysis . library(readxl) library(tidyverse) library(hrbrthemes) library(lubridate) library(janitor) library(reshape2) . The data is publicly available and can be loaded into R as . mf_data &lt;- readr::read_csv(&quot;https://www.mortality.org/Public/STMF/Outputs/stmf.csv&quot;, skip=2) knitr::kable(head(mf_data)) . CountryCode Year Week Sex D0_14 D15_64 D65_74 D75_84 D85p DTotal R0_14 R15_64 R65_74 R75_84 R85p RTotal Split SplitSex Forecast . AUT | 2000 | 1 | m | 7 | 183 | 212 | 249 | 163 | 814 | 0.0005204 | 0.0035126 | 0.0376068 | 0.0951375 | 0.2318343 | 0.0109252 | 0 | 0 | 0 | . AUT | 2000 | 1 | f | 2 | 104 | 141 | 338 | 468 | 1053 | 0.0001562 | 0.0020023 | 0.0195527 | 0.0614423 | 0.2243570 | 0.0132385 | 0 | 0 | 0 | . AUT | 2000 | 1 | b | 9 | 287 | 353 | 587 | 631 | 1867 | 0.0003428 | 0.0027586 | 0.0274739 | 0.0723052 | 0.2262420 | 0.0121196 | 0 | 0 | 0 | . AUT | 2000 | 2 | m | 4 | 195 | 195 | 259 | 187 | 840 | 0.0002974 | 0.0037430 | 0.0345912 | 0.0989583 | 0.2659694 | 0.0112741 | 0 | 0 | 0 | . AUT | 2000 | 2 | f | 6 | 109 | 126 | 312 | 509 | 1062 | 0.0004687 | 0.0020986 | 0.0174727 | 0.0567159 | 0.2440122 | 0.0133516 | 0 | 0 | 0 | . AUT | 2000 | 2 | b | 10 | 304 | 321 | 571 | 696 | 1902 | 0.0003809 | 0.0029220 | 0.0249834 | 0.0703344 | 0.2495474 | 0.0123468 | 0 | 0 | 0 | . The dataset contains multiple variables for the death counts for different age groups (starting with D) and the death rate (starting with R). Furthermore, most variables are not in a human-readable format yet. So let’s transform the dataset a bit. . mf_data &lt;- mf_data %&gt;% janitor::clean_names() %&gt;% gather(starts_with(&quot;d&quot;), key=&quot;age_group&quot;, value=&quot;death_count&quot;) %&gt;% gather(starts_with(&quot;r&quot;), key=&quot;age_group2&quot;, value=&quot;death_rate&quot;) %&gt;% mutate( sex = case_when( sex == &quot;m&quot; ~ &quot;male&quot;, sex == &quot;f&quot; ~ &quot;female&quot;, sex == &quot;b&quot; ~ &quot;both&quot; ), age_group = str_replace(age_group, &quot;_&quot;, &quot;-&quot;), age_group = str_sub(age_group, start=2), age_group = if_else(age_group == &quot;85p&quot;, &quot;85+&quot;, age_group), country = recode(country_code, AUT = &quot;Austria&quot;, BEL = &quot;Belgium&quot;, BGR = &quot;Bulgaria&quot;, CZE = &quot;Czech Republic&quot;, DEUTNP = &quot;Germany&quot;, DNK = &quot;Denmark&quot;, ESP = &quot;Spain&quot;, EST = &quot;Estonia&quot;, FIN = &quot;Finland&quot;, FRATNP = &quot;France&quot;, GBRTENW = &quot;England &amp; Wales&quot;, GBR_SCO = &quot;Great Britain&quot;, HUN = &quot;Hungary&quot;, ISL = &quot;Iceland&quot;, ITA = &quot;Italy&quot;, LUX = &quot;Luxembourg&quot;, NLD = &quot;Netherlands&quot;, NOR = &quot;Norway&quot;, PRT = &quot;Portugal&quot;, SVK = &quot;Slovakia&quot;, SWE = &quot;Sweden&quot;, USA = &quot;United States&quot;) ) %&gt;% select(age_group, country, year, sex, week, death_count, death_rate) knitr::kable(head(mf_data)) . age_group country year sex week death_count death_rate . 0-14 | Austria | 2000 | male | 1 | 7 | 0.0005204 | . 0-14 | Austria | 2000 | female | 1 | 2 | 0.0001562 | . 0-14 | Austria | 2000 | both | 1 | 9 | 0.0003428 | . 0-14 | Austria | 2000 | male | 2 | 4 | 0.0002974 | . 0-14 | Austria | 2000 | female | 2 | 6 | 0.0004687 | . 0-14 | Austria | 2000 | both | 2 | 10 | 0.0003809 | . Total excess deaths by country . The advantage of weekly mortality data is the opportunity to compare the mortality of March and April of this year with previous years. Below you find a plot of the weekly mortality data of the selected countries for all available years (differs by country). The 2020 mortality is shown as the red line and the pink/red area on the figures shows the period up to the current week (21 as of writing this). . plot_data &lt;- mf_data %&gt;% mutate(is_2020 = year == 2020) curr_week &lt;- week(Sys.Date()) colour_pal &lt;- c(&quot;#cccccc&quot;, &quot;#cc0000&quot;) # total not differentiated by sex plot_data %&gt;% filter(sex == &quot;both&quot;, age_group == &quot;-total&quot;) %&gt;% ggplot(aes(x=week, y=death_count, colour=is_2020, group=year)) + annotate(&quot;rect&quot;, xmin=-Inf, xmax=curr_week, ymin=-Inf, ymax=Inf, fill=&quot;red&quot;, colour=NA, alpha=0.1) + geom_line(show.legend=FALSE) + facet_wrap(~country, ncol=3, scales=&quot;free_y&quot;) + scale_color_manual(values=colour_pal) + labs(x=&quot;Week&quot;, y=&quot;Death count&quot;, title=&quot;Excess mortality 2020&quot;, caption=&quot;Source: mortality.org&quot;) + theme_ipsum_rc() . . A few things stand out from this figure besides the COVID-19 mortality trends. First, larger countries show trends that are more smoooth than countries with smaller populations such as Iceland. Countries also seem to deal differently with the first and last week of the year: the UK reports high mortality in the first weeks of the year and low numbers at the end while the trend is reversed for Spain. Another thing to note is that not all countries always report the final data for 2020 and the lag of reporting differs significantly between countries. The US shows unprecedented low mortality data deaths at the end of the reported period in 2020, which is likely caused by incomplete data for these weeks. . The 2020 mortality data shows clear differences between countries. Excess mortality is referred to the extra deaths above the regular death count expected in a country. While some countries reports no to very low excess mortality, others like Belgium, England, the Netherlands and Spain show significant excess mortality. . Noticeably, Sweden seems to be the only Nordic country that shows significant excess mortality. Furthermore, the only two countries that at some point took the group immunity approach seriously seems to not only show high excess mortality at the peak but also still report significant excess mortality at the latest reported week. This, while other countries have mostly returned to the baseline mortality. . Excess mortality comparison between countries . To better compare the excess mortality between countries we will estimate the excess mortality for all countries. We do this by computing the 5-year average of the total mortality per week from the years 2015 to 2019. We then subtract the mortality numbers for 2020 to arrive at an estimate for the excess mortality. Note that this approach does not take into account that the 5-year historic mortality does not necessarily equal the expected mortality in 2020 without COVID-19. This might differ for a variety of reasons like a bad flu season or changes in demographics. . excess_deaths &lt;- plot_data %&gt;% filter(year &gt;= 2015) %&gt;% group_by(country, sex, age_group, week, is_2020) %&gt;% summarise( death_count = mean(death_count) ) %&gt;% dcast(country + sex + age_group + week ~ is_2020, value.var=&quot;death_count&quot;) %&gt;% mutate(excess_death = `TRUE` - `FALSE`, excess_death_std = excess_death / `FALSE`) %&gt;% select(-`TRUE`, -`FALSE`) %&gt;% na.omit() excess_deaths %&gt;% filter(sex == &quot;both&quot;, age_group == &quot;-total&quot;) %&gt;% ggplot(aes(x=week, y=excess_death, colour=country)) + geom_line() + coord_cartesian(ylim=c(-5000, NA)) + labs(x=&quot;Week&quot;, y=&quot;Excess death count&quot;, title=&quot;Excess mortality 2020&quot;, caption=&quot;Source: mortality.org&quot;, colour=&quot;&quot;) + theme_ipsum_rc() . . This figure further supports the claim that the reported mortality data of the USA is incomplete (it does not show the even larger negative excess in later weeks). Still, this figure makes it hard to compare the numbers between countries since we are looking absolute excess deaths. This is due to the fact that the total mortality of a country is higher for countries with larger populations such as Spain, England and the USA. However, these statistics do represent actual people and actual suffering. . Instead of the total excess we also compute a standardised excess death measure by dividing the excess deaths by the historic mortality data from 2015 to 2019. Furthermore, we remove the data from week 23 onwards for the USA, since this is clearly incomplete. . excess_deaths_filter_US &lt;- excess_deaths %&gt;% filter(!(country == &quot;United States&quot; &amp; week &gt;= 23)) excess_deaths_filter_US %&gt;% filter(sex == &quot;both&quot;, age_group == &quot;-total&quot;) %&gt;% ggplot(aes(x=week, y=excess_death_std, colour=country)) + geom_line() + scale_y_continuous(labels = scales::percent_format()) + labs(x=&quot;Week&quot;, y=&quot;Standardised excess death count&quot;, title=&quot;Standardised excess mortality 2020&quot;, caption=&quot;Source: mortality.org&quot;, colour=&quot;&quot;) + theme_ipsum_rc() . . This figure tells a very different story. While England is showing a high excess mortality in absolute numbers and relative numbers, preliminary data does paint a different picture for the USA. Smaller countries like Belgium, the Netherlands and Sweden do show up less favourable in this figure. As previous figures already suggested: Sweden and England are in the worst shape in recent weeks. Both countries had their peak at a later time and do not show the same large decline as other comparable European countries. . Differences between sexes and ages . Previous analyses have focused on differences between countries while considering the entire population including both sexes and all ages. By now, we learnt that men are not equally impacted by COVID-19 as women. Furthermore, COVID-19 seems to be especially gruesome for the elderly. . To get insights into we start by looking at the standardised excess deaths for both sexes for all populations combined. Note that the standardised excess deaths for all countries combined is a bit tricky. Simply adding all deaths together would heavily bias towards countries with high populations, while taking an average of the standardised excess deaths over countries dismisses the population size entirely. Despite these flaws let’s look at the average of the standardised excess deaths. . excess_deaths_sex &lt;- excess_deaths_filter_US %&gt;% group_by(sex, age_group, week) %&gt;% summarise(excess_death_std = mean(excess_death_std), n_countries = n()) %&gt;% ungroup() last_full_week &lt;- excess_deaths_sex %&gt;% filter(n_countries == max(n_countries)) %&gt;% .$week %&gt;% max excess_deaths_sex %&gt;% filter(sex %in% c(&quot;female&quot;, &quot;male&quot;), age_group == &quot;-total&quot;) %&gt;% ggplot(aes(x=week, y=excess_death_std, colour=sex)) + geom_line() + geom_vline(aes(xintercept=last_full_week), linetype=&quot;dashed&quot;) + scale_y_continuous(labels = scales::percent_format()) + scale_colour_ipsum() + labs(x=&quot;Week&quot;, y=&quot;Standardised excess death count&quot;, title=&quot;Excess mortality by sex 2020&quot;, caption=&quot;Source: mortality.org&quot;, colour=&quot;Sex&quot;) + theme_ipsum_rc() . . Interestingly, it seems that the excess mortality data for men and women look similar but are shifted by one week. The peak for men seems to start one week before women start showing an increase. This might explain the initial discrepancy between men and women as reported in March and April and the lack of reporting of this phenomonum in May. The dashed line represents the last week for which we have data of all 15 countries. Most noticeably, between week 19 and week 20 we observe an increase in the excess death count but this is because England is the only reporting country in week 20. . Now, let’s look at the differences between age groups. . excess_deaths_sex %&gt;% filter(sex == &quot;both&quot;, age_group %in% c(&quot;15-64&quot;, &quot;65-74&quot;, &quot;75-84&quot;, &quot;85+&quot;)) %&gt;% ggplot(aes(x=week, y=excess_death_std, colour=age_group)) + geom_line() + geom_vline(aes(xintercept=last_full_week), linetype=&quot;dashed&quot;) + scale_y_continuous(labels = scales::percent_format()) + scale_colour_ipsum() + labs(x=&quot;Week&quot;, y=&quot;Standardised excess death count&quot;, title=&quot;Excess mortality by age group 2020&quot;, caption=&quot;Source: mortality.org&quot;, colour=&quot;Age group&quot;) + theme_ipsum_rc() . . The data is quite telling: excess mortality increases with age. I omitted the data for the age group of 0-14, because of the large fluctuations in data due to low mortality numbers. . Lastly, we dissect the data by sex and age group to compare individual countries trends. Iceland is removed because of its small population size and therefore heavily fluctuating mortality data. . excess_deaths_filter_US %&gt;% filter(sex != &quot;both&quot;, age_group %in% c(&quot;15-64&quot;, &quot;65-74&quot;, &quot;75-84&quot;, &quot;85+&quot;), week &gt;= 10, country != &quot;Iceland&quot;) %&gt;% mutate(is_UK = if_else(country == &quot;England &amp; Wales&quot;, TRUE, FALSE)) %&gt;% ggplot(aes(x=week, y=excess_death_std, colour=is_UK, group=country)) + geom_line() + facet_grid(age_group ~ sex) + scale_y_continuous(labels=scales::percent_format()) + scale_x_continuous(breaks=seq(10, 20, 2)) + scale_colour_manual(values=colour_pal) + labs(x=&quot;Week&quot;, y=&quot;Standardised excess death count&quot;, title=&quot;Excess mortality dissected by sex and age&quot;, caption=&quot;Source: mortality.org&quot;, colour=&quot;Age group&quot;) + theme_ipsum_rc() + theme(legend.position = &quot;none&quot;) . . This figure is the most surprising to me. All others figures confirm from what I have read so far with increasing and then decreasing excess mortality and differences between countries. However, this figure shows a striking pattern for England. It seems that excess mortality is much larger for the age group 15-64 in England than any other observed country (top two figures). Another striking observation is that men excess death is much higher in England than other countries (right figures). . The last figure shows the importance of dissecting the data across multiple variables to find patterns that otherwise would be hidden do to aggregation (see Simpson Paradox). . Conclusion . Thank you for reading this initial analysis of the new dataset as provided by the excellent HMD project. Stay safe out there! .",
            "url": "https://mvanderbroek.com/r/covid-19/2020/05/26/Excess-Mortality-in-2020-(COVID-19).html",
            "relUrl": "/r/covid-19/2020/05/26/Excess-Mortality-in-2020-(COVID-19).html",
            "date": " • May 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic Regression In Pytorch From Scratch (lesson 2)",
            "content": "In this article we will explore the logistic regression and how we can implement it using PyTorch. Contrary to linear regressions where the output variable $y$ is continuous, logistic regressions concern binary variables, i.e., . y={1,with probability p,0,with probability 1−p.y = begin{cases} 1, &amp; text{with probability }p, 0, &amp; text{with probability }1-p. end{cases}y={1,0,​with probability p,with probability 1−p.​ . We are interested in modelling the conditional probability of $y = 1$ given $ boldsymbol{x}$, i.e., . p=P(y=1∣X;b)=F(XTb).p = P(y = 1 mid boldsymbol{X}; boldsymbol{b}) = F( boldsymbol{X}^T boldsymbol{b}).p=P(y=1∣X;b)=F(XTb). . As an example, we might want to find the probability of a patient having cancer ($y=1$) given the patient’s medical information ($x$). . Note: Now take a moment to think about the following: what properties do we want $F$ to have? . Since we want to model conditional probabilities, we want $F$ to map to the domain $[0, 1]$. It now happens that the sigmoid function, let’s call it $h$, has some very nice properties. It is defined as . h(x)=11+e−x.h(x) = frac{1}{1 + e^{-x}}.h(x)=1+e−x1​. . Ideally, we want $P(y = 1 mid boldsymbol{X}; boldsymbol{b})$ to be close to 1 when $Y$ is 1 and close to 0 when $Y$ is 0. Before explaining how we can find parameters $b$ such that we come closest to the this ideal situation, let’s generate some random data. . %matplotlib inline from fastai.basics import * . n = 100 . Let’s first create a sample of our $ boldsymbol{X}: n times 2$ feature matrix. . x = torch.ones(n, 2) x[:,0].normal_(1, 0.2) x[:,1].normal_(5, 1.) x[:5,:] . tensor([[1.1678, 6.1665], [0.9792, 5.8922], [1.3373, 4.1348], [0.8830, 6.4242], [1.0932, 5.6926]]) . Next, we want to sample our latent variable $ boldsymbol{y}^*$ as . y∗=XTb+ε, boldsymbol{y}^* = boldsymbol{X}^T boldsymbol{b} + boldsymbol{ varepsilon},y∗=XTb+ε, . where $ boldsymbol{b} = [5, -1]$ and $ boldsymbol{ varepsilon} sim text{Logistic}(0, 1)$ . b = tensor(5,-1.) # Create logistic distribution in pytorch using inverse CDF sampling base_distribution = torch.distributions.Uniform(0, 1) transforms = [torch.distributions.SigmoidTransform().inv, torch.distributions.AffineTransform(loc=0, scale=1)] logistic = torch.distributions.TransformedDistribution(base_distribution, transforms) # Take sample of errors and compute y_star error = logistic.rsample([n]) y_star = x@b + error . The dependent variable can be computed as . y={1,if y∗&gt;0,0,else.y = begin{cases} 1, &amp; text{if } y^* &gt; 0, 0, &amp; text{else.} end{cases}y={1,0,​if y∗&gt;0,else.​ . This relates to the probability $p$ as follows . P(y=1)=P(y∗&gt;0)=P(xTb+ε&gt;0)=P(ε&gt;−xTb)=P(ε≤xTb) (logistic regression is symmetric)=F(XTb)=p. begin{aligned} P(y = 1) &amp;= P(y^* &gt; 0) &amp; &amp;= P( boldsymbol{x}^T boldsymbol{b} + varepsilon &gt; 0) &amp; &amp;= P( varepsilon &gt; - boldsymbol{x}^T boldsymbol{b}) &amp; &amp;= P( varepsilon leq boldsymbol{x}^T boldsymbol{b}) &amp; text{ (logistic regression is symmetric)} &amp;= F( boldsymbol{X}^T boldsymbol{b}) = p. end{aligned}P(y=1)​=P(y∗&gt;0)=P(xTb+ε&gt;0)=P(ε&gt;−xTb)=P(ε≤xTb)=F(XTb)=p.​ (logistic regression is symmetric)​ . y = y_star &gt; 0 . Linear regression . Let’s check what happens if we now try to model the relationship between the conditional probability and $ boldsymbol{X}$ as linear, i.e., . p=XTb+ε,ε∼(0,σ2).p = boldsymbol{X}^T boldsymbol{b} + boldsymbol{ varepsilon}, quad boldsymbol{ varepsilon} sim (0, sigma^2).p=XTb+ε,ε∼(0,σ2). . Note that although we have no guarantee that $p$ lies in the interval $[0,1]$, this rarely happens. A bigger problem is the heteroskedasticity of the error term. The linear model assumes the errors are homoskedastic, but it is possible to incoorporate heteroskedastic errors by estimating white standard errors. . Since we are dealing with a linear model, we do not need gradient descent and we can compute the MLE estimator in one line. The OLS (and MLE) estimator for $ boldsymbol{b}$ is given by . bMLE=(XTX)−1XTy. boldsymbol{b}_ text{MLE} = ( boldsymbol{X}^T boldsymbol{X})^{-1} boldsymbol{X}^T boldsymbol{y}.bMLE​=(XTX)−1XTy. . This equation follows immediately from the first order condition of the mean-squared error of the model, i.e., . 0=∂(y−XTb)T(y−XTb)∂b=∂yTy∂b−∂2bTXTy∂b+XTXb∂b=−2XTy+2XTXb. begin{aligned} 0 = frac{ partial ( boldsymbol{y} - boldsymbol{X}^T boldsymbol{b})^T( boldsymbol{y} - boldsymbol{X}^T boldsymbol{b})}{ partial boldsymbol{b}} &amp;= frac{ partial boldsymbol{y}^T boldsymbol{y}}{ partial boldsymbol{b}} - frac{ partial 2 boldsymbol{b}^T boldsymbol{X}^T boldsymbol{y}}{ partial boldsymbol{b}} + frac{ boldsymbol{X}^T boldsymbol{X} boldsymbol{b}}{ partial boldsymbol{b}} &amp;= -2 boldsymbol{X}^T boldsymbol{y} + 2 boldsymbol{X}^T boldsymbol{X} boldsymbol{b}. end{aligned}0=∂b∂(y−XTb)T(y−XTb)​​=∂b∂yTy​−∂b∂2bTXTy​+∂bXTXb​=−2XTy+2XTXb.​ . # Let&#39;s compute the MLE linear regressor b_linear = torch.inverse(x.T@x)@x.T@y.float() y_linear = x@b_linear y_linear_hat = (y_linear &gt; 0.5).float() fig, ax = plt.subplots() ax.scatter(x[:,1], y.float(), label=&#39;y&#39;) ax.scatter(x[:,1], y_linear, label=&#39;y_ols&#39;) leg = ax.legend(); . . Indeed, we observe that almost all observations lay between the $[0, 1]$ interval. . Logistic regression . Unlike the linear regression, the logistic regression has no closed-form solution. The most popular way of estimating the parameters $ boldsymbol{b}$ is to estimate the maximum likelihood estimator: . LL(b;X,y)=∏i=1NP(Y=1∣xi;b)yi(1−P(y=1∣xi;b))1−yi=∏i=1Nh(xiTb)yi(1−h(xiTb))1−yi. begin{aligned} LL( boldsymbol{b}; boldsymbol{X}, boldsymbol{y}) &amp;= prod_{i=1}^N P(Y = 1 mid boldsymbol{x}_i; boldsymbol{b})^{y_i} (1 - P(y = 1 mid boldsymbol{x}_i; boldsymbol{b}))^{1-y_i} &amp;= prod_{i=1}^N h( boldsymbol{x}_i^T boldsymbol{b})^{y_i} (1 - h( boldsymbol{x}_i^T boldsymbol{b}))^{1-y_i}. end{aligned}LL(b;X,y)​=i=1∏N​P(Y=1∣xi​;b)yi​(1−P(y=1∣xi​;b))1−yi​=i=1∏N​h(xiT​b)yi​(1−h(xiT​b))1−yi​.​ . Remember that we wanted $P(y = 1 mid boldsymbol{x}_i; boldsymbol{b})$ to be close to one when $y_i = 1$. If that’s our goal, it means that we want $LL( boldsymbol{b}; boldsymbol{X}, boldsymbol{y})$ to be as large as possible. In other words, we want to find a $ boldsymbol{b}$ such that $LL( boldsymbol{b}; boldsymbol{X}, boldsymbol{y})$ is maximised. . Since computers do not like the product of many numbers between $[0, 1]$ because it results in floating point problems (why would that be?). Therefore, we take the log of the likelihood function. Since the log is a monotonic function, maximising the likelihood is the same as maximising the log-likelihood. Because our objective is now additive, a last trick that we can use is to divide this log-likelihood by the sample size. This gives . ll(b;X,y)=1N∑i=1Nyilog⁡(h(xiTb))+(1−yi)log⁡(1−h(xiTb)).ll( boldsymbol{b}; boldsymbol{X}, boldsymbol{y}) = frac{1}{N} sum_{i=1}^N y_i log(h( boldsymbol{x}_i^T boldsymbol{b})) + (1-y_i) log(1-h( boldsymbol{x}_i^T boldsymbol{b})).ll(b;X,y)=N1​i=1∑N​yi​log(h(xiT​b))+(1−yi​)log(1−h(xiT​b)). . In PyTorch we can simply formulate this as . def ll(x, y, b): return((1/len(y)) * (torch.log(torch.sigmoid(x@b)[y]).sum() + torch.log(1 - torch.sigmoid(x@b)[~y]).sum())) . Like I mentioned before, this (log-)likelihood function does not have a closed-form solution (check it if you want :)). Therefore, we will apply the (stochastic) gradient descent algorithm to train our model. . b = tensor(b_linear) b = nn.Parameter(b) def update(): loss = ll(x, y, b) loss.backward() if t % 10 == 0: print(loss) with torch.no_grad(): print(b.grad) b.sub_(-lr * b.grad) b.grad.zero_() . lr = 1e-1 for t in range(10): update() print(b) . tensor(-0.6698, grad_fn=&lt;MulBackward0&gt;) tensor([-0.1090, -0.7824]) tensor([-0.0060, -0.2512]) tensor([ 0.0269, -0.0826]) tensor([ 0.0369, -0.0312]) tensor([ 0.0400, -0.0153]) tensor([ 0.0409, -0.0104]) tensor([ 0.0412, -0.0088]) tensor([ 0.0412, -0.0083]) tensor([ 0.0412, -0.0082]) tensor([ 0.0411, -0.0081]) Parameter containing: tensor([ 1.3062, -0.2848], requires_grad=True) . y_log = torch.sigmoid(x@b) y_log_hat = (y_log &gt; 0.5).float() fig, ax = plt.subplots() ax.scatter(x[:,1], y.float(), label=&#39;y&#39;) ax.scatter(x[:,1], y_log, label=&#39;y_logistic&#39;) leg = ax.legend(); . . Evaluating the linear and logistic classification models . The most basic tool to evaluate the performance of our classification models is to compute the accuracy. The accuracy is the percentage of correctly predicted labels $ boldsymbol{y}$. For our linear model we have . (y_linear_hat == y.float()).sum().float() * 100 / len(y) . tensor(75.) . and our logistic model has an accuracy of . (y_log_hat == y.float()).sum().float() * 100 / len(y) . tensor(72.) . Conclusion . In this post we learnt about the one of the oldest techniques to model binary outcome variables: the logistic regression. We discussed the theoretical foundation, an alternative modelling technique in the form of a linear model and we learnt how can train a logistic regression from scratch in PyTorch. . After writing this post I realised that we can easily see how the logistic regression fits into the framework we discussed in the previous post with the three fundamental building blocks. . The architecture $f$ that maps input $ boldsymbol{X}$ to outputs $ boldsymbol{y}$ with parameters $ boldsymbol{ theta}$ is in this case the sigmoid function with parameters; | the learning algorithm is a simple (stochastic) gradient descent; | and the objective function is the likelihood function. | See you for lesson 3 of the fast.ai course! .",
            "url": "https://mvanderbroek.com/fast.ai/deep%20learning/machine%20learning/2019/11/08/Logistic-Regression-in-PyTorch-from-Scratch-(Lesson-2).html",
            "relUrl": "/fast.ai/deep%20learning/machine%20learning/2019/11/08/Logistic-Regression-in-PyTorch-from-Scratch-(Lesson-2).html",
            "date": " • Nov 8, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Common Deep Learning Training Pitfalls (lesson 2)",
            "content": "In lesson 2 of the fast-ai course discusses a variety of items. First, we will learn how to apply a model in production. Second, common pitfalls are being discussed. I will provide my econometric background on these pitfalls and how they relate to the statistics literature. Lastly, we will explore the inner working of most ML/DL algorithms by discussing the most elementary one: the logistic regression (LATER POST). . Airplane classifier in production . After our model is trained, it can be used in practice. Often, this is done by creating a public API. This is often done by creating a website where users can submit their ‘item’ that they want to analyse. The website then predicts the class using the trained model and gives back this class (and corresponding probabilities) to the user. Let’s see how this is done. . First, we export our model so that we can load it into our website. . from fastai.vision import * #learn corresponds to the best model of lesson 1 #learn.export(&#39;classifier.pkl&#39;) . path = &#39;./&#39; img = open_image(path + &#39;A320_easyJet_new_livery_2015.jpeg&#39;) img . . learn = load_learner(path, &#39;plane-classifier.pkl&#39;) pred_class, _, _ = learn.predict(img) print(pred_class) . airbus . Although our model does not have the highest accuracy, we do manage to obtain the right class for this photo but note that a model that we randomly give back a class would also be right 50% of the time for any given photo… . To make this model available on a website you can use a service like Render or create a simple flask application. The fast-ai course provides several ways to put a model into production, e.g., click here. . Common pitfalls . Introduction . Deep learning models consists of three fundamental parts: architecture, learning process and objection function. The pitfalls in lesson 2 mostly concern the learning process but note that these parts are not fully decoupled: they are all related. . Let’s consider a function $f$ that maps an input matrix $ boldsymbol{X}$ to an output vector $ boldsymbol{y}$ using parameters $ boldsymbol{ theta}$, i.e., . y=f(θ;X). boldsymbol{y} = f( boldsymbol{ theta}; boldsymbol{X}).y=f(θ;X). . This type of problem of mappping an input to an output is called supervised learning. We want to have a function with parameters $ boldsymbol{ theta}$ that provides an output vector $ hat{ boldsymbol{y}}$ that most closely resembles the real output vector $ boldsymbol{y}$. The question raises, how to judge that our model is “good”? This is where the objective function comes in. This function, let’s call it $c$, takes the predicted outputs $ hat{ boldsymbol{y}}$ and the real outputs $ boldsymbol{y}$ and tells us how “close” the values are. Most objective functions are defined on the interval between 0 and infinity and outputs a low number when the predicted and real values are close and a large value when they are far apart. An example of such a cost function is the mean-squared error function, i.e., . cMSE(y^,y)=∑i(y^i−yi)2.c_ text{MSE}( hat{ boldsymbol{y}}, boldsymbol{y}) = sum_i ( hat{y}_i - y_i)^2.cMSE​(y^​,y)=i∑​(y^​i​−yi​)2. . Fundamental building blocks . Remember that $ boldsymbol{y}$ and $ boldsymbol{X}$ consistute the data we have and $f$ with parameters θ boldsymbol{ theta}θ and the objective function $c$ are the modelling choices. The three main questions for any deep/machine learning problem are: . What architecture $f$ works for our data? | How do we learn the parameters $ boldsymbol{ theta}$? | What defines a good model? | Lesson 2 of the fast-ai course mainly concerns problems with question 2: it is about the learning process. Almost all research and applications are based on so-called gradient-based optimisation. The comes from the idea that any function $f$ at point $ boldsymbol{x}$ can be written as the taylor series expansion, i.e., . f(x)=f(x0)+(x−x0)T∂f(x0)∂x+12(x−x0)T∂2f(x0)∂x2(x−x0)+…f( boldsymbol{x}) = f( boldsymbol{x}_0) + ( boldsymbol{x} - boldsymbol{x}_0)^T frac{ partial f( boldsymbol{x}_0)}{ partial boldsymbol{x}} + frac{1}{2}( boldsymbol{x} - boldsymbol{x}_0)^T frac{ partial^2 f( boldsymbol{x}_0)}{ partial boldsymbol{x}^2}( boldsymbol{x} - boldsymbol{x}_0) + dotsf(x)=f(x0​)+(x−x0​)T∂x∂f(x0​)​+21​(x−x0​)T∂x2∂2f(x0​)​(x−x0​)+… . . Note: the link between this function $f( boldsymbol{x})$ to our deep learning framework is that given data $( boldsymbol{X}, boldsymbol{y})$ we want to optimise the objection function $c$ using architecture fff by selecting the best parameters $ boldsymbol{ theta}$, i.e. we can write the correspondence . f(x)  ⟺  c(θ)=c(f(θ;X),y).f( boldsymbol{x}) iff c( boldsymbol{ theta}) = c(f( boldsymbol{ theta}; boldsymbol{X}), boldsymbol{y}).f(x)⟺c(θ)=c(f(θ;X),y). . . The idea of gradient-based optimisation is to “improve” our function fff by changing the values of $ boldsymbol{x}$ slightly at every step. If $ boldsymbol{x}_0$ is the current value then we compute the gradient $ boldsymbol{g}$ of $f$ at point $ boldsymbol{x}_0$ and take a small step in this direction, i.e., . x1=x0−ϵg. boldsymbol{x}_1 = boldsymbol{x}_0 - epsilon boldsymbol{g}.x1​=x0​−ϵg. . If we fill this into the equation (1) we get . f(x1)=f(x0)−ϵgTg+12ϵ2gTHg+…,f( boldsymbol{x}_1) = f( boldsymbol{x}_0) - epsilon boldsymbol{g}^T boldsymbol{g} + frac{1}{2} epsilon^2 boldsymbol{g}^T boldsymbol{H} boldsymbol{g} + dots,f(x1​)=f(x0​)−ϵgTg+21​ϵ2gTHg+…, . where $ boldsymbol{H} = frac{ partial^2 f( boldsymbol{x}_0)}{ partial boldsymbol{x}^2}$. The idea of gradient-based optimisation comes from the fact that for small values of ϵ epsilonϵ the last term diminishes so that we can write . f(x1)≈f(x0)−ϵgTg.f( boldsymbol{x}_1) approx f( boldsymbol{x}_0) - epsilon boldsymbol{g}^T boldsymbol{g}.f(x1​)≈f(x0​)−ϵgTg. . Since $ boldsymbol{g}^T boldsymbol{g} geq 0$ by definition, we have that fff decreases when we take small steps along the gradient. Hence, we iteratively improve our objective function. . Jeremy Howard from the fast-ai course discusses some common pitfalls in learning the model parameters. I will discuss these pitfalls using the knowledge of the gradient-based optimisation that we just learnt. . Learning rate too high . Although state-of-the-art models do not directly use the gradient to improve the objective function it is closely related. In our gradient-descent update step in (3) the learning rate refers to ϵ epsilonϵ. Before discussing why a high ϵ epsilonϵ is bad, let’s consider this figure. . . In the plot on the right we see what happens when the learning rate is too high. The reason for this divergence is that our approximation only works when epsilon is small. If epsilon gets larger, the last term of equation (2) and higher order terms generally are no longer close to zero. Hence, the true value $f( boldsymbol{x}_1)$ might be larger than $f( boldsymbol{x}_0)$ resulting in the behaviour seen in the right graph. . Learning rate too low . If the learning rate is very low, then the gradient-based optimisation and its first-order linear approximation as in (3) accurately model the cost function $c$ close to the last parameter values $ boldsymbol{ theta}$. However, if the learning rate is too low, it will take many epochs to reach some kind of local minimum. Furthermore, if our function is not globally convex, we might never reach a good local minimum since we cannot escape the current path to the nearest local minimum. . Too few epochs . One problem with too few epochs relates to the figure above. In the left graph, we see that when the learning rate is too low, then it takes many epochs to reach the minimum. Hence, if we set the number of epochs too low, we will not come close to this minimum. . Another problem with too few epochs arises when the objective function is no longer globally convex (which it never is!!). Consider the following figure which shows the “surface” of an objective function with two parameters, say $ theta_0$ and $ theta_1$. . . If we start with randomly selected paramater values it is unlikely that we will ever reach the global minimum of the above objective function. There are ways to deal with this, such as more advanced learning algorithms instead of our basic gradient-descent algorithm in (3) (which still consistutes the basis of all these algorithms). However, if we only allow for a small number of epochs, the learning process does not have the opportunity to explore many regions of the objective surface. . Too many epochs . Too many epochs is different from the other pitfalls we have seen so far. While the previous pitfalls relate to finding parameters $ boldsymbol{ theta}$ that minimises the objective function, too many epochs relates to a problem concerning the data. . When we train our model, we only have access to a limited (although often large) sample of observations. When we train our model on this training data with many epochs using a good learning algorithm we might be able to find a very good local minimum of the objective function. This would correspond to the right graph of the following figure . . Our mean squared error is almost zero. Excellent! Now, what happens when we obtain a new sample and check the objective function under this new data? It is likely that the model on the middle now outperforms the model on the right, since it captures the real relationship between the input data X boldsymbol{X}X and the output $ boldsymbol{y}$ better than the overfitted model. . One remark related to modern model architectures: models are typically so large (in the number of parameters) that it is almost impossible to overfit the model as in the above figure. It would simply require too many epochs and a very good learning algorithm to ever overfit like this. . Conclusion . We learnt how to export our model and call our model on a new observation to predicts its class. In the second part, we discussed a very high-level overview of machine/deep learning models and its fundamental building blocks, namely architecture, learning algorithm and objective function. Lastly, I discussed common pitfalls with regards to training a deep learning network. .",
            "url": "https://mvanderbroek.com/fast.ai/deep%20learning/2019/11/07/Common-Deep-Learning-Training-Pitfalls-(Lesson-2).html",
            "relUrl": "/fast.ai/deep%20learning/2019/11/07/Common-Deep-Learning-Training-Pitfalls-(Lesson-2).html",
            "date": " • Nov 7, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Airplane Classifier With Fast.ai Library (lesson 1)",
            "content": "The fast.ai deep learning course is a practical top-down deep learning course for practicioners. Immediately after the first two hour lecture, it is possible to train an image classifier on your own dataset using state-of-the-art deep learning techniques. The reason for this accesibility is the excellent fast.ai software which builds upon the popular PyTorch deep learning library. . In this post I will show you how I applied the techniques and lessons from the first fast.ai lecture to a plane classifier. I will show you how to download your own dataset, how to train an image classifier using the fast.ai software, and finally how to evaluate the performance of the trained model. . Create Data . The easiest way to create your own dataset for your image classifier is to follow the instructions of the fast.ai notebook which can be found here. Using these instructions I downloaded images of the boeing 737 and airbus A320 airplanes. While, for most people these planes are indistinguisable from each other, there are small differences between the planes. These include the shape of the nose, the wing tips and the position of the engines under the wings. . After downloading the images in their subfolders data/boeing and data/airbus I wanted to split this data into a training, validation and testing set. Since I could not find a simple tool to just give my folder of images and do this split for me, I created a little tool. This creates three folders, data/train, data/validation and data/test. These folders then contain the subfolders corresponding to the classes found in the original data folder, namely, boeing and airbus. . Load Data . We first import some functions from the fast.ai library that we will use for our analysis. . %reload_ext autoreload %autoreload 2 %matplotlib inline . from fastai.vision import * from fastai.metrics import error_rate from fastai.callbacks.tracker import SaveModelCallback . Now, we will select our folder containing the images and load it into the fast.ai library. . help(get_image_files) path = &#39;/home/jupyter/data&#39; . Help on function get_image_files in module fastai.vision.data: get_image_files(c: Union[pathlib.Path, str], check_ext: bool = True, recurse=False) -&gt; Collection[pathlib.Path] Return list of files in `c` that are images. `check_ext` will filter to `image_extensions`. . data = ImageDataBunch.from_folder(path, valid=&#39;validation&#39;, ds_tfms=get_transforms(), size=224).normalize(imagenet_stats) . Let’s check out the sizes of our training and validation set and load a small sample of our images. . data.classes, data.c, len(data.train_ds), len(data.valid_ds) . ([&#39;airbus&#39;, &#39;boeing&#39;], 2, 245, 80) . data.show_batch(rows=3, figsize=(7,8)) . . Train ResNet 34 model . To train our plane classifier we use the state-of-the-art ResNet34 model as our basis. We first try to see what accuracy we can obtain by training the last layers. As we will see, this gives unsatisfactory results. We can simply load the ResNet34 model directly from the fast.ai library and start training it with only two lines of code! . learn = cnn_learner(data, models.resnet34, metrics=[error_rate, accuracy]) . learn.fit_one_cycle(10, callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;accuracy&#39;, name=&#39;model&#39;)]) . epoch train_loss valid_loss error_rate accuracy time . 0 | 1.411946 | 1.175786 | 0.437500 | 0.562500 | 00:06 | . 1 | 1.193475 | 0.849596 | 0.450000 | 0.550000 | 00:04 | . 2 | 1.086779 | 0.961117 | 0.412500 | 0.587500 | 00:04 | . 3 | 0.996292 | 1.146309 | 0.375000 | 0.625000 | 00:05 | . 4 | 0.925049 | 0.991192 | 0.375000 | 0.625000 | 00:04 | . 5 | 0.856052 | 0.941140 | 0.325000 | 0.675000 | 00:04 | . 6 | 0.813291 | 0.995522 | 0.350000 | 0.650000 | 00:04 | . 7 | 0.752976 | 1.061707 | 0.350000 | 0.650000 | 00:04 | . 8 | 0.715641 | 1.104084 | 0.375000 | 0.625000 | 00:05 | . 9 | 0.676570 | 1.111519 | 0.375000 | 0.625000 | 00:04 | . Clearly our model is still very bad. We might want to try to ‘unfreeze’ our model to train all layers instead of the last ones. Furthermore, to speed-up training, we can fix the learning rate of the learning algorithm by monitoring which rates give the best training results. Another trick to obtain a good model is to use early-stopping. Early-stopping helps us with overfitting by recognising when the training loss keeps decreasing but the validation loss no longer decreases. This happens when we overfit our model and optimise our model for the training set and therefore results in worse generalisation performance as can be seen in the rise of validation loss. . learn.unfreeze() learn.lr_find() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . . learn.fit_one_cycle(13, max_lr=slice(1e-4,1e-2), callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;accuracy&#39;, name=&#39;model&#39;)]) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.393597 | 1.036158 | 0.275000 | 0.725000 | 00:05 | . 1 | 0.455709 | 2.487670 | 0.437500 | 0.562500 | 00:05 | . 2 | 0.508346 | 4.054496 | 0.512500 | 0.487500 | 00:05 | . 3 | 0.625813 | 3.290894 | 0.375000 | 0.625000 | 00:05 | . 4 | 0.735456 | 6.999222 | 0.550000 | 0.450000 | 00:05 | . 5 | 0.684926 | 3.586192 | 0.450000 | 0.550000 | 00:05 | . 6 | 0.622755 | 2.036758 | 0.425000 | 0.575000 | 00:05 | . 7 | 0.582402 | 1.667793 | 0.312500 | 0.687500 | 00:05 | . 8 | 0.524937 | 1.206267 | 0.300000 | 0.700000 | 00:05 | . 9 | 0.476152 | 0.784825 | 0.225000 | 0.775000 | 00:05 | . 10 | 0.437106 | 0.674155 | 0.200000 | 0.800000 | 00:05 | . 11 | 0.402961 | 0.615875 | 0.175000 | 0.825000 | 00:05 | . 12 | 0.368615 | 0.577250 | 0.175000 | 0.825000 | 00:05 | . Better model found at epoch 0 with accuracy value: 0.7250000238418579. Better model found at epoch 9 with accuracy value: 0.7749999761581421. Better model found at epoch 10 with accuracy value: 0.800000011920929. Better model found at epoch 11 with accuracy value: 0.824999988079071. . learn.save(&#39;model-1&#39;) . Interpret results . Now that we have trained our model we want to do some inferences. Luckily, fast.ai got us covered. The ClassificationInterpration class provides many handy tools to check the performance of our model. Let’s check out the confusion matrix to see where things go wrong. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . While we do manage to classify many planes correctly, I am curious on what images our model has problems. We can check this out as follows. . interp.plot_top_losses(9, figsize=(15,11)) . . Mmm… personally it is unclear to me why the model has problems identifying these images. In the above we only see the images in square format. I am wondering whether the model is also trained on these squared images or that it uses the original typically horizontal images. . Conclusion . While our model has okay performance. I am not sure what the model has learnt exactly. . There are several things I am interested in learning on my fast-ai and deep learning journey in general: . Is there a way to tell what our model has learnt? For our plane example, the difference between the boeing and airbus planes is mostly in the wing design and shape of the nose of the plane. Is there a way to check whether our model has been able to learn these features? . | So far it has not been clear to me how image classifiers deal with non-square images. Does it crop the images in to use in the model or does the ResNet model allow for variable sized images? . | How do you decide to use a certain transformation of the image data. Is it always okay to augment our dataset by adding transformation of our image data like turning or mirroring images. . | . I hope to see you in the next post about fast.ai! .",
            "url": "https://mvanderbroek.com/fast.ai/deep%20learning/2019/11/06/Airplane-Classifier-with-Fast.ai-library-(Lesson-1).html",
            "relUrl": "/fast.ai/deep%20learning/2019/11/06/Airplane-Classifier-with-Fast.ai-library-(Lesson-1).html",
            "date": " • Nov 6, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Custom Data Splitter Function In Python",
            "content": "Ths month I started with the excellent Deep Learning course by fast.ai and I just finished the first lesson. At the end of the lesson Jeremy, the teacher of the course, assigned a homework assignment to create an image classifier using our own data. After downloading my own dataset I was facing the problem that I wanted to split this dataset into a training and test set using seperate directories but I could not easily find such a program online. Therefore, in this blog post, I will explain how to write such a program and how to make this program easily installable and useable for anyone. I will discuss how to properly handle arguments for command line programs and how to make your software available to download from pip. . Motivation . Most of these image classifier tutorials use a dataset that correctly classifies most of the images using state-of-the-art image classifier models. However, I was curious how these so-called transfer learning techniques would work on a rather challenging dataset. Using a google image downloader, I obtained a dataset with approximately 1,000 Airbus A320 planes and 1,000 Boeing 737s. For a layman, these planes are identical but there are small differences. . The problem I was facing is that created a directory plane_data and two subdirectories airbus and boeing containing the respective image files but I wanted to split this dataset into a training and test dataset. While it is relatively straight-forward to split a data file, e.g., CSV files containing one observation per row, splitting directories containing image files is less straightforward. Hence, I decided to write a little terminal program in python to do the manual work for me. . The Program . The goal of the program is split a directory of images and split it into a training and test directory. . Let’s call our directory with images data with classes boeing and airbus. The two most used ways of saving images beloning to a certain class is to put them in a seperate subdirectory, e.g., data/boeing or to put the class name in the name of the image file, e.g., boeing_img1.jpeg. As of now, I only implemented the former case but the second case is typically implemented using class detection with regex. . The source code of the program is given below. I think the code is pretty self-explanatory but the idea is to 1) identify the subdirectories containing all classes, 2) create a data/train and data/test subdirectory and 3) loop over all classes and split the image files in a train and test selection and move the images to the respective directories, e.g., data/train/boeing and data/test/boeing. Note that the folder variable is the location of the images and the train variable is the percentage of images that we want to keep for training, which is defined as a program option. . def data_splitter(folder, train): entries = [item for item in listdir(folder) if not item.startswith(&#39;.&#39;)] # Get all folders and files dirs = [d for d in entries if isdir(join(folder, d))] files = [f for f in entries if isfile(join(folder, f))] if dirs and files: print(&quot;Folder should contain either files or folders but not both.&quot;) sys.exit() mkdir(join(folder, &#39;train&#39;)) mkdir(join(folder, &#39;test&#39;)) if files: dirs = [&quot;&quot;] for directory in dirs: # Items belonging to the current class items = [item for item in listdir(join(folder, directory)) if not item.startswith(&#39;.&#39;) and isfile(join(folder, directory, item))] if directory != &quot;&quot;: mkdir(join(folder, &#39;train&#39;, directory)) mkdir(join(folder, &#39;test&#39;, directory)) # Shuffle and split dataset according to fractions random.shuffle(items) train_sel = int(len(items) * (train / 100)) train_entries = items[0:train_sel] test_entries = items[train_sel:] for train_file in train_entries: shutil.move(join(folder, directory, train_file), join(folder, &#39;train&#39;, directory, train_file)) for test_file in test_entries: shutil.move(join(folder, directory, test_file), join(folder, &#39;test&#39;, directory, test_file)) if directory != &quot;&quot;: rmdir(join(folder, directory)) . Handling Arguments . The program needs a folder to split up and an optional percentage of image files to put in the training set. While python offers an in-built argument parser in the argparser module, the click module has my preference due to its easy of use. . Adding arguments and options is as simple as adding . @click.command() @click.argument(&#39;folder&#39;) @click.option(&#39;--train&#39;, default=80, help=&quot;Percentage of files for train set&quot;) . In front of our data_splitter(folder, train) program. A click.option allows us to define a default value in case the user does not define the option and a little helper text when the user calls the --help option. . Deploying on pip . Lastly, we want to make it dead-easy to install and use our little command-line program. The most popular method to distribute software in python is to use pip. To make our module suitable for pip, we need to add two files to our module in the main directory: a setup.py and setup.cfg file. An example can be found here. . We further add the following to the data_splitter.py code to make the program easily calleable from pip as we will see in the next section. . if __name__ == &#39;__main__&#39;: try: data_splitter() except FileNotFoundError as fnf_error: print(fnf_error) . The try-except block prevents us from having an ugly and long error callback in case we want to split a non-existing directory. . Using data_splitter . Now that we defined the program, handled program arguments and made our program deployable on pip we can start using it. . The program can be found in the directory https://github.com/markkvdb/data-splitter and can be installed as . pip install git+https://github.com/markkvdb/data-splitter.git#egg=data-splitter . The installer will place the program location in the $PATH variable of your system. . After only adding three lines of code using the click module, we now have a very neat interface when using the program in the command-line. Calling data_splitter --help will show how to use it . Usage: data_splitter [OPTIONS] FOLDER Options: --train INTEGER Percentage of files for train set. --help Show this message and exit. . Improvements . The simple command-line can be improved in various ways: . Implement class recognition using image file names with regex; | Splitting is performed by randomly assigning images to the test or training set. This is not always appriorate if some images belong to the same individual and images are no longer independent. | .",
            "url": "https://mvanderbroek.com/python/2019/10/31/Custom-Data-Splitter-Function-in-Python.html",
            "relUrl": "/python/2019/10/31/Custom-Data-Splitter-Function-in-Python.html",
            "date": " • Oct 31, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Hidden Markov Model Tutorial In R",
            "content": "This document contains an introduction to Hidden Markov Models (HMMs). First, a brief description and the main problems of HMMs will discussed. After, I will provide common strategies to analyse these problems. Lastly, I apply the HMM framework on a speech recognition problem. . Model Formulation . A HMM models a Markov process which affects some observerable process(es). Any HMM model can be defined with 5 elements, namely: . The set of $$N$ hidden states $V = {v_1, dots, v_N}$; | The transition matrix $Q$ where the $i,j$ -th element represents the transition probability of going from hidden state $x_i$ to $x_j$; | A sequence of $T$ observations $Y ={y_1, dots, y_T}$, each drawn from observation set $D ={d_1, dots, d_d}$; | Functions $b_i(y_t)$ that contain the probability of particular observation at time $t$ given that the process is in state $v_i$. The entire set of functions is denoted by $B ={b_j( cdot): forall j in [N]}$; | The initial hidden state probabilities for time $t=0$: $ pi = [ pi_1, dots, pi_N]$. | We indicate $ lambda$ as a short-hand notation for the complete set of HMM parameters, i.e., . λ=(Q,B,π) lambda = (Q, B, pi)λ=(Q,B,π). . The three main problems associated with HMMs are: . Find $P(Y mid lambda)$ for some observation sequence $Y = (y_1, dots, y_T)$. | Given some $Y$ and $ lambda$, find the best (hidden) state sequence $X = (x_1, dots, x_T)$. | Find the HMM parameters that maximises $P(Y mid lambda)$, i.e., find $ lambda^* = text{argmax}_{ lambda}P(Y mid lambda)$. | In the remainder of this article I will provide approaches to solve each of these problems and provide an implementation in R. Before discussing an interesting application for HMMs, I will provide a very simple HMM to discuss the three main problems for clarity. . This simple HMM example in R is given below: . # Define model V = c(&quot;HOT&quot;, &quot;COLD&quot;) Q = matrix(c(0.7, 0.3, 0.4, 0.6), nrow=2, byrow=TRUE) D = c(1, 2, 3) Y = c(1, 3, 2, 3) B = matrix(c(0.2, 0.4, 0.4, 0.6, 0.3, 0.1), nrow=3) pi = c(0.5, 0.5) . Forward probabilities . One interesting problem for HMMs is determining the likelihood of a given sequence of observations given the HMM parameters $ lambda$. As opposed to regular Markov models, this is not straight-forward to compute, since we do not know the underlying hidden state sequence. . One possible solution would be to compute the likelihood of a given observation sequence by all possible hidden state sequences that support this observation sequene. In our toy model, the observation space does not depend on the hidden state, hence all sequences of hidden states have to be considered. . This method is commonly referred to as the law of total expectations or Tower rule. The idea is that we compute $P(Y)$ by using: . P(Y∣λ)=∑x∈XP(Y∣x,λ)P(x∣λ),P(Y mid lambda) = sum_{x in X} P(Y mid x, lambda)P(x mid lambda),P(Y∣λ)=x∈X∑​P(Y∣x,λ)P(x∣λ), . where $X$ is the set of all valid hidden state sequences, e.g., $X ={V^T}$. We can compute the conditional probability of an observation sequence given the hidden state sequence as . P(Y∣x,λ)=∏t=1Tbxt(yt).P(Y mid x, lambda) = prod_{t=1}^T b_{x_t}(y_t).P(Y∣x,λ)=t=1∏T​bxt​​(yt​). . Below, I have the R code that computes the likelihood by brute-force: . # Transform hidden state set to numerical set V_num = seq(1, length(V)) # All possible hidden state sequences 2^12 V_all = permutations(n=length(V), r=length(Y), repeats.allowed=TRUE) # Compute the likelihood given a hidden state sequence get_likelihood = function(V_seq, Y, B, pi, Q) { l1 = prod(B[matrix(c(Y, V_seq), ncol=2)]) # Compute all transition probabilitoes Q_el = matrix(c(V_seq[1:(length(V_seq)-1)], V_seq[2:length(V_seq)]), ncol=2) l2 = pi[V_seq[1]] * prod(Q[Q_el]) return(l1 * l2) } total_l = sum(apply(V_all, 1, get_likelihood, Y, B, pi, Q)) print(total_l) . ## [1] 0.0099748 . This brute-force algorithm is extremely inefficient and is not applicable when the state space and/or sequence length is large. Just like (regular) Markov models we can use the Markov property to compute the likelihood in a more efficient manner. Here, we make use of the fact that the transition probabilities of jumping to certain states only depends on the current state. Let $Y_t$ denote the subset of $Y$ of the first $t$ observations and let $X_t$ be the set of hidden state sequences up to time $t$. Recall that we want to compute $P(Y)$. Let $ alpha_t(j)$ represent the probability of being in state $j$ at time $t$ after seeing the first $t$ observations $Y_t$, given our model specification, i.e., . αt(j) = P(Y,Xt = j ∣ λ). alpha_t(j) = P(Y, X_t = j ∣  lambda).αt​(j) = P(Y,Xt​ = j ∣ λ). . By the law of total expectation we have that . We can use this recursively relationship to efficiently compute $P(Y_T mid lambda)$ by means of the which is presented in Alg. (1). . The R-implementation can be found below . forward_alg = function(Y, V, pi, B, Q) { # Define empty forward matrix forward = matrix(data=0, nrow=length(V), ncol=length(Y)) # Fill first elements forward[,1] = B[Y[1],] * pi # Now for all other time steps for (t in 2:length(Y)) { forward[,t] = forward[,t-1] %*% Q * B[Y[t],] } return(forward) } alpha = forward_alg(Y, V_num, pi, B, Q) print(sum(alpha[, length(Y)])) . ## [1] 0.0099748 . We see that the brute-force method and the forward algorithm produce the same likelihood for our sequence. . Decoding Hidden States . The forward algorithm can be used to determine the likelihood of a certain observed sequence given the Markov model and the hidden state sequence. Note, however, that the hidden state sequence is not observed! It is more interesting to compute the most likely hidden state sequence given the underlying Markov model and the observed state sequence. This task of determining the hidden state sequence is refered to as the. . One possible way to determine the hidden state sequence would be to compute the likelihood of all possible hidden state sequences using the forward algorithm and select the hidden state sequence with the highest likelihood value. Similar to determining the likelihood, this brute-force approach quickly becomes intractable. Instead, we can apply the dynamic programming algorithm called the to decode the hidden state sequence. . Similar to the forward algorithm, the proceeds through the time-series from the start till the end. The can be computed as . (x1∗,…,xT∗)=arg max⁡(x1,…,xT)∈XP(YT,XT∣λ)P(XT∣λ).(x^*_1, dots, x^*_T) = argmax_{(x_1, dots, x_T) in X} P(Y_T, X_T mid lambda) P(X_T mid lambda).(x1∗​,…,xT∗​)=(x1​,…,xT​)∈Xargmax​P(YT​,XT​∣λ)P(XT​∣λ). . Similar to the forward algorithm, we can aply a dynamic programming approach to compute the. Let $v_t(j)$ be the probability of observing sequence $Y_t$ using the hidden sequence $(x^_1, dots, x^_{t-1})$, that is, . The psuedo-code for the is given in Alg 2. . The R implementation of the Viterbi algorithm is given below: . viterbi_alg = function(Y, V, pi, B, Q) { # Define empty forward matrix T_ = length(Y) viterbi = matrix(data=0, nrow=length(V), ncol=length(Y)) path = matrix(data=0, nrow=length(V), ncol=length(Y)) # Fill first elements viterbi[,1] = B[Y[1],] * pi path[,1] = rep(0, length(V)) # Now for all other time steps for (t in 2:length(Y)) { tmp_val = t(viterbi[,t-1] * Q) max_x = max.col(tmp_val) viterbi[,t] = tmp_val[,max_x][, 1] * B[Y[t],] path[,t] = max_x } best_path_prob = max(viterbi[,T_]) best_path_end = which.max(viterbi[,T_]) # Best path best_path = rep(-1, T_) best_path[T_] = best_path_end for (t in (T_-1):1) { best_path[t] = path[best_path[t+1],t+1] } return(list(best_path_prob, best_path)) } viterbi_results = viterbi_alg(Y, V_num, pi, B, Q) print(viterbi_results[[1]]) . ## [1] 0.0037632 . print(V[viterbi_results[[2]]]) . ## [1] &quot;COLD&quot; &quot;HOT&quot; &quot;HOT&quot; &quot;HOT&quot; . Determining the Optimal HMM Parameters . The standard algorithm to estimate the optimal HMM parameters is the Baum-Welch (BW) algorithm which is a special case of the Expectation-Maximisation algorithm. The BW algorithm iteratively updates the HMM parameters and converges to the optimal HMM parameters under mild convergence conditions. . Before discussing the BW algorithm we need some useful probabilities. First, similar to the forward probabilities $ alpha_t(j) = P(y_1, dots, y_t, x_t = j mid lambda)$ we can compute the backward probabilities $ beta_t(j) = P(y_{t+1}, dots, y_T mid x_t = j, lambda)$, i.e., given our HMM parameters $ lambda$ and that the hidden state at time $t$ equals $j$ what is the probability that we observe the sequence $y_{t+1}, dots, y_T$. Similar to the forward probabilities we can determine the backward probabilities using dynamic programming as: The R implementation is given below: . back_prob_alg = function(Y, V, pi, B, Q) { # Define empty forward matrix back_prob = matrix(data=0, nrow=length(V), ncol=length(Y)) T_max = length(Y) # Fill first elements back_prob[, T_max] = rep(1, length(V)) # Now for all other time steps for (t in (T_max-1):1) { back_prob[,t] = Q %*% (back_prob[, t+1] * B[Y[t+1], ]) } return(back_prob) } beta = back_prob_alg(Y, V_num, pi, B, Q) probY = sum(pi * B[Y[1], ] * beta[, 1]) print(probY) . ## [1] 0.0099748 . Recall: we are interesting in estimating $Q$, $B$ and $ pi$ given our observation sequence $Y$. As we will see later on, estimating these quantities typically involve estimating the frequency of being in a certain state and/or counting the expected number of transitions from one state to another. More precisely, we estimate the transition probability from state $i$ to $j$ as . q^i,j=expected number of transitions from state i to jexpected number of transitions from state i. hat{q}_{i,j} = frac{ textrm{expected number of transitions from state } i textrm{ to } j}{ textrm{expected number of transitions from state } i}.q^​i,j​=expected number of transitions from state iexpected number of transitions from state i to j​. . To estimate $b_i(y_t)$, i.e., the probability of observing $y_t$ in state $X_t = i$, we compute: . b^i(yt)=expected number of times in state i while observing ytexpected number of times in state i. hat{b}_i(y_t) = frac{ textrm{expected number of times in state } i textrm{ while observing } y_t}{ textrm{expected number of times in state } i}.b^i​(yt​)=expected number of times in state iexpected number of times in state i while observing yt​​. . The initial state probabilities $ pi$ will follow directly from quantities computed in the EW algorithm. . We now define . γt(j)=P(Xt=j∣Y,λ)=P(Y,Xt=j∣λ)P(Y∣λ) gamma_t(j) = P(X_t = j mid Y, lambda) = frac{P(Y, X_t = j mid lambda)}{P(Y mid lambda)}γt​(j)=P(Xt​=j∣Y,λ)=P(Y∣λ)P(Y,Xt​=j∣λ)​ . which is the probability of being in state $j$ at time $t$ for a state sequence $Y$. By the Markovian conditional independence . αt(j)βt(j)=P(y1,…,yt,Xt=j∣λ)P(yt+1,…,yT∣Xt=j,λ)=P(Y,Xt=j∣λ). alpha_t(j) beta_t(j) = P(y_1, dots, y_t, X_t = j mid lambda) P(y_{t+1}, dots, y_T mid X_t = j, lambda) = P(Y, X_t = j mid lambda).αt​(j)βt​(j)=P(y1​,…,yt​,Xt​=j∣λ)P(yt+1​,…,yT​∣Xt​=j,λ)=P(Y,Xt​=j∣λ). . Hence, we can write $ gamma_t(j)$ in terms of $ alpha_t(j)$ and $ beta_t(j)$ as . γt(j)=αt(j)βt(j)P(Y∣λ). gamma_t(j) = frac{ alpha_t(j) beta_t(j)}{P(Y mid lambda)}.γt​(j)=P(Y∣λ)αt​(j)βt​(j)​. . Recall that $P(Y mid lambda)$ can be easily obtained when computing the forward (or backward) probabilities. . ComputeGamma &lt;- function(alpha, beta, probY) { # Obtain gamma for all t and v gamma &lt;- (alpha * beta) / probY } gamma = ComputeGamma(alpha, beta, probY) . We can now estimate the elements of $B$ as . b^j(k)=∑t=1Tδyt,vkγt(j)∑t=1Tγt(j), hat{b}_j(k) = frac{ sum_{t=1}^T delta_{y_t, v_k} gamma_t(j)}{ sum_{t=1}^T gamma_t(j)},b^j​(k)=∑t=1T​γt​(j)∑t=1T​δyt​,vk​​γt​(j)​, . where $ delta_{i, j}$ evaluates to 1 if $i = j$ and 0 otherwise. . Delta &lt;- function(A, b) { return(as.integer(A == b)) } Delta = Vectorize(Delta, &quot;b&quot;) ComputeBHat &lt;- function(gamma, Y, D) { # Obtain the delta variable delta &lt;- t(Delta(Y, D)) # Compute nominator and denominator deltaGamma &lt;- delta %*% t(gamma) deltaDenom &lt;- matrix(data=1, nrow=nrow(delta), ncol=ncol(delta)) %*% t(gamma) # Divide element-wise BHat &lt;- deltaGamma / deltaDenom return(BHat) } BHat = ComputeBHat(gamma, Y, D) . To estimate the elements of $Q$ we define . ψt(i,j)=P(Xt=i,Xt+1=j∣Y,λ)=P(Xt=i,Xt+1=j,Y∣λ)P(Y∣λ) psi_t(i,j) = P(X_t = i, X_{t+1} = j mid Y, lambda) = frac{P(X_t = i, X_{t+1} = j, Y mid lambda)}{P(Y mid lambda)}ψt​(i,j)=P(Xt​=i,Xt+1​=j∣Y,λ)=P(Y∣λ)P(Xt​=i,Xt+1​=j,Y∣λ)​ . which is the probability of being in state $i$ at time $t$ and being in state $j$ at time $t+1$ for a state sequence $Y$. This can be parameterised in terms of $ alpha_t(j)$ and $ beta_t(j)$ as . ψt(i,j)=αt(i)qi,jbt+1(j)βt+1(j)P(Y∣λ). psi_t(i,j) = frac{ alpha_t(i)q_{i,j}b_{t+1}(j) beta_{t+1}(j)}{P(Y mid lambda)}.ψt​(i,j)=P(Y∣λ)αt​(i)qi,j​bt+1​(j)βt+1​(j)​. . ComputePsi &lt;- function(alpha, beta, B, Q, Y, probY) { # Create empty matrix T_max = ncol(beta) psi = array(-1, dim=c(T_max-1, nrow(Q), nrow(Q))) for (t in 1:(T_max-1)) { psi[t,, ] = cbind(alpha[, t], alpha[, t]) * Q * rbind(B[Y[t+1], ], B[Y[t+1], ]) * rbind(beta[, t+1], beta[, t+1]) } return(psi / probY) } psi = ComputePsi(alpha, beta, B, Q, Y, probY) . The transition probabilities can now be estimated as . qˉi,j=∑t=1T−1ψt(i,j)∑t=1T−1γt(i) bar{q}_{i,j} = frac{ sum_{t = 1}^{T-1} psi_t(i,j)}{ sum_{t=1}^{T-1} gamma_t(i)}qˉ​i,j​=∑t=1T−1​γt​(i)∑t=1T−1​ψt​(i,j)​ . since the expected number of times in a state $j$ is equal to the expected number of transitions from state $j$ (for an ergodic Markov process). . ComputeQ = function(psi, gamma) { QNom = apply(psi, c(2,3), sum) QDenom = t(rep(1, 2)) %x% apply(gamma[, 1:(ncol(gamma)-1)], 1, sum) Q = QNom / QDenom } Q_new = ComputeQ(psi, gamma) . The quantity . πˉj=γ1(j) bar{ pi}_j = gamma_1(j)πˉj​=γ1​(j) . is an estimate for the initial state probability for state $j$. . The idea of the BW algorithm is to iteratively update $ gamma_t( cdot)$ and $psi_t( cdot)$ in one step (E-step) and the estimates for $Q$ and $B$ in the other step (M-step). The exact BW algorithm is provided in Alg. 3. . The R implementation of the Balm-Welsch algorithm is given below: . balm_welch_alg = function(Y, Q, B, pi, V, D) { converged = FALSE max_iter = 1000 iter = 1 while(!converged &amp; iter!= max_iter) { # First compute alpa and beta alpha = forward_alg(Y, V, pi, B, Q) beta = back_prob_alg(Y, V, pi, B, Q) probY = sum(alpha[,ncol(alpha)]) # E-Step gamma = ComputeGamma(alpha, beta, probY) psi = ComputePsi(alpha, beta, B, Q, Y, probY) # M-Step Q = ComputeQ(psi, gamma) B = ComputeBHat(gamma, Y, D) pi = gamma[, 1] # Increase iteration iter = iter + 1 } } . We have now addressed all main problems concerning HMMs. Note, however that our implementation in R is rather simplistic and probably has several numerical issues. Furthermore, we restricted our observed sequence to consist of (finite) discrete options. A common extension is to allow $Y mid X$ to have some continous distribution which depends on $X$ and is parameterised by some $ theta$. Check Gaussian Mixture Hidden Markov Models as an example. . Numerical Issues . The procedures described in this article cannot be used for long observation sequences ( $&gt;100$) due to underflow problems. Recall that . P(Y=Yt,X=XT∣λ)=∏t=1T−1qxt,xt+1∏t=1Tbxt(yt)P(Y= Y_t, X = X_T mid lambda) = prod_{t=1}^{T-1}q_{x_t, x_{t+1}} prod_{t=1}^T b_{x_t}(y_t)P(Y=Yt​,X=XT​∣λ)=t=1∏T−1​qxt​,xt+1​​t=1∏T​bxt​​(yt​) . which consists of a product of numbers between 0 and 1. For sufficiently large $T$ this quantity is equal to zero for computers. Hence, we need to scale the values of $Q$ and $B$ such that we do not have this numerical issue. . The standard approach is to standardise $ alpha_t(j)$ and $ beta_t(j)$ by normalising the values for each $j in [N]$ by multiplying these values with . ct=1∑i=1Nαt(i)c_t = frac{1}{ sum_{i=1}^N alpha_t(i)}ct​=∑i=1N​αt​(i)1​ . to obtain . α^t(j)=αt(j)ct hat{ alpha}_t(j) = alpha_t(j) c_tα^t​(j)=αt​(j)ct​ . and . β^t(j)=βt(j)ct. hat{ beta}_t(j) = beta_t(j) c_t.β^​t​(j)=βt​(j)ct​. . ForwardProbAlgScaled = function(Y, V, pi, B, Q) { # Define empty forward matrix forward &lt;- matrix(data=0, nrow=length(V), ncol=length(Y)) c_scale &lt;- 1 / rep(1, ncol=length(Y)) # Fill first elements forward[,1] &lt;- B[Y[1],] * pi c_scale[1] &lt;- sum(forward[,1]) forward[,1] &lt;- forward[,1] * c_scale[1] # Now for all other time steps for (t in 2:length(Y)) { tmp &lt;- forward[,t-1] %*% Q * B[Y[t],] c_scale[t] &lt;- 1 / sum(tmp) forward[,t] &lt;- tmp * c_scale[t] } return(list(forward, c_scale)) } BackProbAlgScaled = function(Y, V, pi, B, Q, c_scale) { # Define empty forward matrix back_prob &lt;- matrix(data=0, nrow=length(V), ncol=length(Y)) T_max &lt;- length(Y) # Fill first elements back_prob[, T_max] &lt;- rep(1, length(V)) * c_scale[T_max] # Now for all other time steps for (t in (T_max-1):1) { back_prob[,t] &lt;- Q %*% (back_prob[, t+1] * B[Y[t+1], ]) * c_scale[t] } return(back_prob) } ComputeGammaScaled &lt;- function(alpha, beta) { # Obtain gamma for all t and v alpha_beta &lt;- alpha * beta gamma &lt;- t(t(alpha_beta) / apply(alpha_beta, 2, sum)) } ComputePsiScaled &lt;- function(alpha, beta, B, Q, Y) { # Create empty matrix T_max &lt;- ncol(beta) psi &lt;- array(-1, dim=c(T_max-1, nrow(Q), nrow(Q))) for (t in 1:(T_max-1)) { psi[t,, ] &lt;- cbind(alpha[, t], alpha[, t]) * Q * rbind(B[Y[t+1], ], B[Y[t+1], ]) * rbind(beta[, t+1], beta[, t+1]) psi[t,, ] &lt;- psi[t,, ] / sum(psi[t,, ]) } return(psi) } . The adapted Balm-Welch algorithm is then given by . BalmWelchAlg = function(Y, Q, B, pi, V, D) { converged = FALSE max_iter = 1000 iter = 1 prob_old = log(1e-12) while(!converged &amp; iter!= max_iter) { # First compute alpa and beta forward_list &lt;- ForwardProbAlgScaled(Y, V, pi, B, Q) c_scale &lt;- forward_list[[2]] alpha &lt;- forward_list[[1]] beta &lt;- BackProbAlgScaled(Y, V, pi, B, Q, c_scale) prob_new &lt;- log(1 / prod(c_scale)) # E-Step gamma &lt;- ComputeGammaScaled(alpha, beta) psi &lt;- ComputePsiScaled(alpha, beta, B, Q, Y) # M-Step Q &lt;- ComputeQ(psi, gamma) B &lt;- ComputeBHat(gamma, Y, D) pi &lt;- gamma[, 1] # Update stopping rules if (prob_new - prob_old &lt; 1e-4) { converged &lt;- TRUE } prob_old = prob_new iter &lt;- iter + 1 } return(list(Q = Q, B = B, pi = pi)) } model &lt;- BalmWelchAlg(Y, Q, B, pi, V, D) model[[&quot;Q&quot;]] . ## [,1] [,2] ## [1,]1 4.738597e-19 ## [2,]1 4.788997e-18 . model[[&quot;B&quot;]] . ## [,1] [,2] ## [1,] 1.974110e-43 1.000000e+00 ## [2,] 3.333333e-01 9.278520e-20 ## [3,] 6.666667e-01 5.643931e-18 . model[[&quot;pi&quot;]] . ## [1] 5.922329e-43 1.000000e+00 .",
            "url": "https://mvanderbroek.com/r/machine%20learning/2019/05/08/Hidden-Markov-Model-Tutorial-in-R.html",
            "relUrl": "/r/machine%20learning/2019/05/08/Hidden-Markov-Model-Tutorial-in-R.html",
            "date": " • May 8, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Predicting Sale Prices Of Houses In Ames",
            "content": "In this tutorial I will discuss how you can go from a raw dataset to a predictive model. For this tutorial we will make use of the Ames Dataset and see whether we can predict house prices based on characteristics provided in the dataset. . The analysis in this tutorial is done in Python using the pandas, scikit-learn and matplotlib packages. We will start by exploring the raw data and see whether we can already see some patterns in the data or that some features should be discarded right away. Next, we will make a straight-forward pipeline that will transform our dataset and fit a linear regression model. Finally, we will evaluate the performance of this model. . I do have limited knowledge with data analysis (I mainly use R), so this tutorial will be most informative for people like me: beginners! . Raw Data &amp; Initial Analysis . First, we need to download the dataset provided in the link above (direct download link here). . import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import date from sklearn.impute import SimpleImputer from sklearn.preprocessing import LabelBinarizer, StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import warnings # Do not display warnings warnings.filterwarnings(&#39;ignore&#39;) # Import the Ames Housing data (put this in the same folder as your Python file) house_data = pd.read_csv(&quot;AmesHousing.txt&quot;, sep=&quot; t&quot;) # I do not like spaces in the column names of a dataset so I first replace spaces # by underscores house_data.columns = house_data.columns.str.replace(&#39; &#39;, &#39;_&#39;) . Next we want to get a feeling for what exactly we can find in the dataset, so let’s look at some relevant information. . # Get some feeling for the data print(house_data.head(10)) house_info = house_data.describe() house_data.info() . Order PID MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street 0 1 526301100 20 RL 141.0 31770 Pave 1 2 526350040 20 RH 80.0 11622 Pave 2 3 526351010 20 RL 81.0 14267 Pave 3 4 526353030 20 RL 93.0 11160 Pave 4 5 527105010 60 RL 74.0 13830 Pave 5 6 527105030 60 RL 78.0 9978 Pave 6 7 527127150 120 RL 41.0 4920 Pave 7 8 527145080 120 RL 43.0 5005 Pave 8 9 527146030 120 RL 39.0 5389 Pave 9 10 527162130 60 RL 60.0 7500 Pave Alley Lot_Shape Land_Contour ... Pool_Area Pool_QC Fence 0 NaN IR1 Lvl ... 0 NaN NaN 1 NaN Reg Lvl ... 0 NaN MnPrv 2 NaN IR1 Lvl ... 0 NaN NaN 3 NaN Reg Lvl ... 0 NaN NaN 4 NaN IR1 Lvl ... 0 NaN MnPrv 5 NaN IR1 Lvl ... 0 NaN NaN 6 NaN Reg Lvl ... 0 NaN NaN 7 NaN IR1 HLS ... 0 NaN NaN 8 NaN IR1 Lvl ... 0 NaN NaN 9 NaN Reg Lvl ... 0 NaN NaN Misc_Feature Misc_Val Mo_Sold Yr_Sold Sale_Type Sale_Condition SalePrice 0 NaN 0 5 2010 WD Normal 215000 1 NaN 0 6 2010 WD Normal 105000 2 Gar2 12500 6 2010 WD Normal 172000 3 NaN 0 4 2010 WD Normal 244000 4 NaN 0 3 2010 WD Normal 189900 5 NaN 0 6 2010 WD Normal 195500 6 NaN 0 4 2010 WD Normal 213500 7 NaN 0 1 2010 WD Normal 191500 8 NaN 0 3 2010 WD Normal 236500 9 NaN 0 6 2010 WD Normal 189000 [10 rows x 82 columns] &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2930 entries, 0 to 2929 Data columns (total 82 columns): Order 2930 non-null int64 PID 2930 non-null int64 MS_SubClass 2930 non-null int64 MS_Zoning 2930 non-null object Lot_Frontage 2440 non-null float64 Lot_Area 2930 non-null int64 Street 2930 non-null object Alley 198 non-null object Lot_Shape 2930 non-null object Land_Contour 2930 non-null object Utilities 2930 non-null object Lot_Config 2930 non-null object Land_Slope 2930 non-null object Neighborhood 2930 non-null object Condition_1 2930 non-null object Condition_2 2930 non-null object Bldg_Type 2930 non-null object House_Style 2930 non-null object Overall_Qual 2930 non-null int64 Overall_Cond 2930 non-null int64 Year_Built 2930 non-null int64 Year_Remod/Add 2930 non-null int64 Roof_Style 2930 non-null object Roof_Matl 2930 non-null object Exterior_1st 2930 non-null object Exterior_2nd 2930 non-null object Mas_Vnr_Type 2907 non-null object Mas_Vnr_Area 2907 non-null float64 Exter_Qual 2930 non-null object Exter_Cond 2930 non-null object Foundation 2930 non-null object Bsmt_Qual 2850 non-null object Bsmt_Cond 2850 non-null object Bsmt_Exposure 2847 non-null object BsmtFin_Type_1 2850 non-null object BsmtFin_SF_1 2929 non-null float64 BsmtFin_Type_2 2849 non-null object BsmtFin_SF_2 2929 non-null float64 Bsmt_Unf_SF 2929 non-null float64 Total_Bsmt_SF 2929 non-null float64 Heating 2930 non-null object Heating_QC 2930 non-null object Central_Air 2930 non-null object Electrical 2929 non-null object 1st_Flr_SF 2930 non-null int64 2nd_Flr_SF 2930 non-null int64 Low_Qual_Fin_SF 2930 non-null int64 Gr_Liv_Area 2930 non-null int64 Bsmt_Full_Bath 2928 non-null float64 Bsmt_Half_Bath 2928 non-null float64 Full_Bath 2930 non-null int64 Half_Bath 2930 non-null int64 Bedroom_AbvGr 2930 non-null int64 Kitchen_AbvGr 2930 non-null int64 Kitchen_Qual 2930 non-null object TotRms_AbvGrd 2930 non-null int64 Functional 2930 non-null object Fireplaces 2930 non-null int64 Fireplace_Qu 1508 non-null object Garage_Type 2773 non-null object Garage_Yr_Blt 2771 non-null float64 Garage_Finish 2771 non-null object Garage_Cars 2929 non-null float64 Garage_Area 2929 non-null float64 Garage_Qual 2771 non-null object Garage_Cond 2771 non-null object Paved_Drive 2930 non-null object Wood_Deck_SF 2930 non-null int64 Open_Porch_SF 2930 non-null int64 Enclosed_Porch 2930 non-null int64 3Ssn_Porch 2930 non-null int64 Screen_Porch 2930 non-null int64 Pool_Area 2930 non-null int64 Pool_QC 13 non-null object Fence 572 non-null object Misc_Feature 106 non-null object Misc_Val 2930 non-null int64 Mo_Sold 2930 non-null int64 Yr_Sold 2930 non-null int64 Sale_Type 2930 non-null object Sale_Condition 2930 non-null object SalePrice 2930 non-null int64 dtypes: float64(11), int64(28), object(43) memory usage: 1.8+ MB . Since we want to build a model that predicts the house price given a set of features of this given house, let’s first check these prices by creating a histogram. . plt.hist(house_data.SalePrice, bins=50) plt.xlabel(&quot;House price (in $)&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() # Pandas also provides a built-in plotting env # house_data.SalePrice.plot.hist() . . The Year_Built feature would be a candidate to give insights in the sale price of a house. Below we report the density of the year that each house in the dataset is built. . house_data[&#39;Year_Built&#39;].plot.kde() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1ab908d0&gt; . . We can check the relationship between the sale price and the year houses are built by creating a scatter plot. . house_data.plot.scatter(x=&#39;Year_Built&#39;, y=&#39;SalePrice&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19375160&gt; . . This plot suggest that houses that are built more recently tend to have a higher sale price (on average). Note that this probably due to the geographical area we are considering. In old cities, like Amsterdam, we can imagine that (part) of the old houses are momumental buildings and therefore have higher sale price. . In the analysis above, we have looked into the relationship of the sale price and a numerical feature. A scatter plot is usually a good option to check if there might be an interesting relationship, however for categorical features we need different techniques. . We have information on the type of sale for each house, e.g., some houses are sold with adjecent land, or a family member bought the house. A tool to look at the relationship between this (categorical) feature and the sale price is to look at the histogram for each type of sale condition. . house_data[&#39;SalePrice&#39;].hist(by=house_data[&#39;Sale_Condition&#39;], bins=30) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1ad44eb8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1acd0f98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1af29208&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1a944470&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1ac636d8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1ae64940&gt;]], dtype=object) . . Another feature that is likely to have an influence on the sale price is in what neighbourhood the house is located. We could again create histograms of the sale price for each neighbourhood, but since there are more than 20 neighbourhoods, we can do something else. We take the mean of the sale price for each neighbourhood and present this in a bar plot. . # Check saleprice per neighbourhood avg_price_neigh = house_data.groupby(&#39;Neighborhood&#39;).agg({&#39;SalePrice&#39; : &#39;mean&#39;}).reset_index() avg_price_neigh.plot.bar(x=&#39;Neighborhood&#39;, y=&#39;SalePrice&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1b026dd8&gt; . . Feature Selection . The dataset provides more than 80 features, and not all features are equally informative for sale price. Selecting good features and transforming existing features is often a rather ad-hoc procedure, since every dataset is unique. However, there are a few standard procedures that usually help in creating a significantly better dataset. . Trimming . One of these techniques is to delete outliers from your dataset. Whether or not this is necesarry also depends on the type of model that is used, e.g., linear models are usually very sensitive to outliers. . Let’s look at a scatter plot of the living area and sale price. . # Check living space and sale price (there seems to be 5 outliers...) house_data.plot.scatter(x=&#39;Gr_Liv_Area&#39;, y=&#39;SalePrice&#39;) house_data = house_data.query(&#39;Gr_Liv_Area &lt; 4000&#39;) . . There seem to be 4-5 outliers with very high living areas. For this analysis we will remove these, however if the goal of your model is to also have good predictions for these outliers it might be worthwhile to keep them. . Feature enhancement . There are many ways to enhance existing features. Here, I will show how categorical variables can be improved. To do this, I select all features that are of the type object and save the histogram for these features in a folder figures. . # Check categorical variables and see if we need to delete them house_data_cat = house_data.select_dtypes(include=&#39;object&#39;) for col in house_data_cat.columns: house_data_cat[col].value_counts().plot.bar(title=col) plt.savefig(&#39;figures/&#39; + col + &#39;_hist.pdf&#39;) plt.clf() house_data_cat[&#39;Bsmt_Qual&#39;].value_counts().plot.bar(title=&#39;Basement Quality&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1d629be0&gt; . . There are a considerable number of categorical features that are ordinal, that is, the categories have a certain ordering. In this case, we have information about whether the feature has a value Poor, Fair, Average, Good or Excellent. However, especially the most extreme outcomes are rather unlikely. As we will later see this results in a large number of features (when we one hot encode them) with only minimal extra predictive power. Therefore we combine certain groups. . ord_groups = {&#39;Fa&#39;: &#39;Bad&#39;, &#39;Po&#39;: &#39;Bad&#39;, &#39;TA&#39;: &#39;Average&#39;, &#39;Gd&#39;: &#39;Good&#39;, &#39;Ex&#39;:&#39;Good&#39;} columns_ord = [&#39;Bsmt_Cond&#39;, &#39;Bsmt_Qual&#39;, &#39;Exter_Cond&#39;, &#39;Exter_Qual&#39;, &#39;Fireplace_Qu&#39;, &#39;Garage_Cond&#39;, &#39;Garage_Qual&#39;, &#39;Heating_QC&#39;, &#39;Kitchen_Qual&#39;, &#39;Pool_QC&#39;] for col in columns_ord: house_data[col].replace(ord_groups, inplace=True) house_data[col] = house_data[col].astype(&#39;category&#39;, categories=[&#39;Bad&#39;, &#39;Average&#39;, &#39;Good&#39;], ordered=True) . Missing Values . Most real-life datasets have missing values for a subset of its features. There are different ways to deal with these missing values. Usually, when a feature’s value is missing in most of the samples, it is better to just discard them. Let’s see if we have any of these variables. . missing_vals = house_data.isnull().sum(axis = 0) print(missing_vals) . Order 0 PID 0 MS_SubClass 0 MS_Zoning 0 Lot_Frontage 490 Lot_Area 0 Street 0 Alley 2727 Lot_Shape 0 Land_Contour 0 Utilities 0 Lot_Config 0 Land_Slope 0 Neighborhood 0 Condition_1 0 Condition_2 0 Bldg_Type 0 House_Style 0 Overall_Qual 0 Overall_Cond 0 Year_Built 0 Year_Remod/Add 0 Roof_Style 0 Roof_Matl 0 Exterior_1st 0 Exterior_2nd 0 Mas_Vnr_Type 23 Mas_Vnr_Area 23 Exter_Qual 0 Exter_Cond 0 ... Bedroom_AbvGr 0 Kitchen_AbvGr 0 Kitchen_Qual 0 TotRms_AbvGrd 0 Functional 0 Fireplaces 0 Fireplace_Qu 1422 Garage_Type 157 Garage_Yr_Blt 159 Garage_Finish 159 Garage_Cars 1 Garage_Area 1 Garage_Qual 159 Garage_Cond 159 Paved_Drive 0 Wood_Deck_SF 0 Open_Porch_SF 0 Enclosed_Porch 0 3Ssn_Porch 0 Screen_Porch 0 Pool_Area 0 Pool_QC 2914 Fence 2354 Misc_Feature 2820 Misc_Val 0 Mo_Sold 0 Yr_Sold 0 Sale_Type 0 Sale_Condition 0 SalePrice 0 Length: 82, dtype: int64 . Let’s get rid of features that have many missing values. . house_data = house_data.drop(columns=[&#39;Alley&#39;, &#39;Fireplace_Qu&#39;, &#39;Pool_QC&#39;, &#39;Misc_Feature&#39;, &#39;Misc_Val&#39;]) . For some features a missing value is not really missing, since it can indicate that the value of this feature is zero when it’s missing. Fence and Pool_Area belong to this group. For these features we decide to create a binary variable indicating whether or not the house has this feature (and we do not care about the type or size of feature). . # Make some variables useful house_data[&#39;Fence&#39;] = house_data[&#39;Fence&#39;].notna() house_data[&#39;Pool&#39;] = house_data[&#39;Pool_Area&#39;] &gt; 0 . Similar to the categorical features that are rated from poor until excellent, there are also features with different categories. Again, it might be worthwhile to combine categories in the dataset that only occur a very infrequently. In this case we combine categories that consistute less than 1 percent of the total samples. . house_data_obj = house_data.select_dtypes(include=&#39;object&#39;) for col in house_data_obj.columns: series = house_data[col].value_counts() mask = (series/series.sum() * 100).lt(1) house_data[col] = np.where(house_data[col].isin(series[mask].index),&#39;Other&#39;, house_data[col]) house_data[col] = house_data[col].astype(&#39;category&#39;) . Note that the 1 percent of the total sample is rather ad-hoc. Imagine that you have a dataset consisting of millions of samples, then there is no need to get rid of these categories, since we still have sufficient information (unless you need balanced categories). . To see what effect our transformations and enhanced have had on our features, we can have a look at the basement quality feature again. . house_data[&#39;Bsmt_Qual&#39;].value_counts().plot.bar(title=&#39;Basement Quality&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1db4d668&gt; . . Data pipeline . Now that we have cleaned and transformed our dataset we can look at if it is possible to create a decent model that can predict sale prices of houses the model has not seen before. However, we first have to deal with a few steps before we can use our dataset in these models. . We need to make sure that we do not evaluate on data that we used to train and estimate our model. So we split our dataset in a training and test part. . X = house_data.drop(columns=[&#39;SalePrice&#39;]) y = house_data[&#39;SalePrice&#39;] # We need to split the set into a training and test set. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . Imputer . Even though we got rid of the features with many missing values, there are still features that have a few missing values. It is possible to use advanced techiques to “imputate” these missing values, however we will a simple imputing technique, which replaces the missing values with the most frequent value for that feature. . imputer = SimpleImputer(strategy=&#39;most_frequent&#39;) . Standardisation &amp; Encoding . Most machine learning models require that the scale of the features in the dataset are similar. Therefore, we will standardise all numerical features in the dataset by removing the mean of that feature and dividing by the standard deviation, i.e., xscaled=x−xˉstd(x)x_{scaled} = frac{x - bar{x}}{ text{std}(x)}xscaled​=std(x)x−xˉ​ . We also have categorical features which we cannot directly feed to the machine learning models. All features need to be represented by a numerical value. Both nominal and ordinal features can be transformed by a technique called One-hot encoding, where a categorical feature is replaced by a number of dummies that indicate one of these categories. . # Next we need to scale the numerical columns and encode the categorical # variables. This can be done by splitting the dataset into two parts. cat_cols = X_train.select_dtypes(include=&#39;category&#39;).columns num_cols = X_train.select_dtypes(include=&#39;number&#39;).columns cat_cols_mask = X_train.columns.isin(cat_cols) num_cols_mask = X_train.columns.isin(num_cols) ct = ColumnTransformer( [(&#39;scale&#39;, StandardScaler(), num_cols_mask), (&#39;encode&#39;, OneHotEncoder(), cat_cols_mask)]) . Note that ordinal values can also be transformed into a single numerical value, where each category is represented by a number that is ordered according to the ordering of these categories. The advantage of this procedure is that we require less features to represent this feature numerically, however for most machine learning models it also results in a linear relationship between the that feature and the feature we want to predict. . Machine learning model . Now that we have a dataset that only consists of numerical values we can apply any machine learning we want. scikit-learn provides many machine learning models with a common interface (except for the hyperparameters), so it is easy to implement many different models. In our case I will only use a linear regression. . linear_reg = LinearRegression() . Combine all steps of pipeline . Pipelines can be used to automatically perform all steps of the model estimation (including preprocessing steps). It also makes sure that the preprocessing is done correctly, e.g., the scaling for the test dataset is the one used for the training dataset (Reader: why is this absolutely necessary?). An overview of what the pipeline does is given in the figure below (copyright by Sebastian Raschka). . . pipe = make_pipeline(imputer, ct, linear_reg) . Training &amp; Evaluating model . Training the model is as simple as . pipe = pipe.fit(X_train, y_train) . Usually, the best model (within or between) class(es) of models can be determined by using cross-validation or splitting the training dataset into a training and validation part (if you have enough data). I will skip this step and go immediately to the evaluation of our model. . In the step above we estimated and trained our model based on the training set. To see how well it performs we can look at what sale prices our model predict for data it has not seen before. . y_pred = pipe.predict(X_test) errors = y_pred - y_test . scikit-learn can provide a score for the model, but we will look at the error of the prediction instead. First let’s plot the histogram of the errors . plt.hist(errors, bins=30) plt.show() . . The histogram suggest that the errors are normally distributed (assumption of the linear regression). Furthermore, it seems that most predictions have an error less than 20,000$. To me this seems like a reasonable model. . Conclusion . In this tutorial we have seen how we can go from raw data to a predictive model that has a reasonable performance. However, there are many things we have not discussed yet. To name a few: . Consider multiple models and select the best one using cross-validation. | Use a more advanced missing value imputation technique, e.g., MICE. | .",
            "url": "https://mvanderbroek.com/machine%20learning/python/2018/10/07/Predicting-Sale-Prices-of-Houses-in-Ames.html",
            "relUrl": "/machine%20learning/python/2018/10/07/Predicting-Sale-Prices-of-Houses-in-Ames.html",
            "date": " • Oct 7, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Train Xor Logic Gate In Neural Network",
            "content": "Deep learning (DL) is a thriving research field with an increasing number of practical applications. One of the models used in DL are so called artificial neural networks (ANN). In this tutorial I will not discuss exactly how these ANNs work, but instead I will show how flexible these models can be by training an ANN that will act as a XOR logic gate. . XOR gate . For those of you unfamiliar with logical gates, a logical gate takes two binary values as inputs and produces a single binary output. For the XOR gate it will output a 1 one value if only one of the input values is 1, and 0 otherwise, i.e., graphically: . Input 1 Input 2 Output . 0 | 0 | 0 | . 1 | 0 | 1 | . 0 | 1 | 1 | . 1 | 1 | 0 | . XOR gate as ANN . GoodFellow et al. show that this XOR gate can be learned by an ANN with one hidden layer consisting of two neurons. We have two input neurons, one hidden layer and an output layer with a single neuron. This network can be graphically represented as: . . When I started learning about Deep Learning and these ANN in particular I started wondering whether I could train the small ANN to learn to act like an XOR gate. Since I am still relatively new to these networks I thought it would be a good exercise to program the backpropagation algorithm that trains these models myself. . The network in Python . I decided to model this network in Python, since it is the most popular language for Deep Learning because of the active development of packages like numpy, tensorflow, keras, etc. As I will show below it is very easy to implement the model as described above and train it using a package like keras. However, since I wanted to get a better understanding of the backpropagation algorithm I decided to first implement this algorithm. . Before we start doing that let us first define the four possible combinations of inputs and corresponding outputs, i.e., . X=[00100111],y=[0110] boldsymbol{X} = begin{bmatrix} 0 &amp; 0 1 &amp; 0 0 &amp; 1 1 &amp; 1 end{bmatrix}, quad boldsymbol{y} = begin{bmatrix} 0 1 1 0 end{bmatrix}X=⎣⎢⎢⎢⎡​0101​0011​⎦⎥⎥⎥⎤​,y=⎣⎢⎢⎢⎡​0110​⎦⎥⎥⎥⎤​ . In python we get . import numpy as np # Possible outputs X = np.matrix(&#39;0 0; 1 0; 0 1; 1 1&#39;) y = np.array([0, 1, 1, 0]) . Own implementation backpropagation algorithm . From this moment onwards I assume that you have a basic understanding of how an ANN works, and understand the basic math behind it. First, I will define some functions that are needed to implement the backpropagation algorithm to solve this problem. . def relu(z): &quot;&quot;&quot;ReLU activation function&quot;&quot;&quot; return np.maximum(z, 0, z) def relu_prime(z): &quot;&quot;&quot;First derivative of the ReLU activation function&quot;&quot;&quot; return 1*(z&gt;0) def sigmoid(z): &quot;&quot;&quot;Sigmoid activation function&quot;&quot;&quot; return 1 / (1 + np.exp(-z)) def sigmoid_prime(z): &quot;&quot;&quot;First derivative of sigmoid activation function&quot;&quot;&quot; return np.multiply(sigmoid(z), 1-sigmoid(z)) def cost(a, y): &quot;&quot;&quot;Calculate MSE&quot;&quot;&quot; return ((a - y) ** 2).mean() def cost_grad(a, y): &quot;&quot;&quot;First derivate of MSE function&quot;&quot;&quot; return a - y def weighted_sum(W, a, b): &quot;&quot;&quot;Compute the weighted average z for all neurons in new layer&quot;&quot;&quot; return W.dot(a) + b def forward_prop(x, W, b): &quot;&quot;&quot;Calculate z and a for every neuron using current weights and biases&quot;&quot;&quot; a = [None] * len(layer_sizes) z = [None] * len(layer_sizes) a[0] = x.T for l in range(1, len(a)): z[l] = weighted_sum(W[l], a[l-1], b[l]) a[l] = sigmoid(z[l]) return (a, z) def back_prop(a, z, W, y): &quot;&quot;&quot;Calculate error delta for every neuron&quot;&quot;&quot; delta = [None] * len(layer_sizes) end_node = len(a)-1 delta[end_node] = np.multiply(cost_grad(a[end_node], y), sigmoid_prime(z[end_node])) for l in reversed(range(1, end_node)): delta[l] = np.multiply(W[l+1].T.dot(delta[l+1]), sigmoid_prime(z[l])) return delta def calc_gradient(W, b, a, delta, eta): &quot;&quot;&quot;Update W and b using gradient descent steps based&quot;&quot;&quot; W_grad = [None] * len(W) b_grad = [None] * len(b) for l in range(1, len(W)): W_grad[l] = a[l-1].dot(delta[l].T) b_grad[l] = delta[l] return (W_grad, b_grad) def backpropagation_iter(X, y, W, b, eta): &quot;&quot;&quot;One iteration of the backpropagation algorithm, i.e., forward- and backward propagate and compute gradient&quot;&quot;&quot; y_pred = [None] * len(y) for i in range(n): # First we propagate forward through the network to obtain activation levels and z. a, z = forward_prop(X[i, :], W, b) y_pred[i] = np.max(a[-1]) # Back propagate to obtain delta&#39;s. delta = back_prop(a, z, W, y[i]) # This allows us to compute the gradient for this instance. Add this to all. W_grad, b_grad = calc_gradient(W, b, a, delta, eta) if i == 0: W_grad_sum = W_grad b_grad_sum = b_grad else: for l in range(1, len(W_grad)): W_grad_sum[l] += W_grad[l] b_grad_sum[l] += b_grad[l] # Update weights and bias for l in range(1, len(W)): W[l] = W[l] - (eta/n) * W_grad_sum[l] b[l] = b[l] - (eta/n) * b_grad_sum[l] # Show MSE MSE = cost(y_pred, y) return (W, b, y_pred, MSE) . We also need to initialise the weights and bias of every link and neuron. It is important to do this randomly. We also set the number of iterations and the learning rate for the gradient descent method. . # Initialise layer sizes of all layers in the neural network layer_sizes = [X.shape[1], 2, 1] # Initialise weights and activation and weight vectors as None. W = [None] * len(layer_sizes) b = [None] * len(layer_sizes) # Initialise weights randomly for l in range(1, len(layer_sizes)): W[l] = np.random.random((layer_sizes[l], layer_sizes[l-1])) b[l] = np.random.random((layer_sizes[l], 1)) # Set number of iterations for backpropagation to work, size, and learning rate n_iter = 100 n = X.shape[0] eta = 0.1 . Below we run our backpropagation algorithm for 100 iterations. For every iteration we display the MSE error of the ANN. Interestingly, we observe that the MSE first drops rapidly, but the MSE does not converge to zero. In other words, the training as described above does not lead to a perfect XOR gate; it can only classify 3 pair of inputs correctly. . for iter in range(n_iter+1): W, b, y_pred, MSE = backpropagation_iter(X, y, W, b, eta) # Only print every 10 iterations if iter % 10 == 0: print(&#39;Iteration {0}: {1}&#39;.format(iter, MSE)) . Iteration 0: 0.38656561971217596 Iteration 10: 0.37967674088143133 Iteration 20: 0.37237614217772685 Iteration 30: 0.3647250789164269 Iteration 40: 0.3567771690665022 Iteration 50: 0.34860542693257024 Iteration 60: 0.3403016700706688 Iteration 70: 0.33197373064753966 Iteration 80: 0.32374029713366215 Iteration 90: 0.3157236870674678 Iteration 100: 0.30804138226491107 . Keras implementation . Above I showed how to implement the backpropagation algorithm in python. Below, I will show how to implement the same model using the keras library. . from keras.models import Sequential from keras.layers import Dense from IPython.display import SVG from keras.utils.vis_utils import model_to_dot model = Sequential() model.add(Dense(units=2, activation=&#39;relu&#39;, input_dim=2)) model.add(Dense(units=1, activation=&#39;linear&#39;)) model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;sgd&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, y, epochs=300, batch_size=4) SVG(model_to_dot(model, show_shapes=True).create(prog=&#39;dot&#39;, format=&#39;svg&#39;)) . Epoch 1/300 4/4 [==============================] - 0s 37ms/step - loss: 0.6039 - acc: 0.7500 Epoch 2/300 4/4 [==============================] - 0s 252us/step - loss: 0.5813 - acc: 0.7500 ... ... Epoch 300/300 4/4 [==============================] - 0s 338us/step - loss: 0.2679 - acc: 0.2500 . . Eye-balling solution . Apparantly the model of both our own implementation and the implementation in Keras is unable to find a minimum without making any erorrs. However, since we have a near trivial problem it is also possible to eye-ball the optimal weight and bias matrices such that the cost is minimised. To see how this works, let’s consider the following values for weights and biases (obtained from GoodFellow et al.): . W(1)=[1111],b(1)=[0−1],W(2)=[1−2],b(2)=0 boldsymbol{W}^{(1)} = begin{bmatrix} 1 &amp; 1 1 &amp; 1 end{bmatrix}, quad boldsymbol{b}^{(1)} = begin{bmatrix} 0 &amp; -1 end{bmatrix}, quad boldsymbol{W}^{(2)} = begin{bmatrix} 1 &amp; -2 end{bmatrix}, quad boldsymbol{b}^{(2)} = 0W(1)=[11​11​],b(1)=[0​−1​],W(2)=[1​−2​],b(2)=0 . The ReLU activation function is used for the first layer. To see how these weights give the correct answer, let’s first “forward” from the input layer to the hidden layer, i.e., . z(1)=XW(1)+b(1)=[0−1101021]. boldsymbol{z}^{(1)} = boldsymbol{X} boldsymbol{W}^{(1)} + boldsymbol{b}^{(1)} = begin{bmatrix} 0 &amp; -1 1 &amp; 0 1 &amp; 0 2 &amp; 1 end{bmatrix}.z(1)=XW(1)+b(1)=⎣⎢⎢⎢⎡​0112​−1001​⎦⎥⎥⎥⎤​. . Applying the activation function to obtain the activation values we get . a(1)=ReLU(z(1))=[00101021], boldsymbol{a}^{(1)} = text{ReLU} left( boldsymbol{z}^{(1)} right) = begin{bmatrix} 0 &amp; 0 1 &amp; 0 1 &amp; 0 2 &amp; 1 end{bmatrix},a(1)=ReLU(z(1))=⎣⎢⎢⎢⎡​0112​0001​⎦⎥⎥⎥⎤​, . where the ReLU function is applied element-wise. We now multiply these activations values with the weights corresponding to the last layer to get . a(2)=z(2)=a(1)W(2)+b(2)=[0110] boldsymbol{a}^{(2)} = boldsymbol{z}^{(2)} = boldsymbol{a}^{(1)} boldsymbol{W}^{(2)} + boldsymbol{b}^{(2)} = begin{bmatrix} 0 1 1 0 end{bmatrix}a(2)=z(2)=a(1)W(2)+b(2)=⎣⎢⎢⎢⎡​0110​⎦⎥⎥⎥⎤​ . Observe that the activation values of the last (output) layer correspond exactly to the values of $ boldsymbol{y}$. .",
            "url": "https://mvanderbroek.com/deep%20learning/python/2018/09/26/Train-XOR-Logic-Gate-in-Neural-Network.html",
            "relUrl": "/deep%20learning/python/2018/09/26/Train-XOR-Logic-Gate-in-Neural-Network.html",
            "date": " • Sep 26, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Map Plots In R In A Tidyverse Way",
            "content": "I show how you can plot your own map in R using a few lines of code using a pipe-based workflow. Several powerful functions of the sf packages are presented. . Analysis . This week I worked on a project for which I needed to create a map plot with some statistics for selected European countries; I was unfamiliar with this kind of plots, so I searched online for possible solutions. I like the tidyverse workflow, so I naturally looked for any tutorials using this style. The first hit was informative, but it didn’t have a high resolution map for Europe. Furthermore, I like to be able to use any custom map, so I searched for ways to import a custom map. . naturalearthdata.com provides many open-source maps. I decided to select the world map with country borders on a 1:10m scale (can be found here). . library(sf) # For handling geospatial data library(ggplot2) # Plotting library library(dplyr) # Data manipulation in tidyverse way library(ggthemes) # Additional themese for the ggplot2 library library(knitr) # Nice tables for this document # This will create a natural-earth subfolder with the map data in the data folder. if (!file.exists(&quot;data/natural-earth&quot;)) { tmp_file &lt;- tempfile(fileext=&quot;.zip&quot;) download.file(&quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip&quot;, tmp_file) unzip(tmp_file, exdir = &quot;data/natural-earth&quot;) } . Importing these maps, however, was not straightforward to me. These lecture slides provides a way to import custom maps, but the syntax of the sp package seems very untuitive with S4 objects for the polygons. Furthermore, the SpatialDataFrame objects do not support a pipe-based workflow. However, this tutorial presents how the modern sf package can be used to manipulate, plot and import spatial data in a tidyverse manner. . Importing our world map is as easy as . map_data &lt;- st_read(&quot;data/natural-earth/&quot;, &quot;ne_10m_admin_0_countries&quot;) . ## Reading layer `ne_10m_admin_0_countries&#39; from data source `Map-Plotting/data/natural-earth&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 255 features and 94 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.6341 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs . The map_data uses data.frames for its features and saves the geometric features as a list in the column geometry. We can now easily explore the data in map_data, e.g., . features_map_data &lt;- map_data %&gt;% as_tibble() %&gt;% select(-geometry) %&gt;% head(10) kable(features_map_data) . featurecla scalerank LABELRANK SOVEREIGNT SOV_A3 ADM0_DIF LEVEL TYPE ADMIN ADM0_A3 GEOU_DIF GEOUNIT GU_A3 SU_DIF SUBUNIT SU_A3 BRK_DIFF NAME NAME_LONG BRK_A3 BRK_NAME BRK_GROUP ABBREV POSTAL FORMAL_EN FORMAL_FR NAME_CIAWF NOTE_ADM0 NOTE_BRK NAME_SORT NAME_ALT MAPCOLOR7 MAPCOLOR8 MAPCOLOR9 MAPCOLOR13 POP_EST POP_RANK GDP_MD_EST POP_YEAR LASTCENSUS GDP_YEAR ECONOMY INCOME_GRP WIKIPEDIA FIPS_10_ ISO_A2 ISO_A3 ISO_A3_EH ISO_N3 UN_A3 WB_A2 WB_A3 WOE_ID WOE_ID_EH WOE_NOTE ADM0_A3_IS ADM0_A3_US ADM0_A3_UN ADM0_A3_WB CONTINENT REGION_UN SUBREGION REGION_WB NAME_LEN LONG_LEN ABBREV_LEN TINY HOMEPART MIN_ZOOM MIN_LABEL MAX_LABEL NE_ID WIKIDATAID NAME_AR NAME_BN NAME_DE NAME_EN NAME_ES NAME_FR NAME_EL NAME_HI NAME_HU NAME_ID NAME_IT NAME_JA NAME_KO NAME_NL NAME_PL NAME_PT NAME_RU NAME_SV NAME_TR NAME_VI NAME_ZH . Admin-0 country | 5 | 2 | Indonesia | IDN | 0 | 2 | Sovereign country | Indonesia | IDN | 0 | Indonesia | IDN | 0 | Indonesia | IDN | 0 | Indonesia | Indonesia | IDN | Indonesia | NA | Indo. | INDO | Republic of Indonesia | NA | Indonesia | NA | NA | Indonesia | NA | 6 | 6 | 6 | 11 | 260580739 | 17 | 3028000 | 2017 | 2010 | 2016 | 4. Emerging region: MIKT | 4. Lower middle income | -99 | ID | ID | IDN | IDN | 360 | 360 | ID | IDN | 23424846 | 23424846 | Exact WOE match as country | IDN | IDN | -99 | -99 | Asia | Asia | South-Eastern Asia | East Asia &amp; Pacific | 9 | 9 | 5 | -99 | 1 | 0 | 1.7 | 6.7 | 1159320845 | Q252 | إندونيسيا | ইন্দোনেশিয়া | Indonesien | Indonesia | Indonesia | Indonésie | Ινδονησία | इंडोनेशिया | Indonézi | a Indonesia | Indonesia | インドネシア | 인도네시아 | Indonesië | Indonezja | Indonési | a Индонезия | Indonesie | n Endonezya | Indonesia | 印度尼西亚 | . Admin-0 country | 5 | 3 | Malaysia | MYS | 0 | 2 | Sovereign country | Malaysia | MYS | 0 | Malaysia | MYS | 0 | Malaysia | MYS | 0 | Malaysia | Malaysia | MYS | Malaysia | NA | Malay. | MY | Malaysia | NA | Malaysia | NA | NA | Malaysia | NA | 2 | 4 | 3 | 6 | 31381992 | 15 | 863000 | 2017 | 2010 | 2016 | 6. Developing region | 3. Upper middle income | -99 | MY | MY | MYS | MYS | 458 | 458 | MY | MYS | 23424901 | 23424901 | Exact WOE match as country | MYS | MYS | -99 | -99 | Asia | Asia | South-Eastern Asia | East Asia &amp; Pacific | 8 | 8 | 6 | -99 | 1 | 0 | 3.0 | 8.0 | 1159321083 | Q833 | ماليزيا | মালয়েশিয়া | Malaysia | Malaysia | Malasia | Malaisie | Μαλαισία | मलेशिया | Malajzia | Malaysia | Malesia | マレーシア | 말레이시아 | Maleisië | Malezja | Malásia | Малайзия | Malaysia | Malezya | Malaysia | 马来西亚 | . Admin-0 country | 6 | 2 | Chile | CHL | 0 | 2 | Sovereign country | Chile | CHL | 0 | Chile | CHL | 0 | Chile | CHL | 0 | Chile | Chile | CHL | Chile | NA | Chile | CL | Republic of Chile | NA | Chile | NA | NA | Chile | NA | 5 | 1 | 5 | 9 | 17789267 | 14 | 436100 | 2017 | 2002 | 2016 | 5. Emerging region: G20 | 3. Upper middle income | -99 | CI | CL | CHL | CHL | 152 | 152 | CL | CHL | 23424782 | 23424782 | Exact WOE match as country | CHL | CHL | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 5 | 5 | 5 | -99 | 1 | 0 | 1.7 | 6.7 | 1159320493 | Q298 | تشيلي | চিলি | Chile | Chile | Chile | Chili | Χιλή | चिली | Chile | Chili | Cile | チリ | 칠레 | Chili | Chile | Chile | Чили | Chile | Şili | Chile | 智利 | . Admin-0 country | 0 | 3 | Bolivia | BOL | 0 | 2 | Sovereign country | Bolivia | BOL | 0 | Bolivia | BOL | 0 | Bolivia | BOL | 0 | Bolivia | Bolivia | BOL | Bolivia | NA | Bolivia | BO | Plurinational State of Bolivia | NA | Bolivia | NA | NA | Bolivia | NA | 1 | 5 | 2 | 3 | 11138234 | 14 | 78350 | 2017 | 2001 | 2016 | 5. Emerging region: G20 | 4. Lower middle income | -99 | BL | BO | BOL | BOL | 068 | 068 | BO | BOL | 23424762 | 23424762 | Exact WOE match as country | BOL | BOL | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 7 | 7 | 7 | -99 | 1 | 0 | 3.0 | 7.5 | 1159320439 | Q750 | بوليفيا | বলিভিয়া | Bolivien | Bolivia | Bolivia | Bolivie | Βολιβία | बोलिविया | Bolívia | Bolivia | Bolivia | ボリビア | 볼리비아 | Bolivia | Boliwia | Bolívia | Боливия | Bolivia | Bolivya | Bolivia | 玻利維亞 | . Admin-0 country | 0 | 2 | Peru | PER | 0 | 2 | Sovereign country | Peru | PER | 0 | Peru | PER | 0 | Peru | PER | 0 | Peru | Peru | PER | Peru | NA | Peru | PE | Republic of Peru | NA | Peru | NA | NA | Peru | NA | 4 | 4 | 4 | 11 | 31036656 | 15 | 410400 | 2017 | 2007 | 2016 | 5. Emerging region: G20 | 3. Upper middle income | -99 | PE | PE | PER | PER | 604 | 604 | PE | PER | 23424919 | 23424919 | Exact WOE match as country | PER | PER | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 4 | 4 | 4 | -99 | 1 | 0 | 2.0 | 7.0 | 1159321163 | Q419 | بيرو | পেরু | Peru | Peru | Perú | Pérou | Περού | पेरू | Peru | Peru | Perù | ペルー | 페루 | Peru | Peru | Peru | Перу | Peru | Peru | Peru | 秘鲁 | . Admin-0 country | 0 | 2 | Argentina | ARG | 0 | 2 | Sovereign country | Argentina | ARG | 0 | Argentina | ARG | 0 | Argentina | ARG | 0 | Argentina | Argentina | ARG | Argentina | NA | Arg. | AR | Argentine Republic | NA | Argentina | NA | NA | Argentina | NA | 3 | 1 | 3 | 13 | 44293293 | 15 | 879400 | 2017 | 2010 | 2016 | 5. Emerging region: G20 | 3. Upper middle income | -99 | AR | AR | ARG | ARG | 032 | 032 | AR | ARG | 23424747 | 23424747 | Exact WOE match as country | ARG | ARG | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 9 | 9 | 4 | -99 | 1 | 0 | 2.0 | 7.0 | 1159320331 | Q414 | الأرجنتين | আর্জেন্টিনা | Argentinien | Argentina | Argentina | Argentine | Αργεντινή | अर्जेण्टीना | Argentí | na Argentina | Argentina | アルゼンチン | 아르헨티나 | Argentinië | Argentyna | Argenti | na Аргентина | Argentin | a Arjantin | Argentina | 阿根廷 | . Admin-0 country | 3 | 3 | United Kingdom | GB1 | 1 | 2 | Dependency | Dhekelia Sovereign Base Area | ESB | 0 | Dhekelia Sovereign Base Area | ESB | 0 | Dhekelia Sovereign Base Area | ESB | 0 | Dhekelia | Dhekelia | ESB | Dhekelia | NA | Dhek. | DH | NA | NA | NA | U.K. Base | NA | Dhekelia Sovereign Base Area | NA | 6 | 6 | 6 | 3 | 7850 | 5 | 314 | 2013 | -99 | 2013 | 2. Developed region: nonG7 | 2. High income: nonOECD | -99 | -99 | -99 | -99 | -99 | -99 | -099 | -99 | -99 | -99 | -99 | No WOE equivalent. | GBR | ESB | -99 | -99 | Asia | Asia | Western Asia | Europe &amp; Central Asia | 8 | 8 | 5 | 3 | -99 | 0 | 6.5 | 11.0 | 1159320709 | Q9206745 | ديكيليا كانتونمنت | দেখেলিয়া ক্যান্টনমেন্ | ট Dekelia | Dhekelia Cantonment | Dekelia | Dhekelia | Ντεκέλια Κάντονμεντ | ढेकेलिया छावनी | Dekéli | a Dhekelia Cantonment | Base di Dheke | lia デケリア | 데켈리아 지 | 역 Dhekelia Cantonme | nt Dhekelia | Dekeli | a Декелия | Dhekeli | a Dhekelia Kantonu | Căn cứ quân sự Dhekelia | NA | . Admin-0 country | 6 | 5 | Cyprus | CYP | 0 | 2 | Sovereign country | Cyprus | CYP | 0 | Cyprus | CYP | 0 | Cyprus | CYP | 0 | Cyprus | Cyprus | CYP | Cyprus | NA | Cyp. | CY | Republic of Cyprus | NA | Cyprus | NA | NA | Cyprus | NA | 1 | 2 | 3 | 7 | 1221549 | 12 | 29260 | 2017 | 2001 | 2016 | 6. Developing region | 2. High income: nonOECD | -99 | CY | CY | CYP | CYP | 196 | 196 | CY | CYP | -90 | 23424994 | WOE lists as subunit of united Cyprus | CYP | CYP | -99 | -99 | Asia | Asia | Western Asia | Europe &amp; Central Asia | 6 | 6 | 4 | -99 | 1 | 0 | 4.5 | 9.5 | 1159320533 | Q229 | قبرص | সাইপ্রাস | Republik Zypern | Cyprus | Chipre | Chypre | Κύπρος | साइप्रस | Ciprus | Siprus | Cipro | キプロス | 키프로스 | Cyprus | Cypr | Chipre | Кипр | Cypern | Kıbrıs Cumhuriyeti | Cộng hòa Síp | 賽普勒斯 | . Admin-0 country | 0 | 2 | India | IND | 0 | 2 | Sovereign country | India | IND | 0 | India | IND | 0 | India | IND | 0 | India | India | IND | India | NA | India | IND | Republic of India | NA | India | NA | NA | India | NA | 1 | 3 | 2 | 2 | 1281935911 | 18 | 8721000 | 2017 | 2011 | 2016 | 3. Emerging region: BRIC | 4. Lower middle income | -99 | IN | IN | IND | IND | 356 | 356 | IN | IND | 23424848 | 23424848 | Exact WOE match as country | IND | IND | -99 | -99 | Asia | Asia | Southern Asia | South Asia | 5 | 5 | 5 | -99 | 1 | 0 | 1.7 | 6.7 | 1159320847 | Q668 | الهند | ভারত | Indien | India | India | Inde | Ινδία | भारत | India | India | India | インド | 인도 | India | Indie | Índia | Индия | Indien | Hindistan | Ấn Độ | 印度 | . Admin-0 country | 0 | 2 | China | CH1 | 1 | 2 | Country | China | CHN | 0 | China | CHN | 0 | China | CHN | 0 | China | China | CHN | China | NA | China | CN | People&#39;s Republic of China | NA | China | NA | NA | China | NA | 4 | 4 | 4 | 3 | 1379302771 | 18 | 21140000 | 2017 | 2010 | 2016 | 3. Emerging region: BRIC | 3. Upper middle income | -99 | CH | CN | CHN | CHN | 156 | 156 | CN | CHN | 23424781 | 23424781 | Exact WOE match as country | CHN | CHN | -99 | -99 | Asia | Asia | Eastern Asia | East Asia &amp; Pacific | 5 | 5 | 5 | -99 | 1 | 0 | 1.7 | 5.7 | 1159320471 | Q148 | جمهورية الصين الشعبية | গণপ্রজাতন্ত্রী চীন | Volksrepublik China | People&#39;s Republic of China | República Popular China | République populaire de Chine | Λαϊκή Δημοκρατία της Κίνας | चीनी जनवादी गणराज् | य Kína | Republik Rakyat Tiongko | k Cina | 中華人民共和国 | 중화인민공화국 | Volksrepubliek China | Chińska Republika Ludowa | China | Китайская Народная Республика | Kina | Çin Halk Cumhuriyeti | Cộng hòa Nhân dân Trung Hoa | 中华人民共和国 | . For this tutorial we want to focus on a European countries, hence we need to filter the data to only contain the european countries’ info. Fortunately, the map_data contains a feature CONTINTENT, so we can easily filter out the unwanted countries. . europe_map_data &lt;- map_data %&gt;% select(NAME, CONTINENT, SUBREGION, POP_EST) %&gt;% filter(CONTINENT == &quot;Europe&quot;) . Lets try to plot a map of European countries. New versions of ggplot2 contain a function geom_sf which supports plotting sf objects directly, so lets try it… . ggplot(europe_map_data) + geom_sf() + theme_minimal() . . That does not seem to work… the reason is that, even though we removed the data of non European countries, we never changed the bbox setting of our data. The bbox object sets the longitude and latitude range for our plot, which is still for the whole europe. To change this we can use the st_crop function as . europe_map_data &lt;- europe_map_data %&gt;% st_crop(xmin=-25, xmax=55, ymin=35, ymax=71) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant ## throughout all geometries ggplot(europe_map_data) + geom_sf() + theme_minimal() . . If you’re familiar with the ggplot2 workflow, it is now easy to construct the aesthetic mappings like you’re used to. Our map_data contains a feature SUBREGION and Europe is divided into Northern, Eastern, Southern and Western Europe. We can easily visualise this in our European map as . ggplot(europe_map_data) + geom_sf(aes(fill=SUBREGION)) + theme_minimal() . . The sf has many in-built functions; one of these functions is st_area which can be used to compute the area of polygons. The population density of each country can be easily plotted by . europe_map_data &lt;- europe_map_data %&gt;% mutate(area = as.numeric(st_area(.))) %&gt;% mutate(pop_density = POP_EST / area) ggplot(europe_map_data) + geom_sf(aes(fill=pop_density)) + theme_minimal() + scale_fill_continuous_tableau(palette = &quot;Green&quot;) . . Using aggregating functions of the tidyverse package is also straight-forward. Lets create a similar population density plot but instead for each subregion of Europe. . subregion_data &lt;- europe_map_data %&gt;% group_by(SUBREGION) %&gt;% summarise(area = sum(area), pop_est = sum(POP_EST)) %&gt;% ungroup() %&gt;% mutate(pop_density = pop_est / area) ggplot(subregion_data) + geom_sf(aes(fill=pop_density)) + theme_minimal() + scale_fill_continuous_tableau(palette = &quot;Green&quot;) . . As a last exercise lets find the centroid for each country. . # First get all centroids of each European country get_coordinates = function(data) { return_data &lt;- data %&gt;% st_geometry() %&gt;% st_centroid() %&gt;% st_coordinates() %&gt;% as_data_frame() } europe_centres &lt;- europe_map_data %&gt;% group_by(NAME) %&gt;% do(get_coordinates(.)) europe_map_data &lt;- europe_map_data %&gt;% left_join(europe_centres, by=&quot;NAME&quot;) . Actually, I only want to see the centroid of the Netherlands… . netherlands_map_data = europe_map_data %&gt;% filter(NAME == &quot;Netherlands&quot;) %&gt;% st_crop(xmin=1, xmax=10, ymin=50, ymax=55) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant ## throughout all geometries ggplot(netherlands_map_data) + geom_sf() + geom_point(aes(x=X, y=Y, colour=&quot;red&quot;)) + theme_minimal() . . Setup . The analysis of this tutorial is performed using R version 3.5.1. To use the st_crop function from the sf package version 0.6.3 is needed. geom_sf also requires a recent version of ggplot2. .",
            "url": "https://mvanderbroek.com/r/visualisation/2018/09/19/Map-Plots-in-R-in-a-Tidyverse-Way.html",
            "relUrl": "/r/visualisation/2018/09/19/Map-Plots-in-R-in-a-Tidyverse-Way.html",
            "date": " • Sep 19, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! My name is Mark van der Broek and I’m passionate about combining operations research, data science and computer science. . History . At the age of 12 I discovered the power of fun in programming by making small hobby websites (offline nowadays) using technologies like HTML, CSS and PHP. During my undergraduate degree in Econometrics and Operations Research I found my love for modeling problems using mathematical models in applications like finance, actuarial sciences, logistics and data analytics. I quickly realised that becoming fluent in the main programming languages like Python, R and C++ was fundemental for applying these mathematical models in real-life. This is why I followed an extra-curricular C++ programming course by Frank Brokken and Jurjen Bokma. More recently with the increasing popularity of machine learning and deep learning I started following the hands-on fast.ai course. . . If you need my full resume, please send me an email. .",
          "url": "https://mvanderbroek.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "JobFunnel (&gt;1.2K stars on github) . . Tool for scraping job websites, and filtering and reviewing the job listings written in Python. . Added proxy support for the scraping to hide your IP address. | Developed user validation procedures for the configuration script. | Set-up unit-test framework and refactored part of the code base to suport unit-testing. | . . ghoR . R package for downloading WHO datasets using their GHO API with data transformation and saving tools. . Set-up skeleton for a complete R package including unit-tests and automatic documentation generation. | Created automatic database detection to prevent downloading existing dataset multiple times. | . . MDSDHVRP-Solver (sexy name alert!) . High-performance meta-heuristic algorithm written in C++ to solve a vehicle routing problem with multiple depots and split-delivery support. . Performed extensive profiling analysis to find bottlenecks in the developed algorithm. | Made major contributions to the code-base in a Object-Oriented-Programming environment. | .",
          "url": "https://mvanderbroek.com/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "Useful Links",
          "content": "Reinforcement Learning . Key RL Papers: Most influential papers in RL research.Ã° | Introduction to Reinforcement Learning: Excellent introduction to Reinforcement Learning | UCL Course in RL: Popular RL course given at University College London by David Silver. | . Machine Learning / Statistics . Towards Model Explainability using SHAP: A new avenue in model explainability for ML models. | Maximum Likelihood and Maximum A Posterior Explained: Understanding these basic concepts in statistics. | The Data Science Lifecycle: Everything about DS projects. | Recipe for Training Neural Networks: Hands-on tips and tricks. | Colah’s Blog: Interesting machine learning blog with excellent visual explanations and intuitions. | Guide to Feature Extraction: Hitchhiker’s guide to feature extraction. | Predictive Maintenance Solutions: Predictive mainentance solutions from a Machine Learning perspective. | It’s not just p=0.048 vs. p=0.052: Another discussion about statistical significance. | Data Project Checklist: Data projects are much more than training an accurate model. | Neat Visualisations: Neat visualisations of important statistical concepts. | . Programming . C++ Quiz: Fun C++ puzzles. | C++ Annotations: Excellent reference manual for the C++ programming language. | Teach Yourself to Program in 10 Years: Not everything happens overnight. | String Formatting in Python 3: Take advantage of the advanced string formatting techniques in Python 3. | The Missing Semester of Your CS Education: Short course to master essential productivity tools for programmers. | The Internals of PostgreSQL: Deep-dive into the internal of the database that everyone uses. | . Miscellaneous . DeepL Translator: Best language translator. | Hacker News: Tech news. | Why Books Don’t Work: Argument why books are ineffective for absorbing knowledge. | 100 Best Books of the 21th Century: Includes some of my favourite books. | Sapiens as a blog post: Relates to the link-above about why books don’t work. This is an excellent way to freshen up your memory of the book. | .",
          "url": "https://mvanderbroek.com/useful/",
          "relUrl": "/useful/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mvanderbroek.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}