{
  
    
        "post0": {
            "title": "Visualising Vehicle Routing Problem Solutions in Python",
            "content": "Visualising results is of major importance for any researcher and practicer. In this article I will show how we can visualise solutions for a particular vehicle routing problem in Python using matplotlib. Before, showing you how this visualisation is done, I first discuss the problem in more detail. The next two section contain a extensive problem definition and a discussion of our solution approach. After, an explanation of the in and output format of the problem is dicussed. Lastly, the code for visualising solutions is provided. . Problem Definition . Consider the problem of dispatching and scheduling vehicles from multiple depots to serve and deliver goods to clients at different locations. One application is supplying products to supermarkets, i.e. clients, from a number of distribution centres. A client can be served by more than one vehicle, for example, if the client demands a large quantity or if the vehicle capacity is small. Serving a customer requires a given service time, which corresponds to activities such as handling and unloading goods from the vehicle. At each depot, different types of vehicles are available, which differ in speed and travel costs. As an example, think of trucks that vary in size and fuel type. The use of vehicles comes at a cost, corresponding to fuel expenditures and driver wages. The objective is to minimize total costs, while serving all customers and taking into account restrictions such as travel time regulations and inventory constraints. . The main characteristics of the problem are the inclusion of multiple depots and the option to split client deliveries. Applications of this problem include waste collection in large cities, multi-warehouse distribution systems, and disaster relief logistics. . Solution Approach . For a course, Niels van der Laan and I developed a meta-heuristic approach for this problem. The implementation can be found here. The meta-heuristic approach is an extension of the sequential large-neighbourhood search algorithm by HemmelMayr (2015). More information about the implementation and results of our meta-heuristic can be found in the report. . An overview of the main algorithm is given below. . . Input and Output Files . The explanation below can also be found in the README of the github repository. . Input file . Now that we have build our solver we can use by providing a file with our problem instance. The input file should be defined according to the following scheme . txt 2 2 30 0 200 8 80 25 1 50 8 110 10 0 57.84 50.38 203 1 1 1 -84.84 -39.79 160 5 1 0 -64.35 -90.76 8 0.05 1 6.77 -84.59 5 0.09 2 -58.57 14.63 15 0.04 3 59.16 89.24 5 0.09 4 46.49 77.78 23 0.08 5 55.12 81.50 19 0.05 6 59.28 89.63 4 0.09 7 51.71 54.42 24 0.06 8 10.22 56.17 21 0.07 9 27.20 -83.00 19 0.09 10 -20.87 63.39 5 0.07 11 3.21 -68.39 4 0.03 12 27.90 -61.73 1 0.03 13 63.60 -25.71 15 0.03 14 32.38 -49.92 8 0.04 15 23.90 -75.21 13 0.02 16 -94.52 39.39 23 0.02 17 -97.87 -47.92 8 0.06 18 -73.88 -47.22 7 0.05 19 -86.10 5.76 17 0.04 20 -69.45 5.35 5 0.05 21 -9.02 95.03 4 0.02 22 0.87 -7.88 24 0.09 23 -97.95 29.66 15 0.03 24 -78.38 -14.60 4 0.02 25 -19.18 -66.77 6 0.07 26 -13.62 -76.94 7 0.01 27 -47.75 -26.75 2 0.04 28 -55.08 -20.71 1 0.01 29 46.03 -22.40 1 0.02 . The first line specifies the number of different vehicles, depots and customers, respectively. . txt 2 2 30 . The second part defines the vehicle number, capacity, maximum duration of trip, speed and cost per unit of distance. . txt 0 200 8 80 25 1 50 8 110 10 . The third part defines the depot id, x-coordinate, y-coordinate, total capacity of items, number of vehicles of type 1 and the number of vehicles of type 2. . txt 0 57.84 50.38 203 1 1 1 -84.84 -39.79 160 5 1 . Note that if we have more vehicle types, then the lines above will be extended as needed. The last part concerns the customers. The numbers on the rows are: id, x-coordinate, y-coordinate, items demanded and the processing time. . txt 0 -64.35 -90.76 8 0.05 1 6.77 -84.59 5 0.09 2 -58.57 14.63 15 0.04 3 59.16 89.24 5 0.09 4 46.49 77.78 23 0.08 5 55.12 81.50 19 0.05 6 59.28 89.63 4 0.09 7 51.71 54.42 24 0.06 8 10.22 56.17 21 0.07 9 27.20 -83.00 19 0.09 10 -20.87 63.39 5 0.07 11 3.21 -68.39 4 0.03 12 27.90 -61.73 1 0.03 13 63.60 -25.71 15 0.03 14 32.38 -49.92 8 0.04 15 23.90 -75.21 13 0.02 16 -94.52 39.39 23 0.02 17 -97.87 -47.92 8 0.06 18 -73.88 -47.22 7 0.05 19 -86.10 5.76 17 0.04 20 -69.45 5.35 5 0.05 21 -9.02 95.03 4 0.02 22 0.87 -7.88 24 0.09 23 -97.95 29.66 15 0.03 24 -78.38 -14.60 4 0.02 25 -19.18 -66.77 6 0.07 26 -13.62 -76.94 7 0.01 27 -47.75 -26.75 2 0.04 28 -55.08 -20.71 1 0.01 29 46.03 -22.40 1 0.02 . Example input files can be found in the src/data folder. . Output file . The output file of the input file in the example looks as follows . txt 58119.35 0 0 595.22 14880.60 103 0.55 7.44 7.99 0 8 1 5 2 15 3 5 4 23 5 19 6 4 7 24 0 1 617.83 6178.27 50 0.29 5.62 5.91 8 21 9 19 10 5 11 4 12 1 1 0 610.17 15254.28 96 0.31 7.63 7.94 13 15 14 8 15 13 16 23 17 8 18 7 19 17 20 5 1 0 608.21 15205.18 63 0.29 7.60 7.89 21 4 22 24 23 15 24 4 25 6 26 7 27 2 28 1 1 0 264.04 6601.02 1 0.02 3.30 3.32 29 1 1 0 0.00 0.00 0 0.00 0.00 0.00 1 0 0.00 0.00 0 0.00 0.00 0.00 1 1 0.00 0.00 0 0.00 0.00 0.00 . The first lines gives the total objective function which is the sum of the cost of all trips by the vehicles while satisfying all demand. The following lines have the following structure . txt &lt;depot id&gt; &lt;vehicle type&gt; &lt;...&gt; &lt;...&gt; &lt;total quantity&gt; &lt;service time&gt; &lt;travel time&gt; &lt;total time&gt; &lt;customer id 1&gt; &lt;quantity customer id 1&gt; &lt;customer id 2&gt; &lt;quantity customer id 2&gt; &lt;...&gt; . Import routing problem . Below you find the python implementation of how to import the input file into python objects. . def import_instance_input(fn): depots = [] customers = [] with open(fn) as f: data = f.readlines() curr_line = 0 # first line contains number of depots, types of vehicles and number of customers problem_info = data[curr_line].split() n_type_vehicles = int(problem_info[0]) n_depots = int(problem_info[1]) n_customers = int(problem_info[2]) # process vehicle information curr_line = 2 for idx in range(0, n_type_vehicles): curr_line += 1 # skip empty line curr_line += 1 # process depots for idx in range(0, n_depots): depot_info = data[curr_line].split() depots.append({ &#39;id&#39;: int(depot_info[0]), &#39;x_coord&#39;: float(depot_info[1]), &#39;y_coord&#39;: float(depot_info[2]), &#39;capacity&#39;: int(depot_info[3]) }) curr_line += 1 # skip empty line curr_line += 1 # process customers for idx in range(0, n_customers): customer_info = data[curr_line].split() customers.append({ &#39;id&#39;: int(customer_info[0]), &#39;x_coord&#39;: float(customer_info[1]), &#39;y_coord&#39;: float(customer_info[2]), &#39;demand&#39;: int(customer_info[3]), &#39;processing_time&#39;: float(customer_info[4]) }) curr_line += 1 return (depots, customers) . depots, customers = import_instance_input(&quot;data/instanceB_52_input.txt&quot;) . Import solution . def import_instance_output(fn): trips = [] with open(fn) as f: data = f.readlines() # first line contains objective value obj_val = float(data[0]) # first empty line and then all trips for line_idx in range(2, len(data)): line = data[line_idx].split() # if vehicle goes on trip, add customers customers = [] customer_idx = 8 while customer_idx &lt; len(line): customers.append({ &#39;id&#39;: int(line[customer_idx]), &#39;load&#39;: int(line[customer_idx+1]) }) customer_idx += 2 trips.append({ &#39;depot_id&#39;: int(line[0]), &#39;vehicle_type&#39;: int(line[1]), &#39;travel_cost&#39;: float(line[2]), &#39;total_cost&#39;: float(line[3]), &#39;load&#39;: int(line[4]), &#39;processing_time&#39;: float(line[5]), &#39;travel_time&#39;: float(line[6]), &#39;total_time&#39;: float(line[7]), &#39;customers&#39;: customers }) return trips . trips = import_instance_output(&quot;data/instanceB_52_output.txt&quot;) . Plot solution . To plot the solution, we first visualise the depots and customers on the map. After, we add all the routes given in the solution file. . However, we first start with an empty canvas to display all of this. . # create empty figure fig, ax = plt.subplots(figsize=(9, 9), dpi=300) # all x and y coordinates lay in interval [-100, 100] ax.set_xlim(-100, 100) ax.set_ylim(-100, 100) # remove ticks ax.set_xticks([]) ax.set_yticks([]) # add title ax.set_title(&quot;MDSDHVRP-I Solution&quot;) . Text(0.5, 1.0, &#39;MDSDHVRP-I Solution&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; df_depots = pd.DataFrame(depots) df_customers = pd.DataFrame(customers) # plot depots plot_depots, = ax.plot(df_depots.x_coord.values, df_depots.y_coord.values, ls=&quot;&quot;, marker=&quot;s&quot;, label=&quot;Depot&quot;) # plot customers plot_customers, = ax.plot(df_customers.x_coord.values, df_customers.y_coord.values, ls=&quot;&quot;, marker=&quot;.&quot;, label=&quot;Customers&quot;) # create legend and plot entity_legend = ax.legend(handles=[plot_depots, plot_customers], bbox_to_anchor=(1.05, 1), title=&quot;Entities&quot;) fig . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Add Trip . Adding routes is not as straight-forward as adding the depots and customers. A route only provides the customer ids and the depot it came from but a route should be a collection of coordinates. Let&#39;s create a routine to do this . def get_trip_coordinates(trip, depots, customers): # define an empty trip coordinates list trip_path = [] # if we have no customers we return an empty trip_path if len(trip[&quot;customers&quot;]) == 0: return trip_path # start location of the trip is the depot trip_path.append({ &#39;x&#39;: depots[trip[&quot;depot_id&quot;]][&quot;x_coord&quot;], &#39;y&#39;: depots[trip[&quot;depot_id&quot;]][&quot;y_coord&quot;] }) # get coordinates of all customers for customer in trip[&quot;customers&quot;]: trip_path.append({ &#39;x&#39;: customers[customer[&quot;id&quot;]][&quot;x_coord&quot;], &#39;y&#39;: customers[customer[&quot;id&quot;]][&quot;y_coord&quot;] }) # lastly, we go back to the depot again trip_path.append({ &#39;x&#39;: depots[trip[&quot;depot_id&quot;]][&quot;x_coord&quot;], &#39;y&#39;: depots[trip[&quot;depot_id&quot;]][&quot;y_coord&quot;] }) return trip_path . Now that we can obtain all coordinates we can simply add the paths as regular line plots. . # map to check how many vehicles of certain type go on a trip for the figure n_vehicles = defaultdict(int) plot_lines = [] for trip in trips: # get path trip_path = get_trip_coordinates(trip, depots, customers) # if trip exists, put in dataframe and plot if len(trip_path) &gt; 0: # create dataframe and increase number of vehicles of this type df_trip = pd.DataFrame(trip_path) n_vehicles[trip[&#39;vehicle_type&#39;]] += 1 plot_line, = ax.plot(df_trip.x, df_trip.y, linewidth=0.7, label=f&quot;VehicleType {trip[&#39;vehicle_type&#39;]} (#{n_vehicles[trip[&#39;vehicle_type&#39;]]})&quot;) plot_lines.append(plot_line) # add extra legend for all trips and original one fig.gca().add_artist(entity_legend) ax.legend(handles=plot_lines, bbox_to_anchor=(1.05, 0.85), title=&quot;Trips&quot;) fig . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Conclusion . In this article we explored how we can plot basic vehicle routing problem solutions using matplotlib in Python. .",
            "url": "https://mvanderbroek.com/operations%20research/python/2020/02/28/Visualise-Vehicle-Routing-Problem-Solutions.html",
            "relUrl": "/operations%20research/python/2020/02/28/Visualise-Vehicle-Routing-Problem-Solutions.html",
            "date": " • Feb 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Logistic Regression In Pytorch From Scratch (lesson 2)",
            "content": "In this article we will explore the logistic regression and how we can implement it using PyTorch. Contrary to linear regressions where the output variable $y$ is continuous, logistic regressions concern binary variables, i.e., . y={1,with probability p,0,with probability 1−p.y = begin{cases} 1, &amp; text{with probability }p, 0, &amp; text{with probability }1-p. end{cases}y={1,0,​with probability p,with probability 1−p.​ . We are interested in modelling the conditional probability of $y = 1$ given $ boldsymbol{x}$, i.e., . p=P(y=1∣X;b)=F(XTb).p = P(y = 1 mid boldsymbol{X}; boldsymbol{b}) = F( boldsymbol{X}^T boldsymbol{b}).p=P(y=1∣X;b)=F(XTb). . As an example, we might want to find the probability of a patient having cancer ($y=1$) given the patient’s medical information ($x$). . . Now take a moment to think about the following: what properties do we want $F$ to have? . . Since we want to model conditional probabilities, we want $F$ to map to the domain $[0, 1]$. It now happens that the sigmoid function, let’s call it $h$, has some very nice properties. It is defined as . h(x)=11+e−x.h(x) = frac{1}{1 + e^{-x}}.h(x)=1+e−x1​. . Ideally, we want $P(y = 1 mid boldsymbol{X}; boldsymbol{b})$ to be close to 1 when $Y$ is 1 and close to 0 when $Y$ is 0. Before explaining how we can find parameters $b$ such that we come closest to the this ideal situation, let’s generate some random data. . %matplotlib inline from fastai.basics import * . n = 100 . Let’s first create a sample of our $ boldsymbol{X}: n times 2$ feature matrix. . x = torch.ones(n, 2) x[:,0].normal_(1, 0.2) x[:,1].normal_(5, 1.) x[:5,:] . tensor([[1.1678, 6.1665], [0.9792, 5.8922], [1.3373, 4.1348], [0.8830, 6.4242], [1.0932, 5.6926]]) . Next, we want to sample our latent variable $ boldsymbol{y}^*$ as . y∗=XTb+ε, boldsymbol{y}^* = boldsymbol{X}^T boldsymbol{b} + boldsymbol{ varepsilon},y∗=XTb+ε, . where $ boldsymbol{b} = [5, -1]$ and $ boldsymbol{ varepsilon} sim text{Logistic}(0, 1)$ . b = tensor(5,-1.) # Create logistic distribution in pytorch using inverse CDF sampling base_distribution = torch.distributions.Uniform(0, 1) transforms = [torch.distributions.SigmoidTransform().inv, torch.distributions.AffineTransform(loc=0, scale=1)] logistic = torch.distributions.TransformedDistribution(base_distribution, transforms) # Take sample of errors and compute y_star error = logistic.rsample([n]) y_star = x@b + error . The dependent variable can be computed as . y={1,if y∗&gt;0,0,else.y = begin{cases} 1, &amp; text{if } y^* &gt; 0, 0, &amp; text{else.} end{cases}y={1,0,​if y∗&gt;0,else.​ . This relates to the probability $p$ as follows begin{align} P(y = 1) &amp;= P(y^* &gt; 0) &amp; &amp;= P( boldsymbol{x}^T boldsymbol{b} + varepsilon &gt; 0) &amp; &amp;= P( varepsilon &gt; - boldsymbol{x}^T boldsymbol{b}) &amp; &amp;= P( varepsilon leq boldsymbol{x}^T boldsymbol{b}) &amp; text{ (logistic regression is symmetric)} &amp;= F( boldsymbol{X}^T boldsymbol{b}) = p. end{align} . y = y_star &gt; 0 . Linear regression . Let’s check what happens if we now try to model the relationship between the conditional probability and $ boldsymbol{X}$ as linear, i.e., . p=XTb+ε,p = boldsymbol{X}^T boldsymbol{b} + boldsymbol{ varepsilon},p=XTb+ε, where $ boldsymbol{ varepsilon} sim (0, sigma^2)$. . Note that although we have no guarantee that $p$ lies in the interval $[0,1]$, this rarely happens. A bigger problem is the heteroskedasticity of the error term. The linear model assumes the errors are homoskedastic, but it is possible to incoorporate heteroskedastic errors by estimating white standard errors. . Since we are dealing with a linear model, we do not need gradient descent and we can compute the MLE estimator in one line. The OLS (and MLE) estimator for $ boldsymbol{b}$ is given by . bMLE=(XTX)−1XTy. boldsymbol{b}_ text{MLE} = ( boldsymbol{X}^T boldsymbol{X})^{-1} boldsymbol{X}^T boldsymbol{y}.bMLE​=(XTX)−1XTy. . This equation follows immediately from the first order condition of the mean-squared error of the model, i.e., . begin{align} 0 = frac{ partial ( boldsymbol{y} - boldsymbol{X}^T boldsymbol{b})^T( boldsymbol{y} - boldsymbol{X}^T boldsymbol{b})}{ partial boldsymbol{b}} &amp;= frac{ partial boldsymbol{y}^T boldsymbol{y}}{ partial boldsymbol{b}} - frac{ partial 2 boldsymbol{b}^T boldsymbol{X}^T boldsymbol{y}}{ partial boldsymbol{b}} + frac{ boldsymbol{X}^T boldsymbol{X} boldsymbol{b}}{ partial boldsymbol{b}} &amp;= -2 boldsymbol{X}^T boldsymbol{y} + 2 boldsymbol{X}^T boldsymbol{X} boldsymbol{b}. end{align} . # Let&#39;s compute the MLE linear regressor b_linear = torch.inverse(x.T@x)@x.T@y.float() y_linear = x@b_linear y_linear_hat = (y_linear &gt; 0.5).float() fig, ax = plt.subplots() ax.scatter(x[:,1], y.float(), label=&#39;y&#39;) ax.scatter(x[:,1], y_linear, label=&#39;y_ols&#39;) leg = ax.legend(); . . Indeed, we observe that almost all observations lay between the $[0, 1]$ interval. . Logistic regression . Unlike the linear regression, the logistic regression has no closed-form solution. The most popular way of estimating the parameters $ boldsymbol{b}$ is to estimate the maximum likelihood estimator: . begin{align} LL( boldsymbol{b}; boldsymbol{X}, boldsymbol{y}) &amp;= prod_{i=1}^N P(Y = 1 mid boldsymbol{x}i; boldsymbol{b})^{y_i} (1 - P(y = 1 mid boldsymbol{x}_i; boldsymbol{b}))^{1-y_i} &amp;= prod{i=1}^N h( boldsymbol{x}_i^T boldsymbol{b})^{y_i} (1 - h( boldsymbol{x}_i^T boldsymbol{b}))^{1-y_i}. end{align} . Remember that we wanted $P(y = 1 mid boldsymbol{x}_i; boldsymbol{b})$ to be close to one when $y_i = 1$. If that’s our goal, it means that we want $LL( boldsymbol{b}; boldsymbol{X}, boldsymbol{y})$ to be as large as possible. In other words, we want to find a $ boldsymbol{b}$ such that $LL( boldsymbol{b}; boldsymbol{X}, boldsymbol{y})$ is maximised. . Since computers do not like the product of many numbers between $[0, 1]$ because it results in floating point problems (why would that be?). Therefore, we take the log of the likelihood function. Since the log is a monotonic function, maximising the likelihood is the same as maximising the log-likelihood. Because our objective is now additive, a last trick that we can use is to divide this log-likelihood by the sample size. This gives . ll(b;X,y)=1N∑i=1Nyilog⁡(h(xiTb))+(1−yi)log⁡(1−h(xiTb)).ll( boldsymbol{b}; boldsymbol{X}, boldsymbol{y}) = frac{1}{N} sum_{i=1}^N y_i log(h( boldsymbol{x}_i^T boldsymbol{b})) + (1-y_i) log(1-h( boldsymbol{x}_i^T boldsymbol{b})).ll(b;X,y)=N1​i=1∑N​yi​log(h(xiT​b))+(1−yi​)log(1−h(xiT​b)). . In PyTorch we can simply formulate this as . def ll(x, y, b): return((1/len(y)) * (torch.log(torch.sigmoid(x@b)[y]).sum() + torch.log(1 - torch.sigmoid(x@b)[~y]).sum())) . Like I mentioned before, this (log-)likelihood function does not have a closed-form solution (check it if you want :)). Therefore, we will apply the (stochastic) gradient descent algorithm to train our model. . b = tensor(b_linear) b = nn.Parameter(b) def update(): loss = ll(x, y, b) loss.backward() if t % 10 == 0: print(loss) with torch.no_grad(): print(b.grad) b.sub_(-lr * b.grad) b.grad.zero_() . lr = 1e-1 for t in range(10): update() print(b) . tensor(-0.6698, grad_fn=&lt;MulBackward0&gt;) tensor([-0.1090, -0.7824]) tensor([-0.0060, -0.2512]) tensor([ 0.0269, -0.0826]) tensor([ 0.0369, -0.0312]) tensor([ 0.0400, -0.0153]) tensor([ 0.0409, -0.0104]) tensor([ 0.0412, -0.0088]) tensor([ 0.0412, -0.0083]) tensor([ 0.0412, -0.0082]) tensor([ 0.0411, -0.0081]) Parameter containing: tensor([ 1.3062, -0.2848], requires_grad=True) . y_log = torch.sigmoid(x@b) y_log_hat = (y_log &gt; 0.5).float() fig, ax = plt.subplots() ax.scatter(x[:,1], y.float(), label=&#39;y&#39;) ax.scatter(x[:,1], y_log, label=&#39;y_logistic&#39;) leg = ax.legend(); . . Evaluating the linear and logistic classification models . The most basic tool to evaluate the performance of our classification models is to compute the accuracy. The accuracy is the percentage of correctly predicted labels $ boldsymbol{y}$. For our linear model we have . (y_linear_hat == y.float()).sum().float() * 100 / len(y) . tensor(75.) . and our logistic model has an accuracy of . (y_log_hat == y.float()).sum().float() * 100 / len(y) . tensor(72.) . Conclusion . In this post we learnt about the one of the oldest techniques to model binary outcome variables: the logistic regression. We discussed the theoretical foundation, an alternative modelling technique in the form of a linear model and we learnt how can train a logistic regression from scratch in PyTorch. . After writing this post I realised that we can easily see how the logistic regression fits into the framework we discussed in the previous post with the three fundamental building blocks. . The architecture $f$ that maps input $ boldsymbol{X}$ to outputs $ boldsymbol{y}$ with parameters $ boldsymbol{ theta}$ is in this case the sigmoid function with parameters; | the learning algorithm is a simple (stochastic) gradient descent; | and the objective function is the likelihood function. | See you for lesson 3 of the fast.ai course! .",
            "url": "https://mvanderbroek.com/fast.ai/deep%20learning/machine%20learning/2019/11/08/Logistic-Regression-in-PyTorch-from-Scratch-(Lesson-2).html",
            "relUrl": "/fast.ai/deep%20learning/machine%20learning/2019/11/08/Logistic-Regression-in-PyTorch-from-Scratch-(Lesson-2).html",
            "date": " • Nov 8, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Common Deep Learning Training Pitfalls (lesson 2)",
            "content": "In lesson 2 of the fast-ai course discusses a variety of items. First, we will learn how to apply a model in production. Second, common pitfalls are being discussed. I will provide my econometric background on these pitfalls and how they relate to the statistics literature. Lastly, we will explore the inner working of most ML/DL algorithms by discussing the most elementary one: the logistic regression (LATER POST). . Airplane classifier in production . After our model is trained, it can be used in practice. Often, this is done by creating a public API. This is often done by creating a website where users can submit their ‘item’ that they want to analyse. The website then predicts the class using the trained model and gives back this class (and corresponding probabilities) to the user. Let’s see how this is done. . First, we export our model so that we can load it into our website. . from fastai.vision import * #learn corresponds to the best model of lesson 1 #learn.export(&#39;classifier.pkl&#39;) . path = &#39;./&#39; img = open_image(path + &#39;A320_easyJet_new_livery_2015.jpeg&#39;) img . . learn = load_learner(path, &#39;plane-classifier.pkl&#39;) pred_class, _, _ = learn.predict(img) print(pred_class) . airbus . Although our model does not have the highest accuracy, we do manage to obtain the right class for this photo but note that a model that we randomly give back a class would also be right 50% of the time for any given photo… . To make this model available on a website you can use a service like Render or create a simple flask application. The fast-ai course provides several ways to put a model into production, e.g., click here. . Common pitfalls . Introduction . Deep learning models consists of three fundamental parts: architecture, learning process and objection function. The pitfalls in lesson 2 mostly concern the learning process but note that these parts are not fully decoupled: they are all related. . Let’s consider a function fff that maps an input matrix X boldsymbol{X}X to an output vector y boldsymbol{y}y using parameters θ boldsymbol{ theta}θ, i.e., . y=f(θ;X). boldsymbol{y} = f( boldsymbol{ theta}; boldsymbol{X}).y=f(θ;X). . This type of problem of mappping an input to an output is called supervised learning. We want to have a function with parameters θ boldsymbol{ theta}θ that provides an output vector y^ hat{ boldsymbol{y}}y^​ that most closely resembles the real output vector y boldsymbol{y}y. The question raises, how to judge that our model is “good”? This is where the objective function comes in. This function, let’s call it ccc, takes the predicted outputs y^ hat{ boldsymbol{y}}y^​ and the real outputs y boldsymbol{y}y and tells us how “close” the values are. Most objective functions are defined on the interval between 0 and infinity and outputs a low number when the predicted and real values are close and a large value when they are far apart. An example of such a cost function is the mean-squared error function, i.e., . cMSE(y^,y)=∑i(y^i−yi)2.c_ text{MSE}( hat{ boldsymbol{y}}, boldsymbol{y}) = sum_i ( hat{y}_i - y_i)^2.cMSE​(y^​,y)=i∑​(y^​i​−yi​)2. . Fundamental building blocks . Remember that y boldsymbol{y}y and X boldsymbol{X}X consistute the data we have and fff with parameters θ boldsymbol{ theta}θ and the objective function ccc are the modelling choices. The three main questions for any deep/machine learning problem are: . What architecture fff works for our data? | How do we learn the parameters θ boldsymbol{ theta}θ? | What defines a good model? | Lesson 2 of the fast-ai course mainly concerns problems with question 2: it is about the learning process. Almost all research and applications are based on so-called gradient-based optimisation. The comes from the idea that any function fff at point x boldsymbol{x}x can be written as the taylor series expansion, i.e., . begin{equation} label{taylor} tag{1} f( boldsymbol{x}) = f( boldsymbol{x}_0) + ( boldsymbol{x} - boldsymbol{x}_0)^T frac{ partial f( boldsymbol{x}_0)}{ partial boldsymbol{x}} + frac{1}{2}( boldsymbol{x} - boldsymbol{x}_0)^T frac{ partial^2 f( boldsymbol{x}_0)}{ partial boldsymbol{x}^2}( boldsymbol{x} - boldsymbol{x}_0) + dots end{equation} . . Note: the link between this function f(x)f( boldsymbol{x})f(x) to our deep learning framework is that given data (X,y)( boldsymbol{X}, boldsymbol{y})(X,y) we want to optimise the objection function ccc using architecture fff by selecting the best parameters θ boldsymbol{ theta}θ, i.e. we can write the correspondence . f(x)  ⟺  c(θ)=c(f(θ;X),y).f( boldsymbol{x}) iff c( boldsymbol{ theta}) = c(f( boldsymbol{ theta}; boldsymbol{X}), boldsymbol{y}).f(x)⟺c(θ)=c(f(θ;X),y). . . The idea of gradient-based optimisation is to “improve” our function fff by changing the values of x boldsymbol{x}x slightly at every step. If x0 boldsymbol{x}_0x0​ is the current value then we compute the gradient g boldsymbol{g}g of fff at point x0 boldsymbol{x}_0x0​ and take a small step in this direction, i.e., . x1=x0−ϵg. boldsymbol{x}_1 = boldsymbol{x}_0 - epsilon boldsymbol{g}.x1​=x0​−ϵg. If we fill this into the equation eqref{taylor} we get . begin{equation} label{true_learn} tag{2} f( boldsymbol{x}_1) = f( boldsymbol{x}_0) - epsilon boldsymbol{g}^T boldsymbol{g} + frac{1}{2} epsilon^2 boldsymbol{g}^T boldsymbol{H} boldsymbol{g} + dots, end{equation} . where H=∂2f(x0)∂x2 boldsymbol{H} = frac{ partial^2 f( boldsymbol{x}_0)}{ partial boldsymbol{x}^2}H=∂x2∂2f(x0​)​. The idea of gradient-based optimisation comes from the fact that for small values of ϵ epsilonϵ the last term diminishes so that we can write . begin{equation} label{approx_learn} tag{3} f( boldsymbol{x}_1) approx f( boldsymbol{x}_0) - epsilon boldsymbol{g}^T boldsymbol{g}. end{equation} . Since gTg≥0 boldsymbol{g}^T boldsymbol{g} geq 0gTg≥0 by definition, we have that fff decreases when we take small steps along the gradient. Hence, we iteratively improve our objective function. . Jeremy Howard from the fast-ai course discusses some common pitfalls in learning the model parameters. I will discuss these pitfalls using the knowledge of the gradient-based optimisation that we just learnt. . Learning rate too high . Although state-of-the-art models do not directly use the gradient to improve the objective function it is closely related. In our gradient-descent update step in eqref{approx_learn} the learning rate refers to ϵ epsilonϵ. Before discussing why a high ϵ epsilonϵ is bad, let’s consider this figure. . . In the plot on the right we see what happens when the learning rate is too high. The reason for this divergence is that our approximation only works when epsilon is small. If epsilon gets larger, the last term of equation eqref{true_learn} and higher order terms generally are no longer close to zero. Hence, the true value f(x1)f( boldsymbol{x}_1)f(x1​) might be larger than f(x0)f( boldsymbol{x}_0)f(x0​) resulting in the behaviour seen in the right graph. . Learning rate too low . If the learning rate is very low, then the gradient-based optimisation and its first-order linear approximation as in eqref{approx_learn} accurately model the cost function ccc close to the last parameter values θ boldsymbol{ theta}θ. However, if the learning rate is too low, it will take many epochs to reach some kind of local minimum. Furthermore, if our function is not globally convex, we might never reach a good local minimum since we cannot escape the current path to the nearest local minimum. . Too few epochs . One problem with too few epochs relates to the figure above. In the left graph, we see that when the learning rate is too low, then it takes many epochs to reach the minimum. Hence, if we set the number of epochs too low, we will not come close to this minimum. . Another problem with too few epochs arises when the objective function is no longer globally convex (which it never is!!). Consider the following figure which shows the “surface” of an objective function with two parameters, say θ0 theta_0θ0​ and θ1 theta_1θ1​. . . If we start with randomly selected paramater values it is unlikely that we will ever reach the global minimum of the above objective function. There are ways to deal with this, such as more advanced learning algorithms instead of our basic gradient-descent algorithm in eqref{approx_learn} (which still consistutes the basis of all these algorithms). However, if we only allow for a small number of epochs, the learning process does not have the opportunity to explore many regions of the objective surface. . Too many epochs . Too many epochs is different from the other pitfalls we have seen so far. While the previous pitfalls relate to finding parameters θ boldsymbol{ theta}θ that minimises the objective function, too many epochs relates to a problem concerning the data. . When we train our model, we only have access to a limited (although often large) sample of observations. When we train our model on this training data with many epochs using a good learning algorithm we might be able to find a very good local minimum of the objective function. This would correspond to the right graph of the following figure . . Our mean squared error is almost zero. Excellent! Now, what happens when we obtain a new sample and check the objective function under this new data? It is likely that the model on the middle now outperforms the model on the right, since it captures the real relationship between the input data X boldsymbol{X}X and the output y boldsymbol{y}y better than the overfitted model. . One remark related to modern model architectures: models are typically so large (in the number of parameters) that it is almost impossible to overfit the model as in the above figure. It would simply require too many epochs and a very good learning algorithm to ever overfit like this. . Conclusion . We learnt how to export our model and call our model on a new observation to predicts its class. In the second part, we discussed a very high-level overview of machine/deep learning models and its fundamental building blocks, namely architecture, learning algorithm and objective function. Lastly, I discussed common pitfalls with regards to training a deep learning network. .",
            "url": "https://mvanderbroek.com/fast.ai/deep%20learning/2019/11/07/Common-Deep-Learning-Training-Pitfalls-(Lesson-2).html",
            "relUrl": "/fast.ai/deep%20learning/2019/11/07/Common-Deep-Learning-Training-Pitfalls-(Lesson-2).html",
            "date": " • Nov 7, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Airplane Classifier With Fast.ai Library (lesson 1)",
            "content": "The fast.ai deep learning course is a practical top-down deep learning course for practicioners. Immediately after the first two hour lecture, it is possible to train an image classifier on your own dataset using state-of-the-art deep learning techniques. The reason for this accesibility is the excellent fast.ai software which builds upon the popular PyTorch deep learning library. . In this post I will show you how I applied the techniques and lessons from the first fast.ai lecture to a plane classifier. I will show you how to download your own dataset, how to train an image classifier using the fast.ai software, and finally how to evaluate the performance of the trained model. . Create Data . The easiest way to create your own dataset for your image classifier is to follow the instructions of the fast.ai notebook which can be found here. Using these instructions I downloaded images of the boeing 737 and airbus A320 airplanes. While, for most people these planes are indistinguisable from each other, there are small differences between the planes. These include the shape of the nose, the wing tips and the position of the engines under the wings. . After downloading the images in their subfolders data/boeing and data/airbus I wanted to split this data into a training, validation and testing set. Since I could not find a simple tool to just give my folder of images and do this split for me, I created a little tool. This creates three folders, data/train, data/validation and data/test. These folders then contain the subfolders corresponding to the classes found in the original data folder, namely, boeing and airbus. . Load Data . We first import some functions from the fast.ai library that we will use for our analysis. . %reload_ext autoreload %autoreload 2 %matplotlib inline . from fastai.vision import * from fastai.metrics import error_rate from fastai.callbacks.tracker import SaveModelCallback . Now, we will select our folder containing the images and load it into the fast.ai library. . help(get_image_files) path = &#39;/home/jupyter/data&#39; . Help on function get_image_files in module fastai.vision.data: get_image_files(c: Union[pathlib.Path, str], check_ext: bool = True, recurse=False) -&gt; Collection[pathlib.Path] Return list of files in `c` that are images. `check_ext` will filter to `image_extensions`. . data = ImageDataBunch.from_folder(path, valid=&#39;validation&#39;, ds_tfms=get_transforms(), size=224).normalize(imagenet_stats) . Let’s check out the sizes of our training and validation set and load a small sample of our images. . data.classes, data.c, len(data.train_ds), len(data.valid_ds) . ([&#39;airbus&#39;, &#39;boeing&#39;], 2, 245, 80) . data.show_batch(rows=3, figsize=(7,8)) . . Train ResNet 34 model . To train our plane classifier we use the state-of-the-art ResNet34 model as our basis. We first try to see what accuracy we can obtain by training the last layers. As we will see, this gives unsatisfactory results. We can simply load the ResNet34 model directly from the fast.ai library and start training it with only two lines of code! . learn = cnn_learner(data, models.resnet34, metrics=[error_rate, accuracy]) . learn.fit_one_cycle(10, callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;accuracy&#39;, name=&#39;model&#39;)]) . epoch train_loss valid_loss error_rate accuracy time . 0 | 1.411946 | 1.175786 | 0.437500 | 0.562500 | 00:06 | . 1 | 1.193475 | 0.849596 | 0.450000 | 0.550000 | 00:04 | . 2 | 1.086779 | 0.961117 | 0.412500 | 0.587500 | 00:04 | . 3 | 0.996292 | 1.146309 | 0.375000 | 0.625000 | 00:05 | . 4 | 0.925049 | 0.991192 | 0.375000 | 0.625000 | 00:04 | . 5 | 0.856052 | 0.941140 | 0.325000 | 0.675000 | 00:04 | . 6 | 0.813291 | 0.995522 | 0.350000 | 0.650000 | 00:04 | . 7 | 0.752976 | 1.061707 | 0.350000 | 0.650000 | 00:04 | . 8 | 0.715641 | 1.104084 | 0.375000 | 0.625000 | 00:05 | . 9 | 0.676570 | 1.111519 | 0.375000 | 0.625000 | 00:04 | . Clearly our model is still very bad. We might want to try to ‘unfreeze’ our model to train all layers instead of the last ones. Furthermore, to speed-up training, we can fix the learning rate of the learning algorithm by monitoring which rates give the best training results. Another trick to obtain a good model is to use early-stopping. Early-stopping helps us with overfitting by recognising when the training loss keeps decreasing but the validation loss no longer decreases. This happens when we overfit our model and optimise our model for the training set and therefore results in worse generalisation performance as can be seen in the rise of validation loss. . learn.unfreeze() learn.lr_find() . LR Finder is complete, type {learner_name}.recorder.plot() to see the graph. . learn.recorder.plot() . . learn.fit_one_cycle(13, max_lr=slice(1e-4,1e-2), callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;accuracy&#39;, name=&#39;model&#39;)]) . epoch train_loss valid_loss error_rate accuracy time . 0 | 0.393597 | 1.036158 | 0.275000 | 0.725000 | 00:05 | . 1 | 0.455709 | 2.487670 | 0.437500 | 0.562500 | 00:05 | . 2 | 0.508346 | 4.054496 | 0.512500 | 0.487500 | 00:05 | . 3 | 0.625813 | 3.290894 | 0.375000 | 0.625000 | 00:05 | . 4 | 0.735456 | 6.999222 | 0.550000 | 0.450000 | 00:05 | . 5 | 0.684926 | 3.586192 | 0.450000 | 0.550000 | 00:05 | . 6 | 0.622755 | 2.036758 | 0.425000 | 0.575000 | 00:05 | . 7 | 0.582402 | 1.667793 | 0.312500 | 0.687500 | 00:05 | . 8 | 0.524937 | 1.206267 | 0.300000 | 0.700000 | 00:05 | . 9 | 0.476152 | 0.784825 | 0.225000 | 0.775000 | 00:05 | . 10 | 0.437106 | 0.674155 | 0.200000 | 0.800000 | 00:05 | . 11 | 0.402961 | 0.615875 | 0.175000 | 0.825000 | 00:05 | . 12 | 0.368615 | 0.577250 | 0.175000 | 0.825000 | 00:05 | . Better model found at epoch 0 with accuracy value: 0.7250000238418579. Better model found at epoch 9 with accuracy value: 0.7749999761581421. Better model found at epoch 10 with accuracy value: 0.800000011920929. Better model found at epoch 11 with accuracy value: 0.824999988079071. . learn.save(&#39;model-1&#39;) . Interpret results . Now that we have trained our model we want to do some inferences. Luckily, fast.ai got us covered. The ClassificationInterpration class provides many handy tools to check the performance of our model. Let’s check out the confusion matrix to see where things go wrong. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . While we do manage to classify many planes correctly, I am curious on what images our model has problems. We can check this out as follows. . interp.plot_top_losses(9, figsize=(15,11)) . . Mmm… personally it is unclear to me why the model has problems identifying these images. In the above we only see the images in square format. I am wondering whether the model is also trained on these squared images or that it uses the original typically horizontal images. . Conclusion . While our model has okay performance. I am not sure what the model has learnt exactly. . There are several things I am interested in learning on my fast-ai and deep learning journey in general: . Is there a way to tell what our model has learnt? For our plane example, the difference between the boeing and airbus planes is mostly in the wing design and shape of the nose of the plane. Is there a way to check whether our model has been able to learn these features? | So far it has not been clear to me how image classifiers deal with non-square images. Does it crop the images in to use in the model or does the ResNet model allow for variable sized images? | How do you decide to use a certain transformation of the image data. Is it always okay to augment our dataset by adding transformation of our image data like turning or mirroring images. | . I hope to see you in the next post about fast.ai! .",
            "url": "https://mvanderbroek.com/fast.ai/deep%20learning/2019/11/06/Airplane-Classifier-with-Fast.ai-library-(Lesson-1).html",
            "relUrl": "/fast.ai/deep%20learning/2019/11/06/Airplane-Classifier-with-Fast.ai-library-(Lesson-1).html",
            "date": " • Nov 6, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Custom Data Splitter Function In Python",
            "content": "Ths month I started with the excellent Deep Learning course by fast.ai and I just finished the first lesson. At the end of the lesson Jeremy, the teacher of the course, assigned a homework assignment to create an image classifier using our own data. After downloading my own dataset I was facing the problem that I wanted to split this dataset into a training and test set using seperate directories but I could not easily find such a program online. Therefore, in this blog post, I will explain how to write such a program and how to make this program easily installable and useable for anyone. I will discuss how to properly handle arguments for command line programs and how to make your software available to download from pip. . Motivation . Most of these image classifier tutorials use a dataset that correctly classifies most of the images using state-of-the-art image classifier models. However, I was curious how these so-called transfer learning techniques would work on a rather challenging dataset. Using a google image downloader, I obtained a dataset with approximately 1,000 Airbus A320 planes and 1,000 Boeing 737s. For a layman, these planes are identical but there are small differences. . The problem I was facing is that created a directory plane_data and two subdirectories airbus and boeing containing the respective image files but I wanted to split this dataset into a training and test dataset. While it is relatively straight-forward to split a data file, e.g., CSV files containing one observation per row, splitting directories containing image files is less straightforward. Hence, I decided to write a little terminal program in python to do the manual work for me. . The Program . The goal of the program is split a directory of images and split it into a training and test directory. . Let’s call our directory with images data with classes boeing and airbus. The two most used ways of saving images beloning to a certain class is to put them in a seperate subdirectory, e.g., data/boeing or to put the class name in the name of the image file, e.g., boeing_img1.jpeg. As of now, I only implemented the former case but the second case is typically implemented using class detection with regex. . The source code of the program is given below. I think the code is pretty self-explanatory but the idea is to 1) identify the subdirectories containing all classes, 2) create a data/train and data/test subdirectory and 3) loop over all classes and split the image files in a train and test selection and move the images to the respective directories, e.g., data/train/boeing and data/test/boeing. Note that the folder variable is the location of the images and the train variable is the percentage of images that we want to keep for training, which is defined as a program option. . def data_splitter(folder, train): entries = [item for item in listdir(folder) if not item.startswith(&#39;.&#39;)] # Get all folders and files dirs = [d for d in entries if isdir(join(folder, d))] files = [f for f in entries if isfile(join(folder, f))] if dirs and files: print(&quot;Folder should contain either files or folders but not both.&quot;) sys.exit() mkdir(join(folder, &#39;train&#39;)) mkdir(join(folder, &#39;test&#39;)) if files: dirs = [&quot;&quot;] for directory in dirs: # Items belonging to the current class items = [item for item in listdir(join(folder, directory)) if not item.startswith(&#39;.&#39;) and isfile(join(folder, directory, item))] if directory != &quot;&quot;: mkdir(join(folder, &#39;train&#39;, directory)) mkdir(join(folder, &#39;test&#39;, directory)) # Shuffle and split dataset according to fractions random.shuffle(items) train_sel = int(len(items) * (train / 100)) train_entries = items[0:train_sel] test_entries = items[train_sel:] for train_file in train_entries: shutil.move(join(folder, directory, train_file), join(folder, &#39;train&#39;, directory, train_file)) for test_file in test_entries: shutil.move(join(folder, directory, test_file), join(folder, &#39;test&#39;, directory, test_file)) if directory != &quot;&quot;: rmdir(join(folder, directory)) . Handling Arguments . The program needs a folder to split up and an optional percentage of image files to put in the training set. While python offers an in-built argument parser in the argparser module, the click module has my preference due to its easy of use. . Adding arguments and options is as simple as adding . @click.command() @click.argument(&#39;folder&#39;) @click.option(&#39;--train&#39;, default=80, help=&quot;Percentage of files for train set&quot;) . In front of our data_splitter(folder, train) program. A click.option allows us to define a default value in case the user does not define the option and a little helper text when the user calls the --help option. . Deploying on pip . Lastly, we want to make it dead-easy to install and use our little command-line program. The most popular method to distribute software in python is to use pip. To make our module suitable for pip, we need to add two files to our module in the main directory: a setup.py and setup.cfg file. An example can be found here. . We further add the following to the data_splitter.py code to make the program easily calleable from pip as we will see in the next section. . if __name__ == &#39;__main__&#39;: try: data_splitter() except FileNotFoundError as fnf_error: print(fnf_error) . The try-except block prevents us from having an ugly and long error callback in case we want to split a non-existing directory. . Using data_splitter . Now that we defined the program, handled program arguments and made our program deployable on pip we can start using it. . The program can be found in the directory https://github.com/markkvdb/data-splitter and can be installed as . pip install git+https://github.com/markkvdb/data-splitter.git#egg=data-splitter . The installer will place the program location in the $PATH variable of your system. . After only adding three lines of code using the click module, we now have a very neat interface when using the program in the command-line. Calling data_splitter --help will show how to use it . Usage: data_splitter [OPTIONS] FOLDER Options: --train INTEGER Percentage of files for train set. --help Show this message and exit. . Improvements . The simple command-line can be improved in various ways: . Implement class recognition using image file names with regex; | Splitting is performed by randomly assigning images to the test or training set. This is not always appriorate if some images belong to the same individual and images are no longer independent. | .",
            "url": "https://mvanderbroek.com/python/2019/10/31/Custom-Data-Splitter-Function-in-Python.html",
            "relUrl": "/python/2019/10/31/Custom-Data-Splitter-Function-in-Python.html",
            "date": " • Oct 31, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Hidden Markov Model Tutorial In R",
            "content": "This document contains an introduction to Hidden Markov Models (HMMs). First, a brief description and the main problems of HMMs will discussed. After, I will provide common strategies to analyse these problems. Lastly, I apply the HMM framework on a speech recognition problem. . Model Formulation . A HMM models a Markov process which affects some observerable process(es). Any HMM model can be defined with 5 elements, namely: . The set of NNN hidden states V={v1,…,vN}V = {v_1, dots, v_N }V={v1​,…,vN​}; | The transition matrix QQQ where the i,ji,ji,j -th element represents the transition probability of going from hidden state xix_ixi​ to xjx_jxj​; | A sequence of TTT observations Y={y1,…,yT}Y = {y_1, dots, y_T }Y={y1​,…,yT​}, each drawn from observation set D={d1,…,dd}D = {d_1, dots, d_d }D={d1​,…,dd​}; | Functions bi(yt)b_i(y_t)bi​(yt​) that contain the probability of particular observation at time ttt given that the process is in state viv_ivi​. The entire set of functions is denoted by B={bj(⋅):∀j∈[N]}B = {b_j( cdot): forall j in [N] }B={bj​(⋅):∀j∈[N]}; | The initial hidden state probabilities for time t=0t=0t=0: π=[π1,…,πN] pi = [ pi_1, dots, pi_N]π=[π1​,…,πN​]. | We indicate λ lambdaλ as a short-hand notation for the complete set of HMM parameters, i.e., λ=(Q,B,π) lambda = (Q, B, pi)λ=(Q,B,π). The three main problems associated with HMMs are: . Find P(Y∣λ)P(Y mid lambda)P(Y∣λ) for some observation sequence Y=(y1,…,yT)Y = (y_1, dots, y_T)Y=(y1​,…,yT​). | Given some YYY and λ lambdaλ, find the best (hidden) state sequence X=(x1,…,xT)X = (x_1, dots, x_T)X=(x1​,…,xT​). | Find the HMM parameters that maximises P(Y∣λ)P(Y mid lambda)P(Y∣λ), i.e., find λ∗=argmaxλP(Y∣λ) lambda^* = text{argmax}_{ lambda}P(Y mid lambda)λ∗=argmaxλ​P(Y∣λ). | In the remainder of this article I will provide approaches to solve each of these problems and provide an implementation in R. Before discussing an interesting application for HMMs, I will provide a very simple HMM to discuss the three main problems for clarity. . This simple HMM example in R is given below: . # Define model V = c(&quot;HOT&quot;, &quot;COLD&quot;) Q = matrix(c(0.7, 0.3, 0.4, 0.6), nrow=2, byrow=TRUE) D = c(1, 2, 3) Y = c(1, 3, 2, 3) B = matrix(c(0.2, 0.4, 0.4, 0.6, 0.3, 0.1), nrow=3) pi = c(0.5, 0.5) . Forward probabilities . One interesting problem for HMMs is determining the likelihood of a given sequence of observations given the HMM parameters λ lambdaλ. As opposed to regular Markov models, this is not straight-forward to compute, since we do not know the underlying hidden state sequence. . One possible solution would be to compute the likelihood of a given observation sequence by all possible hidden state sequences that support this observation sequene. In our toy model, the observation space does not depend on the hidden state, hence all sequences of hidden states have to be considered. . This method is commonly referred to as the law of total expectations or Tower rule. The idea is that we compute P(Y)P(Y)P(Y) by using: . P(Y∣λ)=∑x∈XP(Y∣x,λ)P(x∣λ),P(Y mid lambda) = sum_{x in X} P(Y mid x, lambda)P(x mid lambda),P(Y∣λ)=x∈X∑​P(Y∣x,λ)P(x∣λ), . where XXX is the set of all valid hidden state sequences, e.g., X={VT}X = {V^T }X={VT}. We can compute the conditional probability of an observation sequence given the hidden state sequence as . P(Y∣x,λ)=∏t=1Tbxt(yt).P(Y mid x, lambda) = prod_{t=1}^T b_{x_t}(y_t).P(Y∣x,λ)=t=1∏T​bxt​​(yt​). . Below, I have the R code that computes the likelihood by brute-force: . # Transform hidden state set to numerical set V_num = seq(1, length(V)) # All possible hidden state sequences 2^12 V_all = permutations(n=length(V), r=length(Y), repeats.allowed=TRUE) # Compute the likelihood given a hidden state sequence get_likelihood = function(V_seq, Y, B, pi, Q) { l1 = prod(B[matrix(c(Y, V_seq), ncol=2)]) # Compute all transition probabilitoes Q_el = matrix(c(V_seq[1:(length(V_seq)-1)], V_seq[2:length(V_seq)]), ncol=2) l2 = pi[V_seq[1]] * prod(Q[Q_el]) return(l1 * l2) } total_l = sum(apply(V_all, 1, get_likelihood, Y, B, pi, Q)) print(total_l) ## [1] 0.0099748 . This brute-force algorithm is extremely inefficient and is not applicable when the state space and/or sequence length is large. Just like (regular) Markov models we can use the Markov property to compute the likelihood in a more efficient manner. Here, we make use of the fact that the transition probabilities of jumping to certain states only depends on the current state. Let YtY_tYt​ denote the subset of YYY of the first ttt observations and let XtX_tXt​ be the set of hidden state sequences up to time ttt. Recall that we want to compute P(Y)P(Y)P(Y). Let αt(j) alpha_t(j)αt​(j) represent the probability of being in state jjj at time ttt after seeing the first ttt observations YtY_tYt​, given our model specification, i.e., αt(j) = P(Ot, xt = j ∣ λ). By the law of total expectation we have that . We can use this recursively relationship to efficiently compute P(YT∣λ)P(Y_T mid lambda)P(YT​∣λ) by means of the which is presented in Alg. (1). . The R-implementation can be found below . forward_alg = function(Y, V, pi, B, Q) { # Define empty forward matrix forward = matrix(data=0, nrow=length(V), ncol=length(Y)) # Fill first elements forward[,1] = B[Y[1],] * pi # Now for all other time steps for (t in 2:length(Y)) { forward[,t] = forward[,t-1] %*% Q * B[Y[t],] } return(forward) } alpha = forward_alg(Y, V_num, pi, B, Q) print(sum(alpha[, length(Y)])) ## [1] 0.0099748 . We see that the brute-force method and the forward algorithm produce the same likelihood for our sequence. . Decoding Hidden States . The forward algorithm can be used to determine the likelihood of a certain observed sequence given the Markov model and the hidden state sequence. Note, however, that the hidden state sequence is not observed! It is more interesting to compute the most likely hidden state sequence given the underlying Markov model and the observed state sequence. This task of determining the hidden state sequence is refered to as the. . One possible way to determine the hidden state sequence would be to compute the likelihood of all possible hidden state sequences using the forward algorithm and select the hidden state sequence with the highest likelihood value. Similar to determining the likelihood, this brute-force approach quickly becomes intractable. Instead, we can apply the dynamic programming algorithm called the to decode the hidden state sequence. . Similar to the forward algorithm, the proceeds through the time-series from the start till the end. The can be computed as . (x1∗,…,xT∗)=arg max⁡(x1,…,xT)∈XP(YT,XT∣λ)P(XT∣λ).(x^*_1, dots, x^*_T) = argmax_{(x_1, dots, x_T) in X} P(Y_T, X_T mid lambda) P(X_T mid lambda).(x1∗​,…,xT∗​)=(x1​,…,xT​)∈Xargmax​P(YT​,XT​∣λ)P(XT​∣λ). . Similar to the forward algorithm, we can aply a dynamic programming approach to compute the. Let vt(j)v_t(j)vt​(j) be the probability of observing sequence YtY_tYt​ using the hidden sequence (x1∗,…,xt−1∗)(x^*_1, dots, x^*_{t-1})(x1∗​,…,xt−1∗​), that is, . The psuedo-code for the is given in Alg 2. . The R implementation of the Viterbi algorithm is given below: . viterbi_alg = function(Y, V, pi, B, Q) { # Define empty forward matrix T_ = length(Y) viterbi = matrix(data=0, nrow=length(V), ncol=length(Y)) path = matrix(data=0, nrow=length(V), ncol=length(Y)) # Fill first elements viterbi[,1] = B[Y[1],] * pi path[,1] = rep(0, length(V)) # Now for all other time steps for (t in 2:length(Y)) { tmp_val = t(viterbi[,t-1] * Q) max_x = max.col(tmp_val) viterbi[,t] = tmp_val[,max_x][, 1] * B[Y[t],] path[,t] = max_x } best_path_prob = max(viterbi[,T_]) best_path_end = which.max(viterbi[,T_]) # Best path best_path = rep(-1, T_) best_path[T_] = best_path_end for (t in (T_-1):1) { best_path[t] = path[best_path[t+1],t+1] } return(list(best_path_prob, best_path)) } viterbi_results = viterbi_alg(Y, V_num, pi, B, Q) print(viterbi_results[[1]]) ## [1] 0.0037632 print(V[viterbi_results[[2]]]) ## [1] &quot;COLD&quot; &quot;HOT&quot; &quot;HOT&quot; &quot;HOT&quot; . Determining the Optimal HMM Parameters . The standard algorithm to estimate the optimal HMM parameters is the Baum-Welch (BW) algorithm which is a special case of the Expectation-Maximisation algorithm. The BW algorithm iteratively updates the HMM parameters and converges to the optimal HMM parameters under mild convergence conditions. . Before discussing the BW algorithm we need some useful probabilities. First, similar to the forward probabilities αt(j)=P(y1,…,yt,xt=j∣λ) alpha_t(j) = P(y_1, dots, y_t, x_t = j mid lambda)αt​(j)=P(y1​,…,yt​,xt​=j∣λ) we can compute the backward probabilities βt(j)=P(yt+1,…,yT∣xt=j,λ) beta_t(j) = P(y_{t+1}, dots, y_T mid x_t = j, lambda)βt​(j)=P(yt+1​,…,yT​∣xt​=j,λ), i.e., given our HMM parameters λ lambdaλ and that the hidden state at time ttt equals jjj what is the probability that we observe the sequence yt+1,…,yTy_{t+1}, dots, y_Tyt+1​,…,yT​. Similar to the forward probabilities we can determine the backward probabilities using dynamic programming as: The R implementation is given below: . back_prob_alg = function(Y, V, pi, B, Q) { # Define empty forward matrix back_prob = matrix(data=0, nrow=length(V), ncol=length(Y)) T_max = length(Y) # Fill first elements back_prob[, T_max] = rep(1, length(V)) # Now for all other time steps for (t in (T_max-1):1) { back_prob[,t] = Q %*% (back_prob[, t+1] * B[Y[t+1], ]) } return(back_prob) } beta = back_prob_alg(Y, V_num, pi, B, Q) probY = sum(pi * B[Y[1], ] * beta[, 1]) print(probY) ## [1] 0.0099748 . Recall: we are interesting in estimating QQQ, BBB and π piπ given our observation sequence YYY. As we will see later on, estimating these quantities typically involve estimating the frequency of being in a certain state and/or counting the expected number of transitions from one state to another. More precisely, we estimate the transition probability from state iii to jjj as . q^i,j=expected number of transitions from state i to jexpected number of transitions from state i. hat{q}_{i,j} = frac{ textrm{expected number of transitions from state } i textrm{ to } j}{ textrm{expected number of transitions from state } i}.q^​i,j​=expected number of transitions from state iexpected number of transitions from state i to j​. . To estimate bi(yt)b_i(y_t)bi​(yt​), i.e., the probability of observing yty_tyt​ in state Xt=iX_t = iXt​=i, we compute: . b^i(yt)=expected number of times in state i while observing ytexpected number of times in state i. hat{b}_i(y_t) = frac{ textrm{expected number of times in state } i textrm{ while observing } y_t}{ textrm{expected number of times in state } i}.b^i​(yt​)=expected number of times in state iexpected number of times in state i while observing yt​​. . The initial state probabilities π piπ will follow directly from quantities computed in the EW algorithm. . We now define . γt(j)=P(Xt=j∣Y,λ)=P(Y,Xt=j∣λ)P(Y∣λ) gamma_t(j) = P(X_t = j mid Y, lambda) = frac{P(Y, X_t = j mid lambda)}{P(Y mid lambda)}γt​(j)=P(Xt​=j∣Y,λ)=P(Y∣λ)P(Y,Xt​=j∣λ)​ . which is the probability of being in state jjj at time ttt for a state sequence YYY. By the Markovian conditional independence . αt(j)βt(j)=P(y1,…,yt,Xt=j∣λ)P(yt+1,…,yT∣Xt=j,λ)=P(Y,Xt=j∣λ). alpha_t(j) beta_t(j) = P(y_1, dots, y_t, X_t = j mid lambda) P(y_{t+1}, dots, y_T mid X_t = j, lambda) = P(Y, X_t = j mid lambda).αt​(j)βt​(j)=P(y1​,…,yt​,Xt​=j∣λ)P(yt+1​,…,yT​∣Xt​=j,λ)=P(Y,Xt​=j∣λ). . Hence, we can write γt(j) gamma_t(j)γt​(j) in terms of αt(j) alpha_t(j)αt​(j) and βt(j) beta_t(j)βt​(j) as . γt(j)=αt(j)βt(j)P(Y∣λ). gamma_t(j) = frac{ alpha_t(j) beta_t(j)}{P(Y mid lambda)}.γt​(j)=P(Y∣λ)αt​(j)βt​(j)​. . Recall that P(Y∣λ)P(Y mid lambda)P(Y∣λ) can be easily obtained when computing the forward (or backward) probabilities. . ComputeGamma &lt;- function(alpha, beta, probY) { # Obtain gamma for all t and v gamma &lt;- (alpha * beta) / probY } gamma = ComputeGamma(alpha, beta, probY) . We can now estimate the elements of BBB as . b^j(k)=∑t=1Tδyt,vkγt(j)∑t=1Tγt(j), hat{b}_j(k) = frac{ sum_{t=1}^T delta_{y_t, v_k} gamma_t(j)}{ sum_{t=1}^T gamma_t(j)},b^j​(k)=∑t=1T​γt​(j)∑t=1T​δyt​,vk​​γt​(j)​, . where δi,j delta_{i, j}δi,j​ evaluates to 1 if i=ji = ji=j and 0 otherwise. . Delta &lt;- function(A, b) { return(as.integer(A == b)) } Delta = Vectorize(Delta, &quot;b&quot;) ComputeBHat &lt;- function(gamma, Y, D) { # Obtain the delta variable delta &lt;- t(Delta(Y, D)) # Compute nominator and denominator deltaGamma &lt;- delta %*% t(gamma) deltaDenom &lt;- matrix(data=1, nrow=nrow(delta), ncol=ncol(delta)) %*% t(gamma) # Divide element-wise BHat &lt;- deltaGamma / deltaDenom return(BHat) } BHat = ComputeBHat(gamma, Y, D) . To estimate the elements of QQQ we define . ψt(i,j)=P(Xt=i,Xt+1=j∣Y,λ)=P(Xt=i,Xt+1=j,Y∣λ)P(Y∣λ) psi_t(i,j) = P(X_t = i, X_{t+1} = j mid Y, lambda) = frac{P(X_t = i, X_{t+1} = j, Y mid lambda)}{P(Y mid lambda)}ψt​(i,j)=P(Xt​=i,Xt+1​=j∣Y,λ)=P(Y∣λ)P(Xt​=i,Xt+1​=j,Y∣λ)​ . which is the probability of being in state iii at time ttt and being in state jjj at time t+1t+1t+1 for a state sequence YYY. This can be parameterised in terms of αt(j) alpha_t(j)αt​(j) and βt(j) beta_t(j)βt​(j) as . ψt(i,j)=αt(i)qi,jbt+1(j)βt+1(j)P(Y∣λ). psi_t(i,j) = frac{ alpha_t(i)q_{i,j}b_{t+1}(j) beta_{t+1}(j)}{P(Y mid lambda)}.ψt​(i,j)=P(Y∣λ)αt​(i)qi,j​bt+1​(j)βt+1​(j)​. . ComputePsi &lt;- function(alpha, beta, B, Q, Y, probY) { # Create empty matrix T_max = ncol(beta) psi = array(-1, dim=c(T_max-1, nrow(Q), nrow(Q))) for (t in 1:(T_max-1)) { psi[t,, ] = cbind(alpha[, t], alpha[, t]) * Q * rbind(B[Y[t+1], ], B[Y[t+1], ]) * rbind(beta[, t+1], beta[, t+1]) } return(psi / probY) } psi = ComputePsi(alpha, beta, B, Q, Y, probY) . The transition probabilities can now be estimated as . qˉi,j=∑t=1T−1ψt(i,j)∑t=1T−1γt(i) bar{q}_{i,j} = frac{ sum_{t = 1}^{T-1} psi_t(i,j)}{ sum_{t=1}^{T-1} gamma_t(i)}qˉ​i,j​=∑t=1T−1​γt​(i)∑t=1T−1​ψt​(i,j)​ . since the expected number of times in a state jjj is equal to the expected number of transitions from state jjj (for an ergodic Markov process). . ComputeQ = function(psi, gamma) { QNom = apply(psi, c(2,3), sum) QDenom = t(rep(1, 2)) %x% apply(gamma[, 1:(ncol(gamma)-1)], 1, sum) Q = QNom / QDenom } Q_new = ComputeQ(psi, gamma) . The quantity . πˉj=γ1(j) bar{ pi}_j = gamma_1(j)πˉj​=γ1​(j) . is an estimate for the initial state probability for state jjj. . The idea of the BW algorithm is to iteratively update γt(⋅) gamma_t( cdot)γt​(⋅) and psit(⋅)psi_t( cdot)psit​(⋅) in one step (E-step) and the estimates for QQQ and BBB in the other step (M-step). The exact BW algorithm is provided in Alg. 3. . The R implementation of the Balm-Welsch algorithm is given below: . balm_welch_alg = function(Y, Q, B, pi, V, D) { converged = FALSE max_iter = 1000 iter = 1 while(!converged &amp; iter!= max_iter) { # First compute alpa and beta alpha = forward_alg(Y, V, pi, B, Q) beta = back_prob_alg(Y, V, pi, B, Q) probY = sum(alpha[,ncol(alpha)]) # E-Step gamma = ComputeGamma(alpha, beta, probY) psi = ComputePsi(alpha, beta, B, Q, Y, probY) # M-Step Q = ComputeQ(psi, gamma) B = ComputeBHat(gamma, Y, D) pi = gamma[, 1] # Increase iteration iter = iter + 1 } } . We have now addressed all main problems concerning HMMs. Note, however that our implementation in R is rather simplistic and probably has several numerical issues. Furthermore, we restricted our observed sequence to consist of (finite) discrete options. A common extension is to allow Y∣XY mid XY∣X to have some continous distribution which depends on XXX and is parameterised by some θ thetaθ. Check Gaussian Mixture Hidden Markov Models as an example. . Numerical Issues . The procedures described in this article cannot be used for long observation sequences ( &gt;100&gt;100&gt;100) due to underflow problems. Recall that . P(Y=Yt,X=XT∣λ)=∏t=1T−1qxt,xt+1∏t=1Tbxt(yt)P(Y= Y_t, X = X_T mid lambda) = prod_{t=1}^{T-1}q_{x_t, x_{t+1}} prod_{t=1}^T b_{x_t}(y_t)P(Y=Yt​,X=XT​∣λ)=t=1∏T−1​qxt​,xt+1​​t=1∏T​bxt​​(yt​) . which consists of a product of numbers between 0 and 1. For sufficiently large TTT this quantity is equal to zero for computers. Hence, we need to scale the values of QQQ and BBB such that we do not have this numerical issue. . The standard approach is to standardise αt(j) alpha_t(j)αt​(j) and βt(j) beta_t(j)βt​(j) by normalising the values for each j∈[N]j in [N]j∈[N] by multiplying these values with . ct=1∑i=1Nαt(i)c_t = frac{1}{ sum_{i=1}^N alpha_t(i)}ct​=∑i=1N​αt​(i)1​ . to obtain . α^t(j)=αt(j)ct hat{ alpha}_t(j) = alpha_t(j) c_tα^t​(j)=αt​(j)ct​ . and . β^t(j)=βt(j)ct. hat{ beta}_t(j) = beta_t(j) c_t.β^​t​(j)=βt​(j)ct​. . ForwardProbAlgScaled = function(Y, V, pi, B, Q) { # Define empty forward matrix forward &lt;- matrix(data=0, nrow=length(V), ncol=length(Y)) c_scale &lt;- 1 / rep(1, ncol=length(Y)) # Fill first elements forward[,1] &lt;- B[Y[1],] * pi c_scale[1] &lt;- sum(forward[,1]) forward[,1] &lt;- forward[,1] * c_scale[1] # Now for all other time steps for (t in 2:length(Y)) { tmp &lt;- forward[,t-1] %*% Q * B[Y[t],] c_scale[t] &lt;- 1 / sum(tmp) forward[,t] &lt;- tmp * c_scale[t] } return(list(forward, c_scale)) } BackProbAlgScaled = function(Y, V, pi, B, Q, c_scale) { # Define empty forward matrix back_prob &lt;- matrix(data=0, nrow=length(V), ncol=length(Y)) T_max &lt;- length(Y) # Fill first elements back_prob[, T_max] &lt;- rep(1, length(V)) * c_scale[T_max] # Now for all other time steps for (t in (T_max-1):1) { back_prob[,t] &lt;- Q %*% (back_prob[, t+1] * B[Y[t+1], ]) * c_scale[t] } return(back_prob) } ComputeGammaScaled &lt;- function(alpha, beta) { # Obtain gamma for all t and v alpha_beta &lt;- alpha * beta gamma &lt;- t(t(alpha_beta) / apply(alpha_beta, 2, sum)) } ComputePsiScaled &lt;- function(alpha, beta, B, Q, Y) { # Create empty matrix T_max &lt;- ncol(beta) psi &lt;- array(-1, dim=c(T_max-1, nrow(Q), nrow(Q))) for (t in 1:(T_max-1)) { psi[t,, ] &lt;- cbind(alpha[, t], alpha[, t]) * Q * rbind(B[Y[t+1], ], B[Y[t+1], ]) * rbind(beta[, t+1], beta[, t+1]) psi[t,, ] &lt;- psi[t,, ] / sum(psi[t,, ]) } return(psi) } . The adapted Balm-Welch algorithm is then given by . BalmWelchAlg = function(Y, Q, B, pi, V, D) { converged = FALSE max_iter = 1000 iter = 1 prob_old = log(1e-12) while(!converged &amp; iter!= max_iter) { # First compute alpa and beta forward_list &lt;- ForwardProbAlgScaled(Y, V, pi, B, Q) c_scale &lt;- forward_list[[2]] alpha &lt;- forward_list[[1]] beta &lt;- BackProbAlgScaled(Y, V, pi, B, Q, c_scale) prob_new &lt;- log(1 / prod(c_scale)) # E-Step gamma &lt;- ComputeGammaScaled(alpha, beta) psi &lt;- ComputePsiScaled(alpha, beta, B, Q, Y) # M-Step Q &lt;- ComputeQ(psi, gamma) B &lt;- ComputeBHat(gamma, Y, D) pi &lt;- gamma[, 1] # Update stopping rules if (prob_new - prob_old &lt; 1e-4) { converged &lt;- TRUE } prob_old = prob_new iter &lt;- iter + 1 } return(list(Q = Q, B = B, pi = pi)) } model &lt;- BalmWelchAlg(Y, Q, B, pi, V, D) model[[&quot;Q&quot;]] ## [,1] [,2] ## [1,]1 4.738597e-19 ## [2,]1 4.788997e-18 model[[&quot;B&quot;]] ## [,1] [,2] ## [1,] 1.974110e-43 1.000000e+00 ## [2,] 3.333333e-01 9.278520e-20 ## [3,] 6.666667e-01 5.643931e-18 model[[&quot;pi&quot;]] ## [1] 5.922329e-43 1.000000e+00 .",
            "url": "https://mvanderbroek.com/r/machine%20learning/2019/05/08/Hidden-Markov-Model-Tutorial-in-R.html",
            "relUrl": "/r/machine%20learning/2019/05/08/Hidden-Markov-Model-Tutorial-in-R.html",
            "date": " • May 8, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "Predicting Sale Prices Of Houses In Ames",
            "content": "In this tutorial I will discuss how you can go from a raw dataset to a predictive model. For this tutorial we will make use of the Ames Dataset and see whether we can predict house prices based on characteristics provided in the dataset. . The analysis in this tutorial is done in Python using the pandas, scikit-learn and matplotlib packages. We will start by exploring the raw data and see whether we can already see some patterns in the data or that some features should be discarded right away. Next, we will make a straight-forward pipeline that will transform our dataset and fit a linear regression model. Finally, we will evaluate the performance of this model. . I do have limited knowledge with data analysis (I mainly use R), so this tutorial will be most informative for people like me: beginners! . Raw Data &amp; Initial Analysis . First, we need to download the dataset provided in the link above (direct download link here). . import pandas as pd import numpy as np import matplotlib.pyplot as plt from datetime import date from sklearn.impute import SimpleImputer from sklearn.preprocessing import LabelBinarizer, StandardScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import warnings # Do not display warnings warnings.filterwarnings(&#39;ignore&#39;) # Import the Ames Housing data (put this in the same folder as your Python file) house_data = pd.read_csv(&quot;AmesHousing.txt&quot;, sep=&quot; t&quot;) # I do not like spaces in the column names of a dataset so I first replace spaces # by underscores house_data.columns = house_data.columns.str.replace(&#39; &#39;, &#39;_&#39;) . Next we want to get a feeling for what exactly we can find in the dataset, so let’s look at some relevant information. . # Get some feeling for the data print(house_data.head(10)) house_info = house_data.describe() house_data.info() . Order PID MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street 0 1 526301100 20 RL 141.0 31770 Pave 1 2 526350040 20 RH 80.0 11622 Pave 2 3 526351010 20 RL 81.0 14267 Pave 3 4 526353030 20 RL 93.0 11160 Pave 4 5 527105010 60 RL 74.0 13830 Pave 5 6 527105030 60 RL 78.0 9978 Pave 6 7 527127150 120 RL 41.0 4920 Pave 7 8 527145080 120 RL 43.0 5005 Pave 8 9 527146030 120 RL 39.0 5389 Pave 9 10 527162130 60 RL 60.0 7500 Pave Alley Lot_Shape Land_Contour ... Pool_Area Pool_QC Fence 0 NaN IR1 Lvl ... 0 NaN NaN 1 NaN Reg Lvl ... 0 NaN MnPrv 2 NaN IR1 Lvl ... 0 NaN NaN 3 NaN Reg Lvl ... 0 NaN NaN 4 NaN IR1 Lvl ... 0 NaN MnPrv 5 NaN IR1 Lvl ... 0 NaN NaN 6 NaN Reg Lvl ... 0 NaN NaN 7 NaN IR1 HLS ... 0 NaN NaN 8 NaN IR1 Lvl ... 0 NaN NaN 9 NaN Reg Lvl ... 0 NaN NaN Misc_Feature Misc_Val Mo_Sold Yr_Sold Sale_Type Sale_Condition SalePrice 0 NaN 0 5 2010 WD Normal 215000 1 NaN 0 6 2010 WD Normal 105000 2 Gar2 12500 6 2010 WD Normal 172000 3 NaN 0 4 2010 WD Normal 244000 4 NaN 0 3 2010 WD Normal 189900 5 NaN 0 6 2010 WD Normal 195500 6 NaN 0 4 2010 WD Normal 213500 7 NaN 0 1 2010 WD Normal 191500 8 NaN 0 3 2010 WD Normal 236500 9 NaN 0 6 2010 WD Normal 189000 [10 rows x 82 columns] &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2930 entries, 0 to 2929 Data columns (total 82 columns): Order 2930 non-null int64 PID 2930 non-null int64 MS_SubClass 2930 non-null int64 MS_Zoning 2930 non-null object Lot_Frontage 2440 non-null float64 Lot_Area 2930 non-null int64 Street 2930 non-null object Alley 198 non-null object Lot_Shape 2930 non-null object Land_Contour 2930 non-null object Utilities 2930 non-null object Lot_Config 2930 non-null object Land_Slope 2930 non-null object Neighborhood 2930 non-null object Condition_1 2930 non-null object Condition_2 2930 non-null object Bldg_Type 2930 non-null object House_Style 2930 non-null object Overall_Qual 2930 non-null int64 Overall_Cond 2930 non-null int64 Year_Built 2930 non-null int64 Year_Remod/Add 2930 non-null int64 Roof_Style 2930 non-null object Roof_Matl 2930 non-null object Exterior_1st 2930 non-null object Exterior_2nd 2930 non-null object Mas_Vnr_Type 2907 non-null object Mas_Vnr_Area 2907 non-null float64 Exter_Qual 2930 non-null object Exter_Cond 2930 non-null object Foundation 2930 non-null object Bsmt_Qual 2850 non-null object Bsmt_Cond 2850 non-null object Bsmt_Exposure 2847 non-null object BsmtFin_Type_1 2850 non-null object BsmtFin_SF_1 2929 non-null float64 BsmtFin_Type_2 2849 non-null object BsmtFin_SF_2 2929 non-null float64 Bsmt_Unf_SF 2929 non-null float64 Total_Bsmt_SF 2929 non-null float64 Heating 2930 non-null object Heating_QC 2930 non-null object Central_Air 2930 non-null object Electrical 2929 non-null object 1st_Flr_SF 2930 non-null int64 2nd_Flr_SF 2930 non-null int64 Low_Qual_Fin_SF 2930 non-null int64 Gr_Liv_Area 2930 non-null int64 Bsmt_Full_Bath 2928 non-null float64 Bsmt_Half_Bath 2928 non-null float64 Full_Bath 2930 non-null int64 Half_Bath 2930 non-null int64 Bedroom_AbvGr 2930 non-null int64 Kitchen_AbvGr 2930 non-null int64 Kitchen_Qual 2930 non-null object TotRms_AbvGrd 2930 non-null int64 Functional 2930 non-null object Fireplaces 2930 non-null int64 Fireplace_Qu 1508 non-null object Garage_Type 2773 non-null object Garage_Yr_Blt 2771 non-null float64 Garage_Finish 2771 non-null object Garage_Cars 2929 non-null float64 Garage_Area 2929 non-null float64 Garage_Qual 2771 non-null object Garage_Cond 2771 non-null object Paved_Drive 2930 non-null object Wood_Deck_SF 2930 non-null int64 Open_Porch_SF 2930 non-null int64 Enclosed_Porch 2930 non-null int64 3Ssn_Porch 2930 non-null int64 Screen_Porch 2930 non-null int64 Pool_Area 2930 non-null int64 Pool_QC 13 non-null object Fence 572 non-null object Misc_Feature 106 non-null object Misc_Val 2930 non-null int64 Mo_Sold 2930 non-null int64 Yr_Sold 2930 non-null int64 Sale_Type 2930 non-null object Sale_Condition 2930 non-null object SalePrice 2930 non-null int64 dtypes: float64(11), int64(28), object(43) memory usage: 1.8+ MB . Since we want to build a model that predicts the house price given a set of features of this given house, let’s first check these prices by creating a histogram. . plt.hist(house_data.SalePrice, bins=50) plt.xlabel(&quot;House price (in $)&quot;) plt.ylabel(&quot;Frequency&quot;) plt.show() # Pandas also provides a built-in plotting env # house_data.SalePrice.plot.hist() . . The Year_Built feature would be a candidate to give insights in the sale price of a house. Below we report the density of the year that each house in the dataset is built. . house_data[&#39;Year_Built&#39;].plot.kde() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1ab908d0&gt; . . We can check the relationship between the sale price and the year houses are built by creating a scatter plot. . house_data.plot.scatter(x=&#39;Year_Built&#39;, y=&#39;SalePrice&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a19375160&gt; . . This plot suggest that houses that are built more recently tend to have a higher sale price (on average). Note that this probably due to the geographical area we are considering. In old cities, like Amsterdam, we can imagine that (part) of the old houses are momumental buildings and therefore have higher sale price. . In the analysis above, we have looked into the relationship of the sale price and a numerical feature. A scatter plot is usually a good option to check if there might be an interesting relationship, however for categorical features we need different techniques. . We have information on the type of sale for each house, e.g., some houses are sold with adjecent land, or a family member bought the house. A tool to look at the relationship between this (categorical) feature and the sale price is to look at the histogram for each type of sale condition. . house_data[&#39;SalePrice&#39;].hist(by=house_data[&#39;Sale_Condition&#39;], bins=30) . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1ad44eb8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1acd0f98&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1af29208&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1a944470&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1ac636d8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1a1ae64940&gt;]], dtype=object) . . Another feature that is likely to have an influence on the sale price is in what neighbourhood the house is located. We could again create histograms of the sale price for each neighbourhood, but since there are more than 20 neighbourhoods, we can do something else. We take the mean of the sale price for each neighbourhood and present this in a bar plot. . # Check saleprice per neighbourhood avg_price_neigh = house_data.groupby(&#39;Neighborhood&#39;).agg({&#39;SalePrice&#39; : &#39;mean&#39;}).reset_index() avg_price_neigh.plot.bar(x=&#39;Neighborhood&#39;, y=&#39;SalePrice&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1b026dd8&gt; . . Feature Selection . The dataset provides more than 80 features, and not all features are equally informative for sale price. Selecting good features and transforming existing features is often a rather ad-hoc procedure, since every dataset is unique. However, there are a few standard procedures that usually help in creating a significantly better dataset. . Trimming . One of these techniques is to delete outliers from your dataset. Whether or not this is necesarry also depends on the type of model that is used, e.g., linear models are usually very sensitive to outliers. . Let’s look at a scatter plot of the living area and sale price. . # Check living space and sale price (there seems to be 5 outliers...) house_data.plot.scatter(x=&#39;Gr_Liv_Area&#39;, y=&#39;SalePrice&#39;) house_data = house_data.query(&#39;Gr_Liv_Area &lt; 4000&#39;) . . There seem to be 4-5 outliers with very high living areas. For this analysis we will remove these, however if the goal of your model is to also have good predictions for these outliers it might be worthwhile to keep them. . Feature enhancement . There are many ways to enhance existing features. Here, I will show how categorical variables can be improved. To do this, I select all features that are of the type object and save the histogram for these features in a folder figures. . # Check categorical variables and see if we need to delete them house_data_cat = house_data.select_dtypes(include=&#39;object&#39;) for col in house_data_cat.columns: house_data_cat[col].value_counts().plot.bar(title=col) plt.savefig(&#39;figures/&#39; + col + &#39;_hist.pdf&#39;) plt.clf() house_data_cat[&#39;Bsmt_Qual&#39;].value_counts().plot.bar(title=&#39;Basement Quality&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1d629be0&gt; . . There are a considerable number of categorical features that are ordinal, that is, the categories have a certain ordering. In this case, we have information about whether the feature has a value Poor, Fair, Average, Good or Excellent. However, especially the most extreme outcomes are rather unlikely. As we will later see this results in a large number of features (when we one hot encode them) with only minimal extra predictive power. Therefore we combine certain groups. . ord_groups = {&#39;Fa&#39;: &#39;Bad&#39;, &#39;Po&#39;: &#39;Bad&#39;, &#39;TA&#39;: &#39;Average&#39;, &#39;Gd&#39;: &#39;Good&#39;, &#39;Ex&#39;:&#39;Good&#39;} columns_ord = [&#39;Bsmt_Cond&#39;, &#39;Bsmt_Qual&#39;, &#39;Exter_Cond&#39;, &#39;Exter_Qual&#39;, &#39;Fireplace_Qu&#39;, &#39;Garage_Cond&#39;, &#39;Garage_Qual&#39;, &#39;Heating_QC&#39;, &#39;Kitchen_Qual&#39;, &#39;Pool_QC&#39;] for col in columns_ord: house_data[col].replace(ord_groups, inplace=True) house_data[col] = house_data[col].astype(&#39;category&#39;, categories=[&#39;Bad&#39;, &#39;Average&#39;, &#39;Good&#39;], ordered=True) . Missing Values . Most real-life datasets have missing values for a subset of its features. There are different ways to deal with these missing values. Usually, when a feature’s value is missing in most of the samples, it is better to just discard them. Let’s see if we have any of these variables. . missing_vals = house_data.isnull().sum(axis = 0) print(missing_vals) . Order 0 PID 0 MS_SubClass 0 MS_Zoning 0 Lot_Frontage 490 Lot_Area 0 Street 0 Alley 2727 Lot_Shape 0 Land_Contour 0 Utilities 0 Lot_Config 0 Land_Slope 0 Neighborhood 0 Condition_1 0 Condition_2 0 Bldg_Type 0 House_Style 0 Overall_Qual 0 Overall_Cond 0 Year_Built 0 Year_Remod/Add 0 Roof_Style 0 Roof_Matl 0 Exterior_1st 0 Exterior_2nd 0 Mas_Vnr_Type 23 Mas_Vnr_Area 23 Exter_Qual 0 Exter_Cond 0 ... Bedroom_AbvGr 0 Kitchen_AbvGr 0 Kitchen_Qual 0 TotRms_AbvGrd 0 Functional 0 Fireplaces 0 Fireplace_Qu 1422 Garage_Type 157 Garage_Yr_Blt 159 Garage_Finish 159 Garage_Cars 1 Garage_Area 1 Garage_Qual 159 Garage_Cond 159 Paved_Drive 0 Wood_Deck_SF 0 Open_Porch_SF 0 Enclosed_Porch 0 3Ssn_Porch 0 Screen_Porch 0 Pool_Area 0 Pool_QC 2914 Fence 2354 Misc_Feature 2820 Misc_Val 0 Mo_Sold 0 Yr_Sold 0 Sale_Type 0 Sale_Condition 0 SalePrice 0 Length: 82, dtype: int64 . Let’s get rid of features that have many missing values. . house_data = house_data.drop(columns=[&#39;Alley&#39;, &#39;Fireplace_Qu&#39;, &#39;Pool_QC&#39;, &#39;Misc_Feature&#39;, &#39;Misc_Val&#39;]) . For some features a missing value is not really missing, since it can indicate that the value of this feature is zero when it’s missing. Fence and Pool_Area belong to this group. For these features we decide to create a binary variable indicating whether or not the house has this feature (and we do not care about the type or size of feature). . # Make some variables useful house_data[&#39;Fence&#39;] = house_data[&#39;Fence&#39;].notna() house_data[&#39;Pool&#39;] = house_data[&#39;Pool_Area&#39;] &gt; 0 . Similar to the categorical features that are rated from poor until excellent, there are also features with different categories. Again, it might be worthwhile to combine categories in the dataset that only occur a very infrequently. In this case we combine categories that consistute less than 1 percent of the total samples. . house_data_obj = house_data.select_dtypes(include=&#39;object&#39;) for col in house_data_obj.columns: series = house_data[col].value_counts() mask = (series/series.sum() * 100).lt(1) house_data[col] = np.where(house_data[col].isin(series[mask].index),&#39;Other&#39;, house_data[col]) house_data[col] = house_data[col].astype(&#39;category&#39;) . Note that the 1 percent of the total sample is rather ad-hoc. Imagine that you have a dataset consisting of millions of samples, then there is no need to get rid of these categories, since we still have sufficient information (unless you need balanced categories). . To see what effect our transformations and enhanced have had on our features, we can have a look at the basement quality feature again. . house_data[&#39;Bsmt_Qual&#39;].value_counts().plot.bar(title=&#39;Basement Quality&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x1a1db4d668&gt; . . Data pipeline . Now that we have cleaned and transformed our dataset we can look at if it is possible to create a decent model that can predict sale prices of houses the model has not seen before. However, we first have to deal with a few steps before we can use our dataset in these models. . We need to make sure that we do not evaluate on data that we used to train and estimate our model. So we split our dataset in a training and test part. . X = house_data.drop(columns=[&#39;SalePrice&#39;]) y = house_data[&#39;SalePrice&#39;] # We need to split the set into a training and test set. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) . Imputer . Even though we got rid of the features with many missing values, there are still features that have a few missing values. It is possible to use advanced techiques to “imputate” these missing values, however we will a simple imputing technique, which replaces the missing values with the most frequent value for that feature. . imputer = SimpleImputer(strategy=&#39;most_frequent&#39;) . Standardisation &amp; Encoding . Most machine learning models require that the scale of the features in the dataset are similar. Therefore, we will standardise all numerical features in the dataset by removing the mean of that feature and dividing by the standard deviation, i.e., xscaled=x−xˉstd(x)x_{scaled} = frac{x - bar{x}}{ text{std}(x)}xscaled​=std(x)x−xˉ​ . We also have categorical features which we cannot directly feed to the machine learning models. All features need to be represented by a numerical value. Both nominal and ordinal features can be transformed by a technique called One-hot encoding, where a categorical feature is replaced by a number of dummies that indicate one of these categories. . # Next we need to scale the numerical columns and encode the categorical # variables. This can be done by splitting the dataset into two parts. cat_cols = X_train.select_dtypes(include=&#39;category&#39;).columns num_cols = X_train.select_dtypes(include=&#39;number&#39;).columns cat_cols_mask = X_train.columns.isin(cat_cols) num_cols_mask = X_train.columns.isin(num_cols) ct = ColumnTransformer( [(&#39;scale&#39;, StandardScaler(), num_cols_mask), (&#39;encode&#39;, OneHotEncoder(), cat_cols_mask)]) . Note that ordinal values can also be transformed into a single numerical value, where each category is represented by a number that is ordered according to the ordering of these categories. The advantage of this procedure is that we require less features to represent this feature numerically, however for most machine learning models it also results in a linear relationship between the that feature and the feature we want to predict. . Machine learning model . Now that we have a dataset that only consists of numerical values we can apply any machine learning we want. scikit-learn provides many machine learning models with a common interface (except for the hyperparameters), so it is easy to implement many different models. In our case I will only use a linear regression. . linear_reg = LinearRegression() . Combine all steps of pipeline . Pipelines can be used to automatically perform all steps of the model estimation (including preprocessing steps). It also makes sure that the preprocessing is done correctly, e.g., the scaling for the test dataset is the one used for the training dataset (Reader: why is this absolutely necessary?). An overview of what the pipeline does is given in the figure below (copyright by Sebastian Raschka). . . pipe = make_pipeline(imputer, ct, linear_reg) . Training &amp; Evaluating model . Training the model is as simple as . pipe = pipe.fit(X_train, y_train) . Usually, the best model (within or between) class(es) of models can be determined by using cross-validation or splitting the training dataset into a training and validation part (if you have enough data). I will skip this step and go immediately to the evaluation of our model. . In the step above we estimated and trained our model based on the training set. To see how well it performs we can look at what sale prices our model predict for data it has not seen before. . y_pred = pipe.predict(X_test) errors = y_pred - y_test . scikit-learn can provide a score for the model, but we will look at the error of the prediction instead. First let’s plot the histogram of the errors . plt.hist(errors, bins=30) plt.show() . . The histogram suggest that the errors are normally distributed (assumption of the linear regression). Furthermore, it seems that most predictions have an error less than 20,000$. To me this seems like a reasonable model. . Conclusion . In this tutorial we have seen how we can go from raw data to a predictive model that has a reasonable performance. However, there are many things we have not discussed yet. To name a few: . Consider multiple models and select the best one using cross-validation. | Use a more advanced missing value imputation technique, e.g., MICE. | .",
            "url": "https://mvanderbroek.com/machine%20learning/python/2018/10/07/Predicting-Sale-Prices-of-Houses-in-Ames.html",
            "relUrl": "/machine%20learning/python/2018/10/07/Predicting-Sale-Prices-of-Houses-in-Ames.html",
            "date": " • Oct 7, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Train Xor Logic Gate In Neural Network",
            "content": "Deep learning (DL) is a thriving research field with an increasing number of practical applications. One of the models used in DL are so called artificial neural networks (ANN). In this tutorial I will not discuss exactly how these ANNs work, but instead I will show how flexible these models can be by training an ANN that will act as a XOR logic gate. . XOR gate . For those of you unfamiliar with logical gates, a logical gate takes two binary values as inputs and produces a single binary output. For the XOR gate it will output a 1 one value if only one of the input values is 1, and 0 otherwise, i.e., graphically: . Input 1 Input 2 Output . 0 | 0 | 0 | . 1 | 0 | 1 | . 0 | 1 | 1 | . 1 | 1 | 0 | . XOR gate as ANN . GoodFellow et al. show that this XOR gate can be learned by an ANN with one hidden layer consisting of two neurons. We have two input neurons, one hidden layer and an output layer with a single neuron. This network can be graphically represented as: . . When I started learning about Deep Learning and these ANN in particular I started wondering whether I could train the small ANN to learn to act like an XOR gate. Since I am still relatively new to these networks I thought it would be a good exercise to program the backpropagation algorithm that trains these models myself. . The network in Python . I decided to model this network in Python, since it is the most popular language for Deep Learning because of the active development of packages like numpy, tensorflow, keras, etc. As I will show below it is very easy to implement the model as described above and train it using a package like keras. However, since I wanted to get a better understanding of the backpropagation algorithm I decided to first implement this algorithm. . Before we start doing that let us first define the four possible combinations of inputs and corresponding outputs, i.e., . X=[00100111],y=[0110] boldsymbol{X} = begin{bmatrix} 0 &amp; 0 1 &amp; 0 0 &amp; 1 1 &amp; 1 end{bmatrix}, quad boldsymbol{y} = begin{bmatrix} 0 1 1 0 end{bmatrix}X=⎣⎢⎢⎢⎡​0101​0011​⎦⎥⎥⎥⎤​,y=⎣⎢⎢⎢⎡​0110​⎦⎥⎥⎥⎤​ . In python we get . import numpy as np # Possible outputs X = np.matrix(&#39;0 0; 1 0; 0 1; 1 1&#39;) y = np.array([0, 1, 1, 0]) . Own implementation backpropagation algorithm . From this moment onwards I assume that you have a basic understanding of how an ANN works, and understand the basic math behind it. First, I will define some functions that are needed to implement the backpropagation algorithm to solve this problem. . def relu(z): &quot;&quot;&quot;ReLU activation function&quot;&quot;&quot; return np.maximum(z, 0, z) def relu_prime(z): &quot;&quot;&quot;First derivative of the ReLU activation function&quot;&quot;&quot; return 1*(z&gt;0) def sigmoid(z): &quot;&quot;&quot;Sigmoid activation function&quot;&quot;&quot; return 1 / (1 + np.exp(-z)) def sigmoid_prime(z): &quot;&quot;&quot;First derivative of sigmoid activation function&quot;&quot;&quot; return np.multiply(sigmoid(z), 1-sigmoid(z)) def cost(a, y): &quot;&quot;&quot;Calculate MSE&quot;&quot;&quot; return ((a - y) ** 2).mean() def cost_grad(a, y): &quot;&quot;&quot;First derivate of MSE function&quot;&quot;&quot; return a - y def weighted_sum(W, a, b): &quot;&quot;&quot;Compute the weighted average z for all neurons in new layer&quot;&quot;&quot; return W.dot(a) + b def forward_prop(x, W, b): &quot;&quot;&quot;Calculate z and a for every neuron using current weights and biases&quot;&quot;&quot; a = [None] * len(layer_sizes) z = [None] * len(layer_sizes) a[0] = x.T for l in range(1, len(a)): z[l] = weighted_sum(W[l], a[l-1], b[l]) a[l] = sigmoid(z[l]) return (a, z) def back_prop(a, z, W, y): &quot;&quot;&quot;Calculate error delta for every neuron&quot;&quot;&quot; delta = [None] * len(layer_sizes) end_node = len(a)-1 delta[end_node] = np.multiply(cost_grad(a[end_node], y), sigmoid_prime(z[end_node])) for l in reversed(range(1, end_node)): delta[l] = np.multiply(W[l+1].T.dot(delta[l+1]), sigmoid_prime(z[l])) return delta def calc_gradient(W, b, a, delta, eta): &quot;&quot;&quot;Update W and b using gradient descent steps based&quot;&quot;&quot; W_grad = [None] * len(W) b_grad = [None] * len(b) for l in range(1, len(W)): W_grad[l] = a[l-1].dot(delta[l].T) b_grad[l] = delta[l] return (W_grad, b_grad) def backpropagation_iter(X, y, W, b, eta): &quot;&quot;&quot;One iteration of the backpropagation algorithm, i.e., forward- and backward propagate and compute gradient&quot;&quot;&quot; y_pred = [None] * len(y) for i in range(n): # First we propagate forward through the network to obtain activation levels and z. a, z = forward_prop(X[i, :], W, b) y_pred[i] = np.max(a[-1]) # Back propagate to obtain delta&#39;s. delta = back_prop(a, z, W, y[i]) # This allows us to compute the gradient for this instance. Add this to all. W_grad, b_grad = calc_gradient(W, b, a, delta, eta) if i == 0: W_grad_sum = W_grad b_grad_sum = b_grad else: for l in range(1, len(W_grad)): W_grad_sum[l] += W_grad[l] b_grad_sum[l] += b_grad[l] # Update weights and bias for l in range(1, len(W)): W[l] = W[l] - (eta/n) * W_grad_sum[l] b[l] = b[l] - (eta/n) * b_grad_sum[l] # Show MSE MSE = cost(y_pred, y) return (W, b, y_pred, MSE) . We also need to initialise the weights and bias of every link and neuron. It is important to do this randomly. We also set the number of iterations and the learning rate for the gradient descent method. . # Initialise layer sizes of all layers in the neural network layer_sizes = [X.shape[1], 2, 1] # Initialise weights and activation and weight vectors as None. W = [None] * len(layer_sizes) b = [None] * len(layer_sizes) # Initialise weights randomly for l in range(1, len(layer_sizes)): W[l] = np.random.random((layer_sizes[l], layer_sizes[l-1])) b[l] = np.random.random((layer_sizes[l], 1)) # Set number of iterations for backpropagation to work, size, and learning rate n_iter = 100 n = X.shape[0] eta = 0.1 . Below we run our backpropagation algorithm for 100 iterations. For every iteration we display the MSE error of the ANN. Interestingly, we observe that the MSE first drops rapidly, but the MSE does not converge to zero. In other words, the training as described above does not lead to a perfect XOR gate; it can only classify 3 pair of inputs correctly. . for iter in range(n_iter+1): W, b, y_pred, MSE = backpropagation_iter(X, y, W, b, eta) # Only print every 10 iterations if iter % 10 == 0: print(&#39;Iteration {0}: {1}&#39;.format(iter, MSE)) . Iteration 0: 0.38656561971217596 Iteration 10: 0.37967674088143133 Iteration 20: 0.37237614217772685 Iteration 30: 0.3647250789164269 Iteration 40: 0.3567771690665022 Iteration 50: 0.34860542693257024 Iteration 60: 0.3403016700706688 Iteration 70: 0.33197373064753966 Iteration 80: 0.32374029713366215 Iteration 90: 0.3157236870674678 Iteration 100: 0.30804138226491107 . Keras implementation . Above I showed how to implement the backpropagation algorithm in python. Below, I will show how to implement the same model using the keras library. . from keras.models import Sequential from keras.layers import Dense from IPython.display import SVG from keras.utils.vis_utils import model_to_dot model = Sequential() model.add(Dense(units=2, activation=&#39;relu&#39;, input_dim=2)) model.add(Dense(units=1, activation=&#39;linear&#39;)) model.compile(loss=&#39;mean_squared_error&#39;, optimizer=&#39;sgd&#39;, metrics=[&#39;accuracy&#39;]) model.fit(X, y, epochs=300, batch_size=4) SVG(model_to_dot(model, show_shapes=True).create(prog=&#39;dot&#39;, format=&#39;svg&#39;)) . Epoch 1/300 4/4 [==============================] - 0s 37ms/step - loss: 0.6039 - acc: 0.7500 Epoch 2/300 4/4 [==============================] - 0s 252us/step - loss: 0.5813 - acc: 0.7500 ... ... Epoch 300/300 4/4 [==============================] - 0s 338us/step - loss: 0.2679 - acc: 0.2500 . . Eye-balling solution . Apparantly the model of both our own implementation and the implementation in Keras is unable to find a minimum without making any erorrs. However, since we have a near trivial problem it is also possible to eye-ball the optimal weight and bias matrices such that the cost is minimised. To see how this works, let’s consider the following values for weights and biases (obtained from GoodFellow et al.): . W(1)=[1111],b(1)=[0−1],W(2)=[1−2],b(2)=0 boldsymbol{W}^{(1)} = begin{bmatrix} 1 &amp; 1 1 &amp; 1 end{bmatrix}, quad boldsymbol{b}^{(1)} = begin{bmatrix} 0 &amp; -1 end{bmatrix}, quad boldsymbol{W}^{(2)} = begin{bmatrix} 1 &amp; -2 end{bmatrix}, quad boldsymbol{b}^{(2)} = 0W(1)=[11​11​],b(1)=[0​−1​],W(2)=[1​−2​],b(2)=0 . The ReLU activation function is used for the first layer. To see how these weights give the correct answer, let’s first “forward” from the input layer to the hidden layer, i.e., . z(1)=XW(1)+b(1)=[0−1101021]. boldsymbol{z}^{(1)} = boldsymbol{X} boldsymbol{W}^{(1)} + boldsymbol{b}^{(1)} = begin{bmatrix} 0 &amp; -1 1 &amp; 0 1 &amp; 0 2 &amp; 1 end{bmatrix}.z(1)=XW(1)+b(1)=⎣⎢⎢⎢⎡​0112​−1001​⎦⎥⎥⎥⎤​. . Applying the activation function to obtain the activation values we get . a(1)=ReLU(z(1))=[00101021], boldsymbol{a}^{(1)} = text{ReLU} left( boldsymbol{z}^{(1)} right) = begin{bmatrix} 0 &amp; 0 1 &amp; 0 1 &amp; 0 2 &amp; 1 end{bmatrix},a(1)=ReLU(z(1))=⎣⎢⎢⎢⎡​0112​0001​⎦⎥⎥⎥⎤​, . where the ReLU function is applied element-wise. We now multiply these activations values with the weights corresponding to the last layer to get . a(2)=z(2)=a(1)W(2)+b(2)=[0110] boldsymbol{a}^{(2)} = boldsymbol{z}^{(2)} = boldsymbol{a}^{(1)} boldsymbol{W}^{(2)} + boldsymbol{b}^{(2)} = begin{bmatrix} 0 1 1 0 end{bmatrix}a(2)=z(2)=a(1)W(2)+b(2)=⎣⎢⎢⎢⎡​0110​⎦⎥⎥⎥⎤​ . Observe that the activation values of the last (output) layer correspond exactly to the values of $ boldsymbol{y}$. .",
            "url": "https://mvanderbroek.com/deep%20learning/python/2018/09/26/Train-XOR-Logic-Gate-in-Neural-Network.html",
            "relUrl": "/deep%20learning/python/2018/09/26/Train-XOR-Logic-Gate-in-Neural-Network.html",
            "date": " • Sep 26, 2018"
        }
        
    
  
    
        ,"post8": {
            "title": "Map Plots In R In A Tidyverse Way",
            "content": "I show how you can plot your own map in R using a few lines of code using a pipe-based workflow. Several powerful functions of the sf packages are presented. . Analysis . This week I worked on a project for which I needed to create a map plot with some statistics for selected European countries; I was unfamiliar with this kind of plots, so I searched online for possible solutions. I like the tidyverse workflow, so I naturally looked for any tutorials using this style. The first hit was informative, but it didn’t have a high resolution map for Europe. Furthermore, I like to be able to use any custom map, so I searched for ways to import a custom map. . naturalearthdata.com provides many open-source maps. I decided to select the world map with country borders on a 1:10m scale (can be found here). . library(sf) # For handling geospatial data library(ggplot2) # Plotting library library(dplyr) # Data manipulation in tidyverse way library(ggthemes) # Additional themese for the ggplot2 library library(knitr) # Nice tables for this document # This will create a natural-earth subfolder with the map data in the data folder. if (!file.exists(&quot;data/natural-earth&quot;)) { tmp_file &lt;- tempfile(fileext=&quot;.zip&quot;) download.file(&quot;https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip&quot;, tmp_file) unzip(tmp_file, exdir = &quot;data/natural-earth&quot;) } . Importing these maps, however, was not straightforward to me. These lecture slides provides a way to import custom maps, but the syntax of the sp package seems very untuitive with S4 objects for the polygons. Furthermore, the SpatialDataFrame objects do not support a pipe-based workflow. However, this tutorial presents how the modern sf package can be used to manipulate, plot and import spatial data in a tidyverse manner. . Importing our world map is as easy as . map_data &lt;- st_read(&quot;data/natural-earth/&quot;, &quot;ne_10m_admin_0_countries&quot;) . ## Reading layer `ne_10m_admin_0_countries&#39; from data source `Map-Plotting/data/natural-earth&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 255 features and 94 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -90 xmax: 180 ymax: 83.6341 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs . The map_data uses data.frames for its features and saves the geometric features as a list in the column geometry. We can now easily explore the data in map_data, e.g., . features_map_data &lt;- map_data %&gt;% as_tibble() %&gt;% select(-geometry) %&gt;% head(10) kable(features_map_data) . featurecla scalerank LABELRANK SOVEREIGNT SOV_A3 ADM0_DIF LEVEL TYPE ADMIN ADM0_A3 GEOU_DIF GEOUNIT GU_A3 SU_DIF SUBUNIT SU_A3 BRK_DIFF NAME NAME_LONG BRK_A3 BRK_NAME BRK_GROUP ABBREV POSTAL FORMAL_EN FORMAL_FR NAME_CIAWF NOTE_ADM0 NOTE_BRK NAME_SORT NAME_ALT MAPCOLOR7 MAPCOLOR8 MAPCOLOR9 MAPCOLOR13 POP_EST POP_RANK GDP_MD_EST POP_YEAR LASTCENSUS GDP_YEAR ECONOMY INCOME_GRP WIKIPEDIA FIPS_10_ ISO_A2 ISO_A3 ISO_A3_EH ISO_N3 UN_A3 WB_A2 WB_A3 WOE_ID WOE_ID_EH WOE_NOTE ADM0_A3_IS ADM0_A3_US ADM0_A3_UN ADM0_A3_WB CONTINENT REGION_UN SUBREGION REGION_WB NAME_LEN LONG_LEN ABBREV_LEN TINY HOMEPART MIN_ZOOM MIN_LABEL MAX_LABEL NE_ID WIKIDATAID NAME_AR NAME_BN NAME_DE NAME_EN NAME_ES NAME_FR NAME_EL NAME_HI NAME_HU NAME_ID NAME_IT NAME_JA NAME_KO NAME_NL NAME_PL NAME_PT NAME_RU NAME_SV NAME_TR NAME_VI NAME_ZH . Admin-0 country | 5 | 2 | Indonesia | IDN | 0 | 2 | Sovereign country | Indonesia | IDN | 0 | Indonesia | IDN | 0 | Indonesia | IDN | 0 | Indonesia | Indonesia | IDN | Indonesia | NA | Indo. | INDO | Republic of Indonesia | NA | Indonesia | NA | NA | Indonesia | NA | 6 | 6 | 6 | 11 | 260580739 | 17 | 3028000 | 2017 | 2010 | 2016 | 4. Emerging region: MIKT | 4. Lower middle income | -99 | ID | ID | IDN | IDN | 360 | 360 | ID | IDN | 23424846 | 23424846 | Exact WOE match as country | IDN | IDN | -99 | -99 | Asia | Asia | South-Eastern Asia | East Asia &amp; Pacific | 9 | 9 | 5 | -99 | 1 | 0 | 1.7 | 6.7 | 1159320845 | Q252 | إندونيسيا | ইন্দোনেশিয়া | Indonesien | Indonesia | Indonesia | Indonésie | Ινδονησία | इंडोनेशिया | Indonézi | a Indonesia | Indonesia | インドネシア | 인도네시아 | Indonesië | Indonezja | Indonési | a Индонезия | Indonesie | n Endonezya | Indonesia | 印度尼西亚 | . Admin-0 country | 5 | 3 | Malaysia | MYS | 0 | 2 | Sovereign country | Malaysia | MYS | 0 | Malaysia | MYS | 0 | Malaysia | MYS | 0 | Malaysia | Malaysia | MYS | Malaysia | NA | Malay. | MY | Malaysia | NA | Malaysia | NA | NA | Malaysia | NA | 2 | 4 | 3 | 6 | 31381992 | 15 | 863000 | 2017 | 2010 | 2016 | 6. Developing region | 3. Upper middle income | -99 | MY | MY | MYS | MYS | 458 | 458 | MY | MYS | 23424901 | 23424901 | Exact WOE match as country | MYS | MYS | -99 | -99 | Asia | Asia | South-Eastern Asia | East Asia &amp; Pacific | 8 | 8 | 6 | -99 | 1 | 0 | 3.0 | 8.0 | 1159321083 | Q833 | ماليزيا | মালয়েশিয়া | Malaysia | Malaysia | Malasia | Malaisie | Μαλαισία | मलेशिया | Malajzia | Malaysia | Malesia | マレーシア | 말레이시아 | Maleisië | Malezja | Malásia | Малайзия | Malaysia | Malezya | Malaysia | 马来西亚 | . Admin-0 country | 6 | 2 | Chile | CHL | 0 | 2 | Sovereign country | Chile | CHL | 0 | Chile | CHL | 0 | Chile | CHL | 0 | Chile | Chile | CHL | Chile | NA | Chile | CL | Republic of Chile | NA | Chile | NA | NA | Chile | NA | 5 | 1 | 5 | 9 | 17789267 | 14 | 436100 | 2017 | 2002 | 2016 | 5. Emerging region: G20 | 3. Upper middle income | -99 | CI | CL | CHL | CHL | 152 | 152 | CL | CHL | 23424782 | 23424782 | Exact WOE match as country | CHL | CHL | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 5 | 5 | 5 | -99 | 1 | 0 | 1.7 | 6.7 | 1159320493 | Q298 | تشيلي | চিলি | Chile | Chile | Chile | Chili | Χιλή | चिली | Chile | Chili | Cile | チリ | 칠레 | Chili | Chile | Chile | Чили | Chile | Şili | Chile | 智利 | . Admin-0 country | 0 | 3 | Bolivia | BOL | 0 | 2 | Sovereign country | Bolivia | BOL | 0 | Bolivia | BOL | 0 | Bolivia | BOL | 0 | Bolivia | Bolivia | BOL | Bolivia | NA | Bolivia | BO | Plurinational State of Bolivia | NA | Bolivia | NA | NA | Bolivia | NA | 1 | 5 | 2 | 3 | 11138234 | 14 | 78350 | 2017 | 2001 | 2016 | 5. Emerging region: G20 | 4. Lower middle income | -99 | BL | BO | BOL | BOL | 068 | 068 | BO | BOL | 23424762 | 23424762 | Exact WOE match as country | BOL | BOL | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 7 | 7 | 7 | -99 | 1 | 0 | 3.0 | 7.5 | 1159320439 | Q750 | بوليفيا | বলিভিয়া | Bolivien | Bolivia | Bolivia | Bolivie | Βολιβία | बोलिविया | Bolívia | Bolivia | Bolivia | ボリビア | 볼리비아 | Bolivia | Boliwia | Bolívia | Боливия | Bolivia | Bolivya | Bolivia | 玻利維亞 | . Admin-0 country | 0 | 2 | Peru | PER | 0 | 2 | Sovereign country | Peru | PER | 0 | Peru | PER | 0 | Peru | PER | 0 | Peru | Peru | PER | Peru | NA | Peru | PE | Republic of Peru | NA | Peru | NA | NA | Peru | NA | 4 | 4 | 4 | 11 | 31036656 | 15 | 410400 | 2017 | 2007 | 2016 | 5. Emerging region: G20 | 3. Upper middle income | -99 | PE | PE | PER | PER | 604 | 604 | PE | PER | 23424919 | 23424919 | Exact WOE match as country | PER | PER | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 4 | 4 | 4 | -99 | 1 | 0 | 2.0 | 7.0 | 1159321163 | Q419 | بيرو | পেরু | Peru | Peru | Perú | Pérou | Περού | पेरू | Peru | Peru | Perù | ペルー | 페루 | Peru | Peru | Peru | Перу | Peru | Peru | Peru | 秘鲁 | . Admin-0 country | 0 | 2 | Argentina | ARG | 0 | 2 | Sovereign country | Argentina | ARG | 0 | Argentina | ARG | 0 | Argentina | ARG | 0 | Argentina | Argentina | ARG | Argentina | NA | Arg. | AR | Argentine Republic | NA | Argentina | NA | NA | Argentina | NA | 3 | 1 | 3 | 13 | 44293293 | 15 | 879400 | 2017 | 2010 | 2016 | 5. Emerging region: G20 | 3. Upper middle income | -99 | AR | AR | ARG | ARG | 032 | 032 | AR | ARG | 23424747 | 23424747 | Exact WOE match as country | ARG | ARG | -99 | -99 | South America | Americas | South America | Latin America &amp; Caribbean | 9 | 9 | 4 | -99 | 1 | 0 | 2.0 | 7.0 | 1159320331 | Q414 | الأرجنتين | আর্জেন্টিনা | Argentinien | Argentina | Argentina | Argentine | Αργεντινή | अर्जेण्टीना | Argentí | na Argentina | Argentina | アルゼンチン | 아르헨티나 | Argentinië | Argentyna | Argenti | na Аргентина | Argentin | a Arjantin | Argentina | 阿根廷 | . Admin-0 country | 3 | 3 | United Kingdom | GB1 | 1 | 2 | Dependency | Dhekelia Sovereign Base Area | ESB | 0 | Dhekelia Sovereign Base Area | ESB | 0 | Dhekelia Sovereign Base Area | ESB | 0 | Dhekelia | Dhekelia | ESB | Dhekelia | NA | Dhek. | DH | NA | NA | NA | U.K. Base | NA | Dhekelia Sovereign Base Area | NA | 6 | 6 | 6 | 3 | 7850 | 5 | 314 | 2013 | -99 | 2013 | 2. Developed region: nonG7 | 2. High income: nonOECD | -99 | -99 | -99 | -99 | -99 | -99 | -099 | -99 | -99 | -99 | -99 | No WOE equivalent. | GBR | ESB | -99 | -99 | Asia | Asia | Western Asia | Europe &amp; Central Asia | 8 | 8 | 5 | 3 | -99 | 0 | 6.5 | 11.0 | 1159320709 | Q9206745 | ديكيليا كانتونمنت | দেখেলিয়া ক্যান্টনমেন্ | ট Dekelia | Dhekelia Cantonment | Dekelia | Dhekelia | Ντεκέλια Κάντονμεντ | ढेकेलिया छावनी | Dekéli | a Dhekelia Cantonment | Base di Dheke | lia デケリア | 데켈리아 지 | 역 Dhekelia Cantonme | nt Dhekelia | Dekeli | a Декелия | Dhekeli | a Dhekelia Kantonu | Căn cứ quân sự Dhekelia | NA | . Admin-0 country | 6 | 5 | Cyprus | CYP | 0 | 2 | Sovereign country | Cyprus | CYP | 0 | Cyprus | CYP | 0 | Cyprus | CYP | 0 | Cyprus | Cyprus | CYP | Cyprus | NA | Cyp. | CY | Republic of Cyprus | NA | Cyprus | NA | NA | Cyprus | NA | 1 | 2 | 3 | 7 | 1221549 | 12 | 29260 | 2017 | 2001 | 2016 | 6. Developing region | 2. High income: nonOECD | -99 | CY | CY | CYP | CYP | 196 | 196 | CY | CYP | -90 | 23424994 | WOE lists as subunit of united Cyprus | CYP | CYP | -99 | -99 | Asia | Asia | Western Asia | Europe &amp; Central Asia | 6 | 6 | 4 | -99 | 1 | 0 | 4.5 | 9.5 | 1159320533 | Q229 | قبرص | সাইপ্রাস | Republik Zypern | Cyprus | Chipre | Chypre | Κύπρος | साइप्रस | Ciprus | Siprus | Cipro | キプロス | 키프로스 | Cyprus | Cypr | Chipre | Кипр | Cypern | Kıbrıs Cumhuriyeti | Cộng hòa Síp | 賽普勒斯 | . Admin-0 country | 0 | 2 | India | IND | 0 | 2 | Sovereign country | India | IND | 0 | India | IND | 0 | India | IND | 0 | India | India | IND | India | NA | India | IND | Republic of India | NA | India | NA | NA | India | NA | 1 | 3 | 2 | 2 | 1281935911 | 18 | 8721000 | 2017 | 2011 | 2016 | 3. Emerging region: BRIC | 4. Lower middle income | -99 | IN | IN | IND | IND | 356 | 356 | IN | IND | 23424848 | 23424848 | Exact WOE match as country | IND | IND | -99 | -99 | Asia | Asia | Southern Asia | South Asia | 5 | 5 | 5 | -99 | 1 | 0 | 1.7 | 6.7 | 1159320847 | Q668 | الهند | ভারত | Indien | India | India | Inde | Ινδία | भारत | India | India | India | インド | 인도 | India | Indie | Índia | Индия | Indien | Hindistan | Ấn Độ | 印度 | . Admin-0 country | 0 | 2 | China | CH1 | 1 | 2 | Country | China | CHN | 0 | China | CHN | 0 | China | CHN | 0 | China | China | CHN | China | NA | China | CN | People&#39;s Republic of China | NA | China | NA | NA | China | NA | 4 | 4 | 4 | 3 | 1379302771 | 18 | 21140000 | 2017 | 2010 | 2016 | 3. Emerging region: BRIC | 3. Upper middle income | -99 | CH | CN | CHN | CHN | 156 | 156 | CN | CHN | 23424781 | 23424781 | Exact WOE match as country | CHN | CHN | -99 | -99 | Asia | Asia | Eastern Asia | East Asia &amp; Pacific | 5 | 5 | 5 | -99 | 1 | 0 | 1.7 | 5.7 | 1159320471 | Q148 | جمهورية الصين الشعبية | গণপ্রজাতন্ত্রী চীন | Volksrepublik China | People&#39;s Republic of China | República Popular China | République populaire de Chine | Λαϊκή Δημοκρατία της Κίνας | चीनी जनवादी गणराज् | य Kína | Republik Rakyat Tiongko | k Cina | 中華人民共和国 | 중화인민공화국 | Volksrepubliek China | Chińska Republika Ludowa | China | Китайская Народная Республика | Kina | Çin Halk Cumhuriyeti | Cộng hòa Nhân dân Trung Hoa | 中华人民共和国 | . For this tutorial we want to focus on a European countries, hence we need to filter the data to only contain the european countries’ info. Fortunately, the map_data contains a feature CONTINTENT, so we can easily filter out the unwanted countries. . europe_map_data &lt;- map_data %&gt;% select(NAME, CONTINENT, SUBREGION, POP_EST) %&gt;% filter(CONTINENT == &quot;Europe&quot;) . Lets try to plot a map of European countries. New versions of ggplot2 contain a function geom_sf which supports plotting sf objects directly, so lets try it… . ggplot(europe_map_data) + geom_sf() + theme_minimal() . . That does not seem to work… the reason is that, even though we removed the data of non European countries, we never changed the bbox setting of our data. The bbox object sets the longitude and latitude range for our plot, which is still for the whole europe. To change this we can use the st_crop function as . europe_map_data &lt;- europe_map_data %&gt;% st_crop(xmin=-25, xmax=55, ymin=35, ymax=71) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant ## throughout all geometries ggplot(europe_map_data) + geom_sf() + theme_minimal() . . If you’re familiar with the ggplot2 workflow, it is now easy to construct the aesthetic mappings like you’re used to. Our map_data contains a feature SUBREGION and Europe is divided into Northern, Eastern, Southern and Western Europe. We can easily visualise this in our European map as . ggplot(europe_map_data) + geom_sf(aes(fill=SUBREGION)) + theme_minimal() . . The sf has many in-built functions; one of these functions is st_area which can be used to compute the area of polygons. The population density of each country can be easily plotted by . europe_map_data &lt;- europe_map_data %&gt;% mutate(area = as.numeric(st_area(.))) %&gt;% mutate(pop_density = POP_EST / area) ggplot(europe_map_data) + geom_sf(aes(fill=pop_density)) + theme_minimal() + scale_fill_continuous_tableau(palette = &quot;Green&quot;) . . Using aggregating functions of the tidyverse package is also straight-forward. Lets create a similar population density plot but instead for each subregion of Europe. . subregion_data &lt;- europe_map_data %&gt;% group_by(SUBREGION) %&gt;% summarise(area = sum(area), pop_est = sum(POP_EST)) %&gt;% ungroup() %&gt;% mutate(pop_density = pop_est / area) ggplot(subregion_data) + geom_sf(aes(fill=pop_density)) + theme_minimal() + scale_fill_continuous_tableau(palette = &quot;Green&quot;) . . As a last exercise lets find the centroid for each country. . # First get all centroids of each European country get_coordinates = function(data) { return_data &lt;- data %&gt;% st_geometry() %&gt;% st_centroid() %&gt;% st_coordinates() %&gt;% as_data_frame() } europe_centres &lt;- europe_map_data %&gt;% group_by(NAME) %&gt;% do(get_coordinates(.)) europe_map_data &lt;- europe_map_data %&gt;% left_join(europe_centres, by=&quot;NAME&quot;) . Actually, I only want to see the centroid of the Netherlands… . netherlands_map_data = europe_map_data %&gt;% filter(NAME == &quot;Netherlands&quot;) %&gt;% st_crop(xmin=1, xmax=10, ymin=50, ymax=55) ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant ## throughout all geometries ggplot(netherlands_map_data) + geom_sf() + geom_point(aes(x=X, y=Y, colour=&quot;red&quot;)) + theme_minimal() . . Setup . The analysis of this tutorial is performed using R version 3.5.1. To use the st_crop function from the sf package version 0.6.3 is needed. geom_sf also requires a recent version of ggplot2. .",
            "url": "https://mvanderbroek.com/r/visualisation/2018/09/19/Map-Plots-in-R-in-a-Tidyverse-Way.html",
            "relUrl": "/r/visualisation/2018/09/19/Map-Plots-in-R-in-a-Tidyverse-Way.html",
            "date": " • Sep 19, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello! My name is Mark van der Broek and I’m passionate about combining operations research, data science and computer science. . History . At the age of 12 I discovered the power of fun in programming by making small hobby websites (offline nowadays) using technologies like HTML, CSS and PHP. During my undergraduate degree in Econometrics and Operations Research I found my love for modeling problems using mathematical models in applications like finance, actuarial sciences, logistics and data analytics. I quickly realised that becoming fluent in the main programming languages like Python, R and C++ was fundemental for applying these mathematical models in real-life. This is why I followed an extra-curricular C++ programming course by Frank Brokken and Jurjen Bokma. More recently with the increasing popularity of machine learning and deep learning I started following the hands-on fast.ai course. . . If you need my full resume, please send me an email. .",
          "url": "https://mvanderbroek.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "JobFunnel (&gt;1.2K stars on github) . . Tool for scraping job websites, and filtering and reviewing the job listings written in Python. . Added proxy support for the scraping to hide your IP address. | Developed user validation procedures for the configuration script. | Set-up unit-test framework and refactored part of the code base to suport unit-testing. | . . ghoR . R package for downloading WHO datasets using their GHO API with data transformation and saving tools. . Set-up skeleton for a complete R package including unit-tests and automatic documentation generation. | Created automatic database detection to prevent downloading existing dataset multiple times. | . . MDSDHVRP-Solver (sexy name alert!) . High-performance meta-heuristic algorithm written in C++ to solve a vehicle routing problem with multiple depots and split-delivery support. . Performed extensive profiling analysis to find bottlenecks in the developed algorithm. | Made major contributions to the code-base in a Object-Oriented-Programming environment. | .",
          "url": "https://mvanderbroek.com/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "Useful Links",
          "content": "Reinforcement Learning . Key RL Papers: Most influential papers in RL research.Ã° | Introduction to Reinforcement Learning: Excellent introduction to Reinforcement Learning | UCL Course in RL: Popular RL course given at University College London by David Silver. | . Machine Learning / Statistics . Towards Model Explainability using SHAP: A new avenue in model explainability for ML models. | Maximum Likelihood and Maximum A Posterior Explained: Understanding these basic concepts in statistics. | The Data Science Lifecycle: Everything about DS projects. | Recipe for Training Neural Networks: Hands-on tips and tricks. | Colah’s Blog: Interesting machine learning blog with excellent visual explanations and intuitions. | Guide to Feature Extraction: Hitchhiker’s guide to feature extraction. | Predictive Maintenance Solutions: Predictive mainentance solutions from a Machine Learning perspective. | It’s not just p=0.048 vs. p=0.052: Another discussion about statistical significance. | Data Project Checklist: Data projects are much more than training an accurate model. | Neat Visualisations: Neat visualisations of important statistical concepts. | . Programming . C++ Quiz: Fun C++ puzzles. | C++ Annotations: Excellent reference manual for the C++ programming language. | Teach Yourself to Program in 10 Years: Not everything happens overnight. | String Formatting in Python 3: Take advantage of the advanced string formatting techniques in Python 3. | The Missing Semester of Your CS Education: Short course to master essential productivity tools for programmers. | . Miscellaneous . DeepL Translator: Best language translator. | Hacker News: Tech news. | Why Books Don’t Work: Argument why books are ineffective for absorbing knowledge. | 100 Best Books of the 21th Century: Includes some of my favourite books. | Sapiens as a blog post: Relates to the link-above about why books don’t work. This is an excellent way to freshen up your memory of the book. | .",
          "url": "https://mvanderbroek.com/useful/",
          "relUrl": "/useful/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mvanderbroek.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}